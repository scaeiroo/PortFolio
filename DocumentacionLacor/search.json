[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LacorDual Sandra Caeiro",
    "section": "",
    "text": "Preparaci√≥n",
    "crumbs": [
      "Introducci√≥n",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "index.html#comprensi√≥n-del-funcionamiento-de-la-producci√≥n",
    "href": "index.html#comprensi√≥n-del-funcionamiento-de-la-producci√≥n",
    "title": "LacorDual Sandra Caeiro",
    "section": "Comprensi√≥n del Funcionamiento de la Producci√≥n",
    "text": "Comprensi√≥n del Funcionamiento de la Producci√≥n\nPara iniciar este proyecto, fue esencial entender c√≥mo funciona el proceso de producci√≥n en Lacor. Esto me permiti√≥ dise√±ar soluciones adecuadas para la gesti√≥n y visualizaci√≥n de los datos. En este sentido, la observaci√≥n de los flujos de trabajo, los tiempos de producci√≥n, y los datos generados a lo largo de cada etapa, fueron clave para la toma de decisiones sobre la mejor manera de estructurar el sistema.",
    "crumbs": [
      "Introducci√≥n",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "index.html#objetivos-del-proyecto",
    "href": "index.html#objetivos-del-proyecto",
    "title": "LacorDual Sandra Caeiro",
    "section": "Objetivos del Proyecto",
    "text": "Objetivos del Proyecto\nEl proyecto se enfoca en optimizar los procesos industriales mediante el an√°lisis de datos planificados y producidos. Se estructura en torno a dos objetivos principales:\n1- Desarrollo de un sistema integral para el an√°lisis de secuencias productivas:\n\nComparaci√≥n entre la planificaci√≥n y la producci√≥n real\nIdentificaci√≥n de desviaciones para la mejora continua\nOptimizaci√≥n de recursos y procesos industriales\n\n2- Implementaci√≥n de una soluci√≥n automatizada que permita:\n\nExtracci√≥n diaria de datos de planificaci√≥n y producci√≥n\nLimpieza y validaci√≥n de los datos hist√≥ricos\nAlmacenamiento centralizado en una base de datos\nVisualizaci√≥n clara y accesible para facilitar la toma de decisiones",
    "crumbs": [
      "Introducci√≥n",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "index.html#configuraci√≥n-del-entorno",
    "href": "index.html#configuraci√≥n-del-entorno",
    "title": "LacorDual Sandra Caeiro",
    "section": "Configuraci√≥n del Entorno",
    "text": "Configuraci√≥n del Entorno\nLa primera fase consisti√≥ en crear una m√°quina virtual con Ubuntu 24.04.1. En ella, instal√© las herramientas necesarias para trabajar en este proyecto:\n\nR 4.4.2: Herramienta principal para la manipulaci√≥n y an√°lisis de datos.\nRStudio: Entorno de desarrollo integrado (IDE) utilizado para trabajar con R.\nPython 3.10.16: Utilizado para scripts adicionales y la integraci√≥n con otras herramientas.\nGitHub Desktop: Para el control de versiones y la colaboraci√≥n en el proyecto.\nVisual Studio Code: Utilizado principalmente para el desarrollo de scripts en Python y otros lenguajes.\n\n\n    \n\nEste entorno proporcion√≥ una base robusta y flexible, permitiendo realizar pruebas y ajustes en las soluciones de manera eficiente. Adem√°s, la configuraci√≥n de la m√°quina virtual permiti√≥ que los recursos del sistema se aislaran, mejorando el rendimiento y la seguridad de los procesos.",
    "crumbs": [
      "Introducci√≥n",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "index.html#metodolog√≠a-de-trabajo",
    "href": "index.html#metodolog√≠a-de-trabajo",
    "title": "LacorDual Sandra Caeiro",
    "section": "Metodolog√≠a de Trabajo",
    "text": "Metodolog√≠a de Trabajo\nEl proyecto se desarrolla siguiendo una metodolog√≠a iterativa, basada en etapas definidas que garantizan un avance ordenado y validado:\n1- Extracci√≥n automatizada:\n\nImplementaci√≥n de sistemas que recojan datos de planificaci√≥n y producci√≥n de forma diaria\nEstandarizaci√≥n del formato de entrada de datos\n\n2- Limpieza y comprobaci√≥n de datos:\n\nDetecci√≥n y resoluci√≥n de duplicados e inconsistencias\nValidaci√≥n de la calidad de los datos hist√≥ricos recopilados\n\n3- Centralizaci√≥n:\n\nCreaci√≥n de una base de datos estructurada\nAutomatizaci√≥n de los procesos de carga y actualizaci√≥n de informaci√≥n\n\n4- Visualizaci√≥n:\n\nDesarrollo de gr√°ficos y dashboards sencillos\nFacilitar la interpretaci√≥n y el an√°lisis por parte de los usuarios finales\nEste enfoque modular permite construir una soluci√≥n robusta y escalable, orientada a maximizar el valor de los datos industriales.",
    "crumbs": [
      "Introducci√≥n",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "Automatizacion_Proceso.html",
    "href": "Automatizacion_Proceso.html",
    "title": "Automatizaci√≥n del Proceso",
    "section": "",
    "text": "Automatizaci√≥n de la Carga de Datos\nEstos scripts est√°n dise√±ados para gestionar las conexiones necesarias y ejecutar de forma automatizada las funciones del script XX_LLAMADA_PRODUCCION.R, garantizando una actualizaci√≥n eficiente de los datos.\nEl script XX_LLAMADA_PRODUCCION.R se ejecuta autom√°ticamente en segundo plano cada d√≠a a las siguientes horas;\nLa tarea programada en cron se configura en el sistema/servidor de Ubuntu editando el crontab con:\ne insertando la siguiente l√≠nea:\nAdem√°s, es necesario otorgar permisos de ejecuci√≥n al script con:\nAsi nos aseguramos una carga de datos continua y sin intervenci√≥n manual. Esta configuraci√≥n optimiza el flujo de trabajo y mantiene la informaci√≥n siempre actualizada.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Automatizaci√≥n del Proceso</span>"
    ]
  },
  {
    "objectID": "Automatizacion_Proceso.html#automatizaci√≥n-de-la-carga-de-datos",
    "href": "Automatizacion_Proceso.html#automatizaci√≥n-de-la-carga-de-datos",
    "title": "Automatizaci√≥n del Proceso",
    "section": "",
    "text": "-6:00 hasta las 22:00 cada 15 mins\n\ncrontab -e\n\n# === Actualizacion Produccion Lacor === #\n\n# Este Script se ejecutara cada **15 minutos**, desde las **05:00 hasta las 22:45**, en las siguientes horas: **05:00, 05:15, 05:30, 05:45, ... hasta las 22:45**.  \n\n*/15 5-22 * * * cd /home/dataml/Documentos/GitHub/Lacor && /usr/bin/Rscript Scripts/XX_LLAMADA_PRODUCCION.R\n\nchmod +x /home/scaeiro/GIT/Lacor/Scripts/XX_LLAMADA_PRODUCCION.R\n\n\nAutomatizacion del Script:\nEl proceso de ejecuci√≥n autom√°tica del script sigue los siguientes pasos:\n\nCarga de configuraci√≥n y funciones: Se importan los archivos de configuraci√≥n y las funciones necesarias.\nActualizaci√≥n desde GitHub: Se ejecuta PullGitHub() para sincronizar el c√≥digo.\nConexi√≥n a bases de datos: Se establecen conexiones con SQL Server y MySQL.\nConsulta y procesamiento de datos: Se ejecutan consultas SQL y se realiza la limpieza y transformaci√≥n de datos.\nCarga de datos a MySQL: Los datos procesados se almacenan en la base de datos MySQL.\nLiberaci√≥n de recursos: Se desconectan las bases de datos y se limpia el entorno de R.\n\n\nXX_LLAMADA_PRODUCCION.R\n#==============================================================================#\n#                     PRODUCCION LACOR SISTEMA AUTOMATIZADO                    #\n#          SISTEMA DE GESTION, MONITORIZACION Y AN√ÅLISIS DE DATOS              #\n#                         DESARROLLADO POR: SANDRA CAEIRO                      #\n#==============================================================================#\n#  FUNCIONES PRINCIPALES:                                                      #\n#  - Integraci√≥n y an√°lisis de datos de producci√≥n.                            #\n#  - C√°lculo autom√°tico de m√©tricas clave.                                     #\n#  - Optimizaci√≥n de procesos mediante automatizaci√≥n.                         #\n#                                                                              #\n#  HERRAMIENTAS UTILIZADAS:                                                    #\n#  - R para procesamiento de datos y generaci√≥n de informes.                   #\n#  - SQL para la extracci√≥n y actualizaci√≥n de datos en la base de datos.      #\n#  - Automatizaci√≥n de flujos de trabajo de producci√≥n.                        #\n#==============================================================================#\n\nsource('./Scripts/0_Cargar_Configuracion.R')\nsource('./Scripts/0_Funciones_Guardado.R')\nsource('./Scripts/0_Funciones_GestionDatos.R')\nsource('./Scripts/1_SQLServer_Base.R')\nsource('./Scripts/1_MySQL_Base.R')\n\nImprimirInicioLog()\n\n#===========================================================#\n#                        PULL GITHUB                        #\n#===========================================================#\n\nPullGitHub()\n\n#===========================================================#\n#                    ABRIR CONEXIONES                       #\n#===========================================================#\n\n# SQL SERVER\nconn &lt;- DameConexionSQLServer(FicheroConf = './Config/Config_PERSONAL.json', \n                              TipoDBElegido = 'SQLServer', AliasElegido = 'DBA', \n                              DominioElegido = 'Remoto', DB_conexion = 'Olanet_Lacor')\nconn2 &lt;- DameConexionSQLServer(FicheroConf = './Config/Config_PERSONAL.json', \n                              TipoDBElegido = 'SQLServer', AliasElegido = 'DBA', \n                              DominioElegido = 'Remoto', DB_conexion = 'Olanet_Lacor_Datos')\n# MYSQL\nMcon &lt;- DameConexionMysql(FicheroConf = './Config/Config_PERSONAL.json',\n                          TipoDBElegido = 'MySQL', AliasElegido ='SUPER',\n                          DominioElegido='Local', DB_conexion = 'LacorDatos')\n\n#===========================================================#\n#           CONSULTA Produccion DIARIO TOTAL SQL            #\n#===========================================================#\nImprimirEncabezado(\"CONSULTAS SQL SERVER\")\n\n# Produccion_query &lt;- getSQLSQLServer(\"./SQL_Sentencias/Select_PRODUCCION.sql\")\n# DF_Produccion_TOTAL &lt;- EjecutarConsultaSQLServer(conn, Produccion_query)\n# DF_Produccion_TOTAL&lt;- FiltrarDuplicadosTOTAL(DF_Produccion_TOTAL)\n# GuardarRData(DF_Produccion_TOTAL,\"./Output/Produccion_TOTAL.RData\")\n\n# == CONSULTA Produccion OF CANTIDAD SQL == #\nProduccion_OFCantidad_query &lt;- getSQLSQLServer(\"./SQL_Sentencias/Select_OF_Cantidad.sql\")\nDF_ProduccionOFCantidad &lt;- EjecutarConsultaSQLServer(conn, Produccion_OFCantidad_query)\nif (!dir.exists(\"./Output/DatosPrevistos\")) {\n  dir.create(\"./Output/DatosPrevistos\", recursive = TRUE)\n}\nGuardarRData(DF_ProduccionOFCantidad,  \"./Output/DatosPrevistos/Datos_OF_Cantidad.RData\")\n\n# == CONSULTA OLANET == #\nOlanetDatos_query &lt;- getSQLSQLServer(\"./SQL_Sentencias/Select_PLANIFICADO.sql\")\nDF_OlanetCOMPLETO &lt;- EjecutarConsultaSQLServer(conn2, OlanetDatos_query)\nGuardarCSV(DF_OlanetCOMPLETO, \"Olanet_Planificado\", \"./Output/CSV\") \n\nrm(OlanetDatos_query)\nrm(Produccion_OFCantidad_query)\n\n#===========================================================#\n#                FUSION Y LIMPIEZA DE DATOS                 #\n#===========================================================#\n\nsource('./Scripts/2_Mantener_DatosActualizados.R')\nsource('./Scripts/3_Trasformacion_Y_Limpieza.R')\nsource('./Scripts/4_ProcesarDatos.R')\nsource('./Scripts/5_CrearMaestros.R')\n\n#===========================================================#\n#                   CARGAR DATOS A MYSQL                    #\n#===========================================================#\n\n# Carga a MySQL\n# Tablas Principales\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/Olanet_Completo.csv\", DB_Tabla = \"Olanet_Completo\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/Olanet_Planificado.csv\", DB_Tabla = \"Olanet_Planificado\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/Olanet_Producido.csv\", DB_Tabla = \"Olanet_Producido\", Mcon = Mcon)\n\n# Maestros\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_Referencia.csv\", DB_Tabla = \"Maestro_Referencia\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_OF.csv\", DB_Tabla = \"Maestro_OF\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_Maquina.csv\", DB_Tabla = \"Maestro_Maquina\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_Expediente.csv\", DB_Tabla = \"Maestro_Expediente\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_EstadoOlanet.csv\", DB_Tabla = \"Maestro_EstadoOlanet\", Mcon = Mcon)\n\n# Desconexion MySQl\ndbDisconnectAllMySQL()\n\n#===========================================================#\n#                        PUSH GITHUB                        #\n#===========================================================#\n\n#PushGitHub()\n\n#===========================================================#\n#                     ELIMINAR ENTORNO                      #\n#===========================================================#\nImprimirEncabezado(\"Eliminar entorno de R\")\nrm(list = ls())\ngc()\n\n# === LOGS === #\nsink()\n# ============ #",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Automatizaci√≥n del Proceso</span>"
    ]
  },
  {
    "objectID": "Automatizacion_Proceso.html#ajustes-en-el-log-de-datos",
    "href": "Automatizacion_Proceso.html#ajustes-en-el-log-de-datos",
    "title": "Automatizaci√≥n del Proceso",
    "section": "Ajustes en el Log de Datos",
    "text": "Ajustes en el Log de Datos\nSe realizaron ajustes en los logs para asegurar una correcta visualizaci√≥n de los datos y una mejor trazabilidad de los procesos. Los logs se a√±adieron para las llamadas de los scripts, lo que permite monitorear el proceso de actualizaci√≥n y garantizar que los datos se est√©n cargando correctamente en cada ejecuci√≥n. Asi de esta menera podemos detectar errores con mucha mas facilidad. Estos estan dentro de la carpeta /Logs/ dentro del repositorio.\n\nEjemplo de Log:\n#===================================================================================================#\n#===================================================================================================#\n#___________________________________________________________________________________________________#\n#_______________________________________üç≥ PRODUCCION LACOR üç≥______________________________________#\n#            üìÖ Fecha actual: 2025-03-25                                                            #\n#            üïí Hora actual: 09:14:17                                                               #\n#                                                                                                   #\n#===================================================================================================#\n#===================================================================================================#\n\n#==============================================================================#\n                            Cargar Datos GitHub\n#==============================================================================#\n\nüò∫ Realizando GitPull para cargar el repositorio...\nYa est√° actualizado. \nRepositorio actualizado desde GitHub a las:  1742890459 \n#==============================================================================#\n                            CONEXI√ìN A SQL SERVER\n#==============================================================================#\n\n‚úÖ CONEXI√ìN A SQL SERVER ESTABLECIDA ‚úÖ\nüìç Host: 192.168.100.214\nüë§ Usuario: datoslim\nüíæ Base de datos: Olanet_Lacor\n#==============================================================================#\n                            CONEXI√ìN A SQL SERVER\n#==============================================================================#\n\n‚úÖ CONEXI√ìN A SQL SERVER ESTABLECIDA ‚úÖ\nüìç Host: 192.168.100.214\nüë§ Usuario: datoslim\nüíæ Base de datos: Olanet_Lacor_Datos\n#==============================================================================#\n                            CONEXI√ìN A MYSQL\n#==============================================================================#\n\n‚úÖ CONEXI√ìN A MYSQL ESTABLECIDA ‚úÖ\nüìç Host: localhost\nüë§ Usuario: LacorIbili\nüíæ Base de datos: LacorDatos\n#==============================================================================#\n                            CONSULTAS SQL SERVER\n#==============================================================================#\n\nüìù SENTENCIA LE√çDA DESDE EL ARCHIVO .SQL üìù\n SELECT       [smOrdenId],       [SmfaseId],       [smMaterialId],       [smMaterialnme],       [CantidadaProducirnbr]   FROM [Olanet_Lacor].[Integration].[OutputMaterials]; \n‚úÖ Consulta ejecutada con √©xito! ‚úÖ\nüìä Resultados obtenidos: 2806 filas.\n‚úÖ Archivo guardado: ' ./Output/DatosPrevistos/Datos_OF_Cantidad.RData ' - Objeto:  DF_ProduccionOFCantidad  - Registros:  2806 \n\nüìù SENTENCIA LE√çDA DESDE EL ARCHIVO .SQL üìù\n SELECT      Olanet.SmFase.smFasePersonalizado1Txt AS 'EXPEDIENTE',     Olanet.SmOrden.smOrdenDsc AS 'REFERENCIA',     Olanet.smMaterial.smMaterialId AS 'MATERIAL_ENTRADA',     Olanet.smMaterial.smMaterialNme AS 'MATERIAL_ENTRADA_DESCR',     Olanet.SmOrden.smOrdenId AS 'OF',     Olanet.SmFase.smFaseId AS 'FASE',     Olanet.SmFase.smFaseNme AS 'FASE_DESCR',     Olanet.smNodo.smNodoId AS 'MAQUINA',     Olanet.smNodo.smNodoNme AS 'MAQ_DESC',     Olanet.SmFase.ssEnumFaseEstadoId AS 'ESTADO_OLANET',     Secuenciador.SmFaseNodo.FechaEstimacionInicioDte AS \"FechaInicio_Plani\",     Secuenciador.SmFaseNodo.FechaEstimacionFinDte AS \"FechaFin_Plani\",     Olanet.SmFase.TiempoCicloNbr AS \"TiempoCiclo\",     Olanet.SmFase.TiempoPreparacionNbr AS \"TiempoPreparacion\",     Secuenciador.SmFaseNodo.duracionTeoricaNbr AS \"TiempoTeorico\" FROM     Olanet.SmOrden JOIN     Olanet.SmFase     ON Olanet.SmOrden.smOrdenUid = Olanet.SmFase.smOrdenUid JOIN     Olanet.smNodo     ON Olanet.SmFase.smNodoUid = Olanet.smNodo.smNodoUid JOIN     Secuenciador.SmFaseNodo     ON Olanet.SmFase.smFaseUid = Secuenciador.SmFaseNodo.smFaseUid JOIN     Olanet.SmFaseMaterialEntrada     ON Olanet.SmFase.smFaseUid = Olanet.SmFaseMaterialEntrada.smFaseUid JOIN     Olanet.SmMaterial     ON Olanet.SmFaseMaterialEntrada.smMaterialUid = Olanet.SmMaterial.SmMaterialUid      \n‚úÖ Consulta ejecutada con √©xito! ‚úÖ\nüìä Resultados obtenidos: 3861 filas.\n‚úÖ Guardado correctamente: ./Output/CSV/Olanet_Planificado.csv - Registros: 3861 \n#==============================================================================#\n                            Datos de Produccion Diaria\n#==============================================================================#\n‚úÖ Consulta ejecutada con √©xito! ‚úÖ\nüìä Resultados obtenidos: 334 filas.\nüîÑ Total de filas originales:  334 \nüéâ Total de filas despu√©s del filtrado:  264 \nüî• Filas eliminadas:  70 \n‚ú® Datos nuevos guardados en: ./Output/Produccion_Diario_DIA_2025-03-25.RData \n#==============================================================================#\n                            CERRAR CONEXIONES SQLSERVER\n#==============================================================================#\n\nüåü‚ú® CERRANDO CONEXIONES A SQL SERVER... ‚ú®üåü\n‚úÖ CONEXIONES SQL SERVER CERRADAS.\n#==============================================================================#\n                            Actualizaci√≥n de Datos\n#==============================================================================#\nüîÑ Iniciando la Fusi√≥n de Datos para actualizar el archivo:\n‚úÖ El archivo RData ' ./Output/Produccion_Diario_DIA_2025-03-25.RData ' se carg√≥ correctamente. Contiene:  ProduccionDiarioDF \n‚úÖ El archivo RData ' ./Output/Produccion_TOTAL.RData ' se carg√≥ correctamente. Contiene:  DF_Produccion_TOTAL \n\nüîç Verificando la estructura de los datos...\n\nüöÆ Eliminando filas duplicadas (si las hay)...\nüîÑ Total de filas originales:  41108 \nüéâ Total de filas despu√©s del filtrado:  40844 \nüî• Filas eliminadas o reemplazadas:  264 \n\nüîÑ Sobrescribiendo el archivos actualizado...\n‚úÖ Archivo guardado: ' ./Output/Produccion_Diario_DIA_2025-03-25.RData ' - Objeto:  DF_ProduccionDiario  - Registros:  264 \n‚úÖ Archivo guardado: ' ./Output/Produccion_TOTAL.RData ' - Objeto:  DF_Produccion_TOTAL  - Registros:  40844 \n\nüíæ Guardando el archivo CSV actualizado en la carpeta Output...\n‚úÖ Guardado correctamente: ./Output/CSV/Produccion_TOTAL.csv - Registros: 40844 \n#==============================================================================#\n                            Trasformaciones y Limpiezas de Datos\n#==============================================================================#\nüîÑ Iniciando la Limpieza de Datos de OLANET:\n‚úÖ El archivo RData ' ./Output/DatosPrevistos/Datos_OF_Cantidad.RData ' se carg√≥ correctamente. Contiene:  DF_ProduccionOFCantidad \n‚úÖ El archivo CSV ' ./Output/CSV/Produccion_TOTAL.csv ' se carg√≥ correctamente. Registros:  40844  - Columnas:  26 \n‚úÖ El archivo CSV ' ./Output/CSV/Olanet_Planificado.csv ' se carg√≥ correctamente. Registros:  3861  - Columnas:  15 \n\nüîç Columnas y Nombres reorganizados correctamente. \n‚úÖ Guardado correctamente: ./OlanetDatos/Olanet_Planificado.csv - Registros: 3861 \n‚úÖ Guardado correctamente: ./OlanetDatos/Olanet_Producido.csv - Registros: 35949 \n#==============================================================================#\n                            Procesar Datos\n#==============================================================================#\n‚úÖ El archivo CSV ' ./OlanetDatos/Olanet_Producido.csv ' se carg√≥ correctamente. Registros:  35949  - Columnas:  26 \n‚úÖ El archivo CSV ' ./OlanetDatos/Olanet_Planificado.csv ' se carg√≥ correctamente. Registros:  3861  - Columnas:  16 \n‚úÖ Guardado correctamente: ./OlanetDatos/Olanet_Completo.csv - Registros: 3861 \n#==============================================================================#\n                            Creacion de MAESTROS OLANET PLANIFICADO\n#==============================================================================#\n‚úÖ El archivo CSV ' ./OlanetDatos/Olanet_Planificado.csv ' se carg√≥ correctamente. Registros:  3861  - Columnas:  16 \n\nüî¢ Creando Maestros Identificadores... \n‚úÖ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_Identificador.csv - Registros: 467 \n‚úÖ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_Maquina.csv - Registros: 9 \n‚úÖ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_Expediente.csv - Registros: 273 \n‚úÖ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_OF.csv - Registros: 1010 \n‚úÖ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_Referencia.csv - Registros: 378 \n‚úÖ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_EstadoOlanet.csv - Registros: 4 \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\nüî¢ Se van a cargar 3861 filas desde el archivo CSV \n\nüßπ Eliminando registros previos:  3861 \n\nüìà Se han cargado 3861 filas en la tabla 'Olanet_Completo'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\nüî¢ Se van a cargar 3861 filas desde el archivo CSV \n\nüßπ Eliminando registros previos:  3861 \n\nüìà Se han cargado 3861 filas en la tabla 'Olanet_Planificado'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\nüî¢ Se van a cargar 35949 filas desde el archivo CSV \n\nüî¢ Creando la tabla y cargando los datos...\n\n‚úÖ Tabla creada y datos cargados con √©xito.\n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\nüî¢ Se van a cargar 377 filas desde el archivo CSV \n\nüßπ Eliminando registros previos:  377 \n\nüìà Se han cargado 377 filas en la tabla 'Maestro_Referencia'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\nüî¢ Se van a cargar 1010 filas desde el archivo CSV \n\nüßπ Eliminando registros previos:  1010 \n\nüìà Se han cargado 1010 filas en la tabla 'Maestro_OF'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\nüî¢ Se van a cargar 9 filas desde el archivo CSV \n\nüßπ Eliminando registros previos:  9 \n\nüìà Se han cargado 9 filas en la tabla 'Maestro_Maquina'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\nüî¢ Se van a cargar 467 filas desde el archivo CSV \n\nüßπ Eliminando registros previos:  467 \n\nüìà Se han cargado 467 filas en la tabla 'Maestro_Identificador'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\nüî¢ Se van a cargar 273 filas desde el archivo CSV \n\nüßπ Eliminando registros previos:  273 \n\nüìà Se han cargado 273 filas en la tabla 'Maestro_Expediente'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\nüî¢ Se van a cargar 4 filas desde el archivo CSV \n\nüßπ Eliminando registros previos:  4 \n\nüìà Se han cargado 4 filas en la tabla 'Maestro_EstadoOlanet'. \n#==============================================================================#\n                            CERRAR CONEXIONES MYSQL\n#==============================================================================#\n\nüîí CERRANDO CONEXIONES MYSQL...\nCONEXIONES MYSQL CERRADAS: 1 conexi√≥n(es) cerrada(s)\n#==============================================================================#\n                            Actualizar Datos GitHub\n#==============================================================================#\n\nüò∫ Realizando GitPush para actualizar el repositorio...\nCambios subidos a GitHub a las:  1742890473 \n#==============================================================================#\n                            Eliminar entorno de R\n#==============================================================================#\n          used (Mb) gc trigger  (Mb) max used  (Mb)\nNcells 1297512 69.3    2466342 131.8  2466342 131.8\nVcells 3596967 27.5   14786712 112.9 14786712 112.9",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Automatizaci√≥n del Proceso</span>"
    ]
  },
  {
    "objectID": "Conexi√≥n_a_la_Base_de_Datos.html",
    "href": "Conexi√≥n_a_la_Base_de_Datos.html",
    "title": "Conexion a la Base de Datos",
    "section": "",
    "text": "Conexi√≥n a Microsoft SQL Server\nEsta fase consisti√≥ en realizar pruebas de conexi√≥n a la base de datos utilizando R. Para ello, primero agregu√© el repositorio de Microsoft e instal√© el controlador ODBC para SQL Server:\nüîó Descarga ODBC Driver 17 para SQL Server\nCon esta configuraci√≥n, cre√© un script base que configura la conexi√≥n y ejecuta consultas SQL para obtener los datos de consumo diario.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Conexion a la Base de Datos</span>"
    ]
  },
  {
    "objectID": "Conexi√≥n_a_la_Base_de_Datos.html#conexi√≥n-a-microsoft-sql-server",
    "href": "Conexi√≥n_a_la_Base_de_Datos.html#conexi√≥n-a-microsoft-sql-server",
    "title": "Conexion a la Base de Datos",
    "section": "",
    "text": "ODBC Driver 17 for SQL Server\n\n\n\n1_SQLServer_Base.R\nEste script 1_SQLServer_Base.R sirve para crear las conexiones y desconexiones con la base de datos SQL y generar un archivo TOTAL que se actualice a diario, obteniendo las producciones diarias y acumul√°ndolas.\n\nDetalles del Script\n1.1. Conexi√≥n a la Base de Datos:\nEl script utiliza la funci√≥n DameConexionSQLServer para establecer la conexi√≥n a la base de datos. La funci√≥n toma como par√°metros la configuraci√≥n almacenada en un archivo JSON, el tipo de base de datos, y detalles sobre el alias, dominio y base de datos a conectar.\nconn &lt;- DameConexionSQLServer(FicheroConf = './Config/Config_PERSONAL.json', \n                              TipoDBElegido = 'SQLServer', AliasElegido = 'DBA', \n                              DominioElegido = 'Remoto', DB_conexion = 'Olanet_Lacor')\n1.2. Desconexi√≥n a la Base de Datos:\nEl script tambi√©n incluye una funci√≥n dbDisconnectAllSQLServer para cerrar las conexiones abiertas a la base de datos, garantizando que no queden conexiones abiertas al finalizar el proceso.\ndbDisconnectAllSQLServer()\n1.3. Ejecutar Consultas SQL:\nLa funci√≥n EjecutarConsultaSQLServer ejecuta una consulta SQL sobre la base de datos. Toma la conexi√≥n y la sentencia SQL como par√°metros, ejecuta la consulta y devuelve los resultados en formato de data frame.\nproduccion_query &lt;- getSQLSQLServer(\"Select_ProduccionDiario.sql\")\nproduccion_diaria &lt;- EjecutarConsultaSQLServer(conn, produccion_query)\n1.4. Leer Sentencias SQL desde un Archivo::\nLa funci√≥n getSQLSQLServer lee un archivo .sql y lo convierte en una sentencia ejecutable en SQL Server.\nsql_sentencia &lt;- getSQLSQLServer(\"path/to/your/sqlfile.sql\")",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Conexion a la Base de Datos</span>"
    ]
  },
  {
    "objectID": "Conexi√≥n_a_la_Base_de_Datos.html#conexi√≥n-a-mysql",
    "href": "Conexi√≥n_a_la_Base_de_Datos.html#conexi√≥n-a-mysql",
    "title": "Conexion a la Base de Datos",
    "section": "Conexi√≥n a MYSQL",
    "text": "Conexi√≥n a MYSQL\nSe cre√≥ el script 1_MySQL_Base.R para importar los datos de producci√≥n a MySQL. La funci√≥n asociada actualiza autom√°ticamente la base de datos, eliminando los datos anteriores y cargando los nuevos registros del archivo .CSV.\nAntes de nada al igual que en SQL Server deberemos instalar el driver de MySQL:\nüîó Descarga MySQL Conector NET 9.2.0\n\nMySQL Conector NET 9.2.0\n\n\n1_MySQL_Base.R\n\nDetalles del Script\n1.1. DameConexionMysql:\nEsta funci√≥n establece una conexi√≥n con la base de datos MySQL utilizando los par√°metros definidos en un archivo de configuraci√≥n JSON. En caso de √©xito, devuelve la conexi√≥n; si ocurre un error, maneja la excepci√≥n y muestra un mensaje de advertencia.\nMcon &lt;- DameConexionMysql(FicheroConf = './Config/Config_PERSONAL.json',\n                          AliasElegido ='SUPER', \n                          DominioElegido='Local')\n1.2. dbDisconnectAllMySQL:\nEsta funci√≥n se encarga de cerrar todas las conexiones abiertas a MySQL, asegurando que no queden conexiones activas al finalizar el proceso.\ndbDisconnectAllMySQL()\n1.3. CargarCSVMySQL:\nEsta funci√≥n se encarga de cargar un archivo .CSV en una tabla de MySQL. Si la tabla ya existe, elimina los datos previos y luego carga los nuevos registros. Si la tabla no existe, la crea y luego realiza la carga de datos.\nCargarCSVMySQL(Mcon = Mcon)",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Conexion a la Base de Datos</span>"
    ]
  },
  {
    "objectID": "Tratamiento_Y_Limpieza.html",
    "href": "Tratamiento_Y_Limpieza.html",
    "title": "Tratamiento y Limpieza de Datos",
    "section": "",
    "text": "2_Mantener_DatosActualizados.R",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tratamiento y Limpieza de Datos</span>"
    ]
  },
  {
    "objectID": "Tratamiento_Y_Limpieza.html#mantener_datosactualizados.r",
    "href": "Tratamiento_Y_Limpieza.html#mantener_datosactualizados.r",
    "title": "Tratamiento y Limpieza de Datos",
    "section": "",
    "text": "Carga y Actualizaci√≥n de Datos:\nEste script est√° dise√±ado para actualizar y fusionar datos de producci√≥n, asegurando que los archivos se mantengan al d√≠a con la informaci√≥n m√°s reciente.\n\nCreacio del RData Diario Antes de iniciar el proceso sacamos los datos del mismo dia y cerramos conexiones ya que la consulta ya esta realizada.\n  #===========================================================#\n  #         CREAR RDATA DIARIO Y CERRAR CONEXIONES            #\n  #===========================================================#\n  Actualizar_Datos_ProduccionDiario(conn)\n  # Desconexion SQLServer\n  dbDisconnectAllSQLServer()\nCargar los DataFrames: El primer paso es cargar los archivos de producci√≥n diaria y total para el d√≠a actual. Usamos Sys.Date() para obtener la fecha actual y luego cargamos los archivos correspondientes.\n  # Obtener la fecha actual üìÖ\n  fecha_actual &lt;- Sys.Date()\n\n  archivo_diario &lt;- paste0(\"./Output/Produccion_Diario_DIA_\", fecha_actual, \".RData\")\n  archivo_total &lt;- \"./Output/Produccion_TOTAL.RData\"\n\n  cat(paste(\"üîÑ Iniciando la Fusi√≥n de Datos para actualizar el archivo:\\n\"))\n  DF_ProduccionDiario &lt;- CargarRData(archivo_diario)\n  DF_Produccion_TOTAL &lt;- CargarRData(archivo_total)\nFusi√≥n de Datos: Una vez cargados los datos, se verifica la estructura de los DataFrames y se ajustan las columnas para garantizar que ambos DataFrames tengan el mismo formato. Esto se hace eliminando la √∫ltima columna del DataFrame diario y renombrando las columnas para que coincidan con el DataFrame total.\n# Verificaci√≥n de la estructura de los datos y renombrado de columnas\ncolumnas_Produccion_diario_df &lt;- colnames(ProduccionDiarioDF)\ncolumnas_Produccion_total_df &lt;- colnames(ProduccionDiario_TOTAL_DF)\nProduccionDiarioDF &lt;- ProduccionDiarioDF[, -ncol(ProduccionDiarioDF)]\nnames(ProduccionDiarioDF) &lt;- columnas_Produccion_total_df\nEliminaci√≥n de Duplicados: Despu√©s de fusionar los datos, se eliminan posibles filas duplicadas utilizando la funci√≥n FiltrarDuplicadosTOTALDIARIO para garantizar que los datos sean √∫nicos y no haya redundancias.\n# Eliminaci√≥n de filas duplicadas\nProduccionDiario_TOTAL_DF &lt;- FiltrarDuplicadosTOTALDIARIO(ProduccionDiario_TOTAL_DF, ProduccionDiarioDF)\nGuardar y Sobrescribir los Datos: Finalmente, los datos fusionados y limpios se guardan en los archivos correspondientes, tanto en formato .RData como en .CSV para su posterior uso y an√°lisis.\n# Guardar archivos actualizados\nGuardarRData(ProduccionDiarioDF, archivo_diario)\nGuardarRData(ProduccionDiario_TOTAL_DF, archivo_total)\nGuardarCSV(ProduccionDiario_TOTAL_DF, \"Produccion_Diario_TOTAL\", \"./Output/CSV\")",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tratamiento y Limpieza de Datos</span>"
    ]
  },
  {
    "objectID": "Tratamiento_Y_Limpieza.html#trasformacion_y_limpieza.r",
    "href": "Tratamiento_Y_Limpieza.html#trasformacion_y_limpieza.r",
    "title": "Tratamiento y Limpieza de Datos",
    "section": "3_Trasformacion_Y_Limpieza.R",
    "text": "3_Trasformacion_Y_Limpieza.R\n\nTransformaciones y Limpiezas de Datos:\nEste script realiza la transformaci√≥n y limpieza de varios DataFrames relacionados con los datos tanto de Producci√≥n como lo Planificado.\n\nCarga de Datos:\nEl primer paso consiste en cargar los datos de las diferentes fuentes, incluyendo archivos .RData y .CSV. Esto proporciona los datos necesarios para realizar las transformaciones posteriores.\ncat(paste(\"üîÑ Iniciando la Limpieza de Datos de OLANET:\\n\"))\nDF_CANTIDAD &lt;-CargarRData(\"./Output/DatosPrevistos/Datos_OF_Cantidad.RData\")\nDF_PRODUCIDO &lt;- CargarCSV(\"./Output/CSV/Produccion_TOTAL.csv\")\nDF_PLANIFICADO &lt;- CargarCSV(\"./Output/CSV/Olanet_Planificado.csv\")\n\n\nTranformaciones y Limpieza:\n\nProcesamiento de Datos Planificados con Cantidades: Se fusiona el DataFrame DF_PLANIFICADO con la informaci√≥n de cantidades previstas de DF_CANTIDAD. Esto permite enriquecer los datos planificados con informaci√≥n sobre las cantidades que se esperan producir.\n       DF_PLANIFICADO_CANTIDAD &lt;- merge(DF_PLANIFICADO, DF_CANTIDAD[, c(\"smOrdenId\", \"SmfaseId\", \"smMaterialId\", \"smMaterialnme\", \"CantidadaProducirnbr\")], \n                                    by.x = c(\"OF\", \"FASE\"), \n                                    by.y = c(\"smOrdenId\", \"SmfaseId\"), \n                                    all.x = TRUE)\n\n    colnames(DF_PLANIFICADO_CANTIDAD)[colnames(DF_PLANIFICADO_CANTIDAD) == \"CantidadaProducirnbr\"] &lt;- \"CantidadPrevista\"\n    DF_PLANIFICADO_CANTIDAD &lt;- DF_PLANIFICADO_CANTIDAD[, -c(ncol(DF_PLANIFICADO_CANTIDAD)-1, ncol(DF_PLANIFICADO_CANTIDAD)-2)]\nFiltrado de Datos Planificados: Se aplica un filtro para eliminar referencias que comienzan con ‚ÄúMTO‚Äù o que contienen espacios, lo que asegura que solo se consideren referencias v√°lidas en el an√°lisis.\n   DF_PLANIFICADO_CANTIDAD &lt;- DF_PLANIFICADO_CANTIDAD %&gt;%\n  filter(!grepl(\"^MTO\", REFERENCIA) & !grepl(\"\\\\s\", REFERENCIA))\nLimpieza de Datos de Producci√≥n: Se procesan los datos de producci√≥n, asegurando que el campo OF contiene solo valores num√©ricos v√°lidos y filtrando aquellos que empiezan con ‚Äú24-‚Äù. Adem√°s, se eliminan las filas que no tienen valor en MATERIAL_SALIDA.\n  DF_PRODUCIDO &lt;- DF_PRODUCIDO %&gt;%\n  filter(!is.na(OF) & grepl(\"^[0-9]+$\", OF)) %&gt;%\n  mutate(OF = as.numeric(OF)) %&gt;%\n  filter(!grepl(\"^24-\", OF))\nDF_PRODUCIDO &lt;- DF_PRODUCIDO[!is.na(DF_PRODUCIDO$MATERIAL_SALIDA), ]\n\n\n\nGuardar los Archivos Resultantes:\nDespu√©s de realizar todas las transformaciones y limpiezas necesarias, los DataFrames resultantes se guardan en archivos .CSV para su posterior an√°lisis y uso.\n# Guardar los DataFrames resultantes en CSV\nGuardarCSV(DF_PLANIFICADO_CANTIDAD,\"Olanet_Planificado\",\"./OlanetDatos\")\nGuardarCSV(DF_PRODUCIDO,\"Olanet_Producido\",\"./OlanetDatos\")",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tratamiento y Limpieza de Datos</span>"
    ]
  },
  {
    "objectID": "Tratamiento_Y_Limpieza.html#resumen-final",
    "href": "Tratamiento_Y_Limpieza.html#resumen-final",
    "title": "Tratamiento y Limpieza de Datos",
    "section": "Resumen Final",
    "text": "Resumen Final\nEn resumen, este conjunto de scripts realiza las siguientes operaciones:\n\nCarga de datos de diferentes fuentes.\nFusi√≥n de datos, eliminaci√≥n de duplicados y valores no validos\nGuardado de los datos procesados en archivos .CSV para su uso posterior.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tratamiento y Limpieza de Datos</span>"
    ]
  },
  {
    "objectID": "Funciones_Adicionales.html",
    "href": "Funciones_Adicionales.html",
    "title": "Funciones Adicionales",
    "section": "",
    "text": "Configuraci√≥n Inicial",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Funciones Adicionales</span>"
    ]
  },
  {
    "objectID": "Funciones_Adicionales.html#configuraci√≥n-inicial",
    "href": "Funciones_Adicionales.html#configuraci√≥n-inicial",
    "title": "Funciones Adicionales",
    "section": "",
    "text": "0_Cargar_Configuracion.R\nEs el primer Script que se carga por lo tanto antes de hacer nada cargamos todas las librerias necesarias, si no las tenemos se nos instalaran automaticamente:\noptions(repos = c(CRAN = \"https://cran.r-project.org\"))\noptions(scipen = 999)  \n#==============================================================================#\nif (! (\"jsonlite\" %in% rownames(installed.packages()))) { install.packages(\"jsonlite\") }\nlibrary(jsonlite)\nif (! (\"dplyr\" %in% rownames(installed.packages()))) { install.packages(\"dplyr\") }\nlibrary(dplyr)\nif (!requireNamespace(\"tidyr\", quietly = TRUE)) {install.packages(\"tidyr\")}\nlibrary(tidyr)  \nif (!(\"RODBC\" %in% rownames(installed.packages()))) { install.packages(\"RODBC\") }\nlibrary(RODBC)\n#==============================================================================#\nif (! (\"RMySQL\" %in% rownames(installed.packages()))) { install.packages(\"RMySQL\") }\nlibrary(RMySQL)\nlibrary(tools)\nif (! (\"readr\" %in% rownames(installed.packages()))) { install.packages(\"readr\") }\nlibrary(readr)\nif (! (\"stringr\" %in% rownames(installed.packages()))) { install.packages(\"stringr\") }\nlibrary(stringr)\nif (! (\"svDialogs\" %in% rownames(installed.packages()))) { install.packages(\"svDialogs\") }\nlibrary(svDialogs)\n#==============================================================================#\n\nFunci√≥n de Configuraci√≥n\n1.1 CargarConfiguracion\nLa funci√≥n CargarConfiguracion() carga y filtra las configuraciones de una base de datos en formato JSON para ajustarlas a las preferencias de base de datos, alias y dominio. La configuraci√≥n se almacena globalmente para ser utilizada en otros scripts.\nCargarConfiguracion &lt;- function(FicheroConf = './Config/Config.json', TipoDBElegido ='SQL Server', AliasElegido = 'DBA', DominioElegido = 'Remoto'){\n  .GlobalEnv$configuracion &lt;- fromJSON(FicheroConf)\n  .GlobalEnv$configuracion &lt;- .GlobalEnv$configuracion %&gt;% filter((toupper(TipoDB) == toupper(TipoDBElegido))) \n  .GlobalEnv$configuracion &lt;- .GlobalEnv$configuracion %&gt;% filter((toupper(Alias) == toupper(AliasElegido))) \n  .GlobalEnv$configuracion &lt;- .GlobalEnv$configuracion %&gt;% filter((toupper(Dominio) == toupper(DominioElegido)))\n  \n  return(.GlobalEnv$configuracion)\n}\nPar√°metros:\n\nFicheroConf: Ruta al archivo de configuraci√≥n (por defecto ‚Äò./Config/Config.json‚Äô).\nTipoDBElegido, AliasElegido, DominioElegido: Par√°metros para filtrar las configuraciones.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Funciones Adicionales</span>"
    ]
  },
  {
    "objectID": "Funciones_Adicionales.html#funciones-de-guardado-y-carga-de-archivos",
    "href": "Funciones_Adicionales.html#funciones-de-guardado-y-carga-de-archivos",
    "title": "Funciones Adicionales",
    "section": "Funciones de Guardado y Carga de Archivos:",
    "text": "Funciones de Guardado y Carga de Archivos:\n\n0_Funciones_Guardado.R\n\nFunci√≥n Encabezados para Logs\n1.2 ImprimirEncabezado\nLa funci√≥n ImprimirEncabezado() imprime un encabezado formateado en la consola para organizar visualmente los scripts.\nImprimirEncabezado &lt;- function(texto) {\n  cat(paste0(\n    \"#==============================================================================#\\n\",\n    paste(\"                           \", texto),\n    \"\\n#==============================================================================#\\n\"\n  ))\n}\nPar√°metros:\n\ntexto: El texto a imprimir como encabezado.\n\n\n\nFunci√≥n Inicio Logs\n1.3 ImprimirInicioLog\nLa funci√≥n ImprimirInicioLog() redirige la salida a un archivo de log con fecha actual y muestra un encabezado decorativo con informaci√≥n sobre la fecha y hora de ejecuci√≥n.\nImprimirInicioLog &lt;- function() {\n  log_file &lt;- paste0('./Logs/LogsProduccion_', Sys.Date(), '.txt')\n  sink(log_file, append = TRUE, split = TRUE)\n  cat(paste0(\n    \"\\n\",\n    \"#===================================================================================================#\\n\",\n    \"#===================================================================================================#\\n\",\n    \"#___________________________________________________________________________________________________#\\n\",\n    \"#_______________________________________üç≥ PRODUCCION LACOR üç≥______________________________________#\\n\",\n    sprintf(\"#            üìÖ Fecha actual: %-70s#\\n\", Sys.Date()),     \n    sprintf(\"#            üïí Hora actual: %-71s#\\n\", format(Sys.time(), \"%H:%M:%S\")),      \n    \"#                                                                                                   #\\n\",\n    \"#===================================================================================================#\\n\",\n    \"#===================================================================================================#\\n\",\n    \"\\n\"\n  ))\n}\nPar√°metros: - No requiere par√°metros de entrada.\n\n\nFunci√≥n de Guardado\n1.4 GuardarCSV\nLa funci√≥n GuardarCSV() Guarda un DataFrame como archivo CSV y verifica que se guard√≥ correctamente.\nGuardarCSV &lt;- function(df, nombre_archivo, ruta) {\n  tryCatch({\n    if (!dir.exists(ruta)) {\n      dir.create(ruta, recursive = TRUE) \n    }\n    ruta_completa &lt;- file.path(ruta, paste0(nombre_archivo, \".csv\"))\n    write.csv(df, ruta_completa, row.names = FALSE, fileEncoding = \"UTF-8\", na = \"\")\n    cat(\"‚úÖ Guardado correctamente:\", ruta_completa, \"- Registros:\", nrow(df), \"\\n\")\n  }, error = function(e) {\n    cat(\"‚ùå Error al guardar:\", nombre_archivo, \"- Mensaje:\", e$message, \"\\n\")\n  })\n}\nPar√°metros:\n\ndf: El DataFrame a guardar.\nnombre_archivo: El nombre del archivo.\nruta: La ruta donde guardar el archivo.\n\n\n\nFunci√≥n de Guardado RData\n1.5 GuardarRData\nLa funci√≥n GuardarRData() Guarda un objeto R en un archivo .RData y verifica que se haya guardado correctamente.\nGuardarRData &lt;- function(objeto, ruta_archivo) {\n  tryCatch({\n    nombre_objeto &lt;- deparse(substitute(objeto))\n    assign(nombre_objeto, objeto)\n    save(list = nombre_objeto, file = ruta_archivo)\n    if (file.exists(ruta_archivo)) {\n      registros &lt;- if (is.data.frame(objeto)) nrow(objeto) else \"N/A\"\n      cat(\"‚úÖ Archivo guardado: '\", ruta_archivo, \"' - Objeto: \", nombre_objeto, \" - Registros: \", registros, \"\\n\")\n    } else {\n      cat(\"‚ùå Error: No se pudo guardar '\", ruta_archivo, \"'.\\n\")\n    }\n  }, error = function(e) {\n    cat(\"‚ùå Error al guardar '\", ruta_archivo, \"': \", e$message, \"\\n\")\n  })\n}\nPar√°metros:\n\nobjeto: El objeto R que deseas guardar (como un DataFrame, lista, etc.).\nruta_archivo: La ruta completa y nombre del archivo .RData donde se guardar√° el objeto.\n\n\n\nFunci√≥n de Carga de CSV\n1.6 CargarCSV\nLa funci√≥n CargarCSV() Carga un archivo CSV y maneja los casos donde el archivo no existe o el formato no es v√°lido\nCargarCSV &lt;- function(ruta_archivo) {\n  if (!file.exists(ruta_archivo)) {\n    cat(\"‚ùå El archivo CSV '\", ruta_archivo, \"' no se encuentra. Verifica el nombre o la fecha.\\n\")\n    stop(\"Archivo CSV no encontrado.\")\n  }\n  \n  tryCatch({\n    df &lt;- read.csv(ruta_archivo, sep = \",\", stringsAsFactors = FALSE, fileEncoding = \"UTF-8\")\n    if (ncol(df) == 1) {\n      df &lt;- read.csv(ruta_archivo, sep = \";\", stringsAsFactors = FALSE, fileEncoding = \"UTF-8\")\n    }\n    cat(\"‚úÖ El archivo CSV '\", ruta_archivo, \"' se carg√≥ correctamente. Registros: \", nrow(df), \" - Columnas: \", ncol(df), \"\\n\")\n    return(df)\n  }, error = function(e) {\n    cat(\"‚ùå Error al cargar '\", ruta_archivo, \"': \", e$message, \"\\n\")\n    stop(e)\n  })\n}\nPar√°metros:\n\nruta_archivo: La ruta completa del archivo CSV que deseas cargar.\n\n\n\nFunci√≥n de Carga de RData\n1.7 CargarRData\nLa funci√≥n CargarRData() Carga un archivo .RData y verifica que el proceso se haya realizado correctamente.\nCargarRData &lt;- function(ruta_archivo) {\n  if (!file.exists(ruta_archivo)) {\n    cat(\"‚ùå El archivo RData '\", ruta_archivo, \"' no se encuentra. Verifica el nombre o la fecha.\\n\")\n    stop(\"Archivo RData no encontrado.\")\n  }\n  objetos_cargados &lt;- load(ruta_archivo, envir = .GlobalEnv)  \n  cat(\"‚úÖ El archivo RData '\", ruta_archivo, \"' se carg√≥ correctamente. Contiene: \", paste(objetos_cargados, collapse = \", \"), \"\\n\")\n  if (length(objetos_cargados) == 1) {\n    return(get(objetos_cargados[1], envir = .GlobalEnv))\n  } else {\n    return(mget(objetos_cargados, envir = .GlobalEnv))\n  }\n}\nPar√°metros:\n\nruta_archivo: La ruta completa del archivo .RData que deseas cargar.\n\n\n\nFunci√≥n de Push para Github\n1.8 PushGitHub\nLa funci√≥n PushGitHub() se encarga de subir los cambios realizados del script para tenerlos actualizados en GitHub.\nPushGitHub &lt;- function() {\n  ImprimirEncabezado(\"Actualizar Datos GitHub\")\n  \n  # === REPOSITORIO === #\n  repo_dir &lt;- \"/home/dataml/Documentos/GitHub/Lacor\" \n  \n  commit_message &lt;- paste(\"Actualizaci√≥n autom√°tica diaria: \", Sys.time())\n  \n  setwd(repo_dir)\n  \n  system(\"git add .\")  \n  system(paste(\"git commit -m '\", commit_message, \"'\", sep = \"\")) \n  system(\"git push origin main\")  \n  \n  cat(\"Cambios subidos a GitHub a las: \", Sys.time(), \"\\n\")\n}\nPar√°metros: - No requiere par√°metros de entrada.\n\n\nFunci√≥n de Pull para Github\n1.8 PullGitHub\nLa funci√≥n PullGitHub() se encarga de cargar el repositorio antes de iniciar cualquier proceso para asi mantener todo actualizado\nPullGitHub &lt;- function() {\n  ImprimirEncabezado(\"Cargar Datos GitHub\")\n  \n  # === REPOSITORIO === #\n  ImprimirEncabezado(\"Actualizar Datos desde GitHub\")\n  \n  repo_dir &lt;- \"/home/dataml/Documentos/GitHub/Lacor\"\n  \n  setwd(repo_dir)\n  \n  system(\"git pull origin main\")\n  \n  cat(\"Repositorio actualizado desde GitHub a las: \", Sys.time(), \"\\n\")\n}\nPar√°metros: - No requiere par√°metros de entrada.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Funciones Adicionales</span>"
    ]
  },
  {
    "objectID": "Funciones_Adicionales.html#funciones-de-gestion-de-datos-y-filtrados",
    "href": "Funciones_Adicionales.html#funciones-de-gestion-de-datos-y-filtrados",
    "title": "Funciones Adicionales",
    "section": "Funciones de Gestion de Datos y Filtrados:",
    "text": "Funciones de Gestion de Datos y Filtrados:\n\n0_Funciones_GestionDatos.R\n\nFunci√≥n Guardar Datos con Fecha\n2.1 Guardar_Datos_con_Fecha\nLa funci√≥n Guardar_Datos_con_Fecha() Guarda un DataFrame de producci√≥n diario con la fecha actual en el nombre del archivo .RData.\nGuardar_Datos_con_Fecha &lt;- function(ProduccionDiarioDF) {\n  fecha_actual &lt;- Sys.Date()\n  output_file &lt;- paste0('./Output/Produccion_Diario_DIA_', fecha_actual, '.RData')\n  save(ProduccionDiarioDF, file = output_file)\n  return(output_file)\n}\nPar√°metros:\n\nProduccionDiarioDF: El DataFrame que contiene los datos de producci√≥n diaria.\n\n\n\nFunci√≥n Obtener √öltima Fecha de Actualizaci√≥n\n2.2 ObtenerUltimaFechaAct\nEsta funci√≥n ObtenerUltimaFechaAct() revisa los archivos de producci√≥n diaria almacenados en la carpeta de salida (./Output/) y devuelve la fecha m√°s reciente de los archivos guardados. Es √∫til para conocer la √∫ltima actualizaci√≥n de datos disponible en el sistema.\nObtenerUltimaFechaAct &lt;- function() {\n  archivo_total &lt;- './Output/Produccion_Diario_TOTAL.RData'\n  \n  if (!file.exists(archivo_total)) {\n    return(NULL) \n  }\n  \n  ProduccionDiario_TOTAL_DF &lt;- get(load(archivo_total))\n  \n  if (nrow(ProduccionDiario_TOTAL_DF) &gt; 0 && \"Jornada\" %in% colnames(ProduccionDiario_TOTAL_DF)) {\n    return(max(ProduccionDiario_TOTAL_DF$Jornada, na.rm = TRUE))\n  } else {\n    warning(\"El archivo total est√° vac√≠o o no contiene la columna 'Jornada'.\")\n    return(NULL)\n  }\n}\nPar√°metros: - No requiere par√°metros de entrada.\n\n\nFunci√≥n Filtrar Duplicados\n2.3 FiltrarDuplicadosTOTAL\nLa funci√≥n FiltrarDuplicadosTOTAL() elimina registros duplicados o no v√°lidos del conjunto de datos de producci√≥n, priorizando registros con incidencias, fechas de fin m√°s recientes y mayores cantidades buenas.\nFiltrarDuplicadosTOTAL &lt;- function(df) {\n  if (!dir.exists(\"./Output\")) {\n    dir.create(\"./Output\")\n  }\n  df &lt;- df %&gt;%\n    filter(!(Estado == \"PARO\" & is.na(Operario) & is.na(EXPEDIENTE) & is.na(REFERENCIA) & is.na(OF)))\n  \n  df &lt;- df %&gt;%\n    group_by(OF, FASE, EXPEDIENTE, REFERENCIA, MAQUINA, FechaInicio_pr, FechaFin_pr) %&gt;%\n    filter(!(is.na(MATERIAL_SALIDA) | MATERIAL_SALIDA == \"\")) %&gt;%\n    ungroup()\n  \n  df_filtrado &lt;- df %&gt;%\n    group_by(OF, FechaInicio_pr, Estado, FASE, MATERIAL_SALIDA) %&gt;% \n    mutate(IncidenciaPrioridad = !is.na(Incidencia),\n           OperarioSeleccionado = coalesce(first(Operario), NA)) %&gt;%  \n    slice_max(order_by = IncidenciaPrioridad, with_ties = TRUE) %&gt;% \n    slice_max(order_by = FechaFin_pr, with_ties = TRUE) %&gt;%\n    slice_max(order_by = Cantidad_Buenas, with_ties = FALSE) %&gt;%\n    select(-IncidenciaPrioridad, -OperarioSeleccionado) %&gt;%\n    ungroup()\n  \n  filas_eliminadas &lt;- nrow(df) - nrow(df_filtrado)\n  \n  cat(\"üîÑ Total de filas originales: \", nrow(df), \"\\n\")\n  cat(\"üéâ Total de filas despu√©s del filtrado: \", nrow(df_filtrado), \"\\n\")\n  cat(\"üî• Filas eliminadas: \", filas_eliminadas, \"\\n\")\n  \n  return(df_filtrado)\n}\nPar√°metros:\n\ndf: El DataFrame del que deseas eliminar los duplicados.\n\n\n\nFunci√≥n Filtrar Duplicados TOTAL\n2.3 FiltrarDuplicadosTOTALDIARIO\nLa funci√≥n FiltrarDuplicadosTOTALDIARIO() optimiza el proceso de filtrado de duplicados al trabajar solo con un subconjunto de los datos totales, enfoc√°ndose en los registros recientes para mejorar el rendimiento.\nFiltrarDuplicadosTOTALDIARIO &lt;- function(df_total, df_diario) {\n  n_filas_a_registrar &lt;- nrow(df_diario) * 2\n  filas_a_registrar &lt;- tail(df_total, n_filas_a_registrar)\n  filas_a_registrar &lt;- filas_a_registrar %&gt;%\n    filter(!(Estado == \"PARO\" & is.na(Operario) & is.na(EXPEDIENTE) & is.na(REFERENCIA) & is.na(OF)))\n  \n  filas_a_registrar &lt;- filas_a_registrar %&gt;%\n    group_by(OF, FASE, EXPEDIENTE, REFERENCIA, MAQUINA, FechaInicio_pr, FechaFin_pr) %&gt;%\n    filter(!(is.na(MATERIAL_SALIDA) | MATERIAL_SALIDA == \"\")) %&gt;%\n    ungroup()\n  \n  df_filtrado &lt;- filas_a_registrar %&gt;%\n    group_by(OF, FechaInicio_pr, Estado, FASE, MATERIAL_SALIDA) %&gt;% \n    mutate(IncidenciaPrioridad = !is.na(Incidencia),\n           OperarioSeleccionado = coalesce(first(Operario), NA)) %&gt;%  \n    slice_max(order_by = IncidenciaPrioridad, with_ties = TRUE) %&gt;% \n    slice_max(order_by = FechaFin_pr, with_ties = TRUE) %&gt;%\n    slice_max(order_by = Cantidad_Buenas, with_ties = FALSE) %&gt;%\n    select(-IncidenciaPrioridad, -OperarioSeleccionado) %&gt;%\n    ungroup()\n  \n  df_sin_filas_a_registrar &lt;- df_total %&gt;%\n    anti_join(filas_a_registrar, by = c(\"OF\", \"FechaInicio_pr\", \"Estado\", \"FASE\", \"MATERIAL_SALIDA\"))\n  df_actualizado &lt;- bind_rows(df_sin_filas_a_registrar, df_filtrado)\n  filas_eliminadas &lt;- nrow(df_total) - nrow(df_actualizado)\n  \n  cat(\"üîÑ Total de filas originales: \", nrow(df_total), \"\\n\")\n  cat(\"üéâ Total de filas despu√©s del filtrado: \", nrow(df_actualizado), \"\\n\")\n  cat(\"üî• Filas eliminadas o reemplazadas: \", filas_eliminadas, \"\\n\")\n  \n  \n  return(df_actualizado)\n}\nPar√°metros:\n\ndf_total: El DataFrame total que contiene todos los registros hist√≥ricos\ndf_diario: El DataFrame con los nuevos registros diarios\n\n\n\nFunci√≥n Filtrar Duplicados Total Diario\n2.5 Actualizar_Datos_ProduccionDiario\nActualizar_Datos_ProduccionDiario() recupera los datos de producci√≥n desde la base de datos, adaptando la consulta SQL seg√∫n exista o no una actualizaci√≥n previa, filtra duplicados y guarda los datos procesados.\nActualizar_Datos_ProduccionDiario &lt;- function(conn) {\n  fecha_ultima_actualizacion &lt;- ObtenerUltimaFechaAct() -1\n  ImprimirEncabezado(\"Datos de Produccion Diaria\")\n  \n  if (is.null(fecha_ultima_actualizacion)) {\n    consulta_sql &lt;- getSQLSQLServer(\"./SQL_Sentencias/Select_PRODUCCION.sql\")\n  } else {\n    consulta_sql &lt;- paste0(\n      \"SELECT  \n            SmFase.smFasePersonalizado1Txt AS 'EXPEDIENTE',\n            smOrdenNme AS 'REFERENCIA', \n            SmMaterial.smMaterialId AS 'MATERIAL_SALIDA', \n            SmMaterial.smMaterialNme AS 'MATERIAL_SALIDA_DESCR', \n            smOrdenId AS 'OF', \n            smFaseId AS 'FASE',\n            smFaseNme AS 'FASE_DESCR', \n            smNodo.smNodoId AS 'MAQUINA',  \n            smNodo.smNodoNme AS 'MAQ_DESC',\n            jornadaDte AS Jornada, \n            OLANET.ObtenerTurnoId(smCalendarioHorarioDetalle.smTurnoUid) AS Turno, \n            ISNULL(ISNULL(ISNULL(shNodoEstadoOperario.FechaInicioLocalDte, shNodoEstadoOperario.FechaInicioDte), ISNULL(shNodoEstadoFase.FechaInicioLocalDte, shNodoEstadoFase.FechaInicioDte)), ISNULL(shNodoEstado.FechaInicioLocalDte, shNodoEstado.FechaInicioDte)) AS FechaInicio_pr,  \n            ISNULL(ISNULL(ISNULL(ISNULL(shNodoEstadoOperario.FechaFinLocalDte, shNodoEstadoOperario.FechaFinDte), ISNULL(shNodoEstadoFase.FechaFinLocalDte, shNodoEstadoFase.FechaFinDte)), ISNULL(shNodoEstado.FechaFinLocalDte,shNodoEstado.FechaFinDte)),GETDATE()) AS FechaFin_pr,  \n            DATEDIFF(SECOND, isnull(ShNodoEstadoFase.fechaInicioDte, ShNodoEstado.fechaInicioDte), iSNULL((isnull(ShNodoEstadoFase.fechafinDte, ShNodoEstado.fechaFinDte)), GETDATE()) ) / 60.0 as Duracion_min, \n            DATEDIFF(SECOND, isnull(ShNodoEstadoFase.fechaInicioDte, ShNodoEstado.fechaInicioDte), iSNULL((isnull(ShNodoEstadoFase.fechafinDte, ShNodoEstado.fechaFinDte)), GETDATE()) ) / 3600.0 as Duracion_h, \n            SmMaterialLote.LoteTxt AS Lote,  \n            (ISNULL(isnull(ShNodoEstadoMaterialSalida.CantidadAutomaticaNbr,0)-isnull(ShNodoEstadoMaterialSalida.cantidadAutomaticaInicialNbr,0)+isnull(ShNodoEstadoMaterialSalida.CantidadManualNbr,0),0)) as Cantidad_Buenas, \n            (ISNULL(ShNodoEstadoMaterialSalidaCantidadRechazo.cantidadNbr,0)) as Cantidad_Malas, \n            CASE WHEN shNodoEstadoFase.NumeroOperariosPresentesNbr = 0 THEN NULL ELSE SmOperario.smOperarioId END as CodOperario, \n            SmOperario.smOperarioNme + ' ' + ISNULL(SmOperario.Apellido1Txt,'') + ' ' + ISNULL(SmOperario.Apellido2Txt,'') as Operario, \n            SmRol.SmRolId AS 'RolOperario',\n            smEstadoId AS Estado,\n            smIncidenciaNme AS Incidencia, \n            ShNodoEstado.observacionesInicioTxt AS ObservacionesInicio,\n            ShNodoEstado.observacionesFinTxt AS ObservacionesFin\n          FROM ShNodoTurno  WITH (NOLOCK) \n          INNER JOIN smNodo  WITH (NOLOCK) \n            ON shNodoTurno.smNodoUid = smNodo.smNodoUid  \n          INNER JOIN ShNodoEstado WITH (NOLOCK) \n            ON ShNodoTurno.ShNodoTurnoUid = ShNodoEstado.ShNodoTurnoUid  \n          INNER JOIN smEstado  WITH (NOLOCK) \n            ON shNodoEstado.smEstadoUid = SmEstado.SmEstadoUid  \n          LEFT JOIN smIncidencia  WITH (NOLOCK) \n            ON shNodoEstado.smIncidenciaUid = smIncidencia.smIncidenciaUid  \n          LEFT JOIN SmCalendarioHorarioDetalle  WITH (NOLOCK) \n            ON ShNodoTurno.smCalendarioHorarioDetalleUid = smCalendarioHorarioDetalle.smCalendarioHorarioDetalleUid  \n          LEFT JOIN ShNodoEstadoFase  WITH (NOLOCK) \n            ON ShNodoEstado.shNodoEstadoUid = shNodoEstadoFase.shNodoEstadoUid  \n          LEFT JOIN ShNodoEstadoOperario  WITH (NOLOCK) \n            ON ShNodoEstado.shNodoEstadoUid = ShNodoEstadoOperario.shNodoEstadoUid   \n          LEFT JOIN ShNodoTurnoOperario  WITH (NOLOCK) \n            ON shNodoTurnoOperario.shNodoTurnoOperarioUid = ShNodoEstadoOperario.shNodoTurnoOperarioUid  \n          LEFT JOIN smOperario  WITH (NOLOCK) \n            ON ShNodoTurnoOperario.smOperarioUid = smOperario.SmOperarioUid \n          LEFT JOIN SmOperarioRol WITH (NOLOCK) \n            ON SmOperario.SmOperarioUid = SmOperarioRol.SmOperarioUid \n          LEFT JOIN SmRol WITH (NOLOCK) \n            ON SmOperarioRol.SmRolUid = SmRol.SmRolUid \n          LEFT JOIN SmMotivoInterrupcion WITH (NOLOCK) \n            ON ShNodoEstadoFase.smMotivoInterrupcionUid = SmMotivoInterrupcion.smMotivoInterrupcionUid  \n          LEFT JOIN SmFase WITH (NOLOCK) \n            ON ShNodoEstadoFase.smFaseUid = SmFase.smFaseUid  \n          LEFT JOIN SmOrden WITH (NOLOCK) \n            ON SmFase.smOrdenUid = SmOrden.smOrdenUid \n          LEFT JOIN ShNodoEstadoMaterialSalida WITH (NOLOCK) \n            ON ShNodoEstado.shNodoEstadoUid = ShNodoEstadoMaterialSalida.shNodoEstadoUid  \n            AND shNodoEstadoFase.shNodoEstadoFaseUid = ShNodoEstadoMaterialSalida.shNodoEstadoFaseUid \n          LEFT JOIN smFaseMaterialSalida  WITH (NOLOCK) \n            ON ShNodoEstadoMaterialSalida.smFaseMaterialSalidaUid = SmFaseMaterialSalida.SmFaseMaterialSalidaUid  \n          LEFT JOIN SmMaterial WITH (NOLOCK) \n            ON smFaseMaterialSalida.smMaterialUid = smMaterial.smMaterialUid  \n          LEFT JOIN SmMaterialLote WITH (NOLOCK) \n            ON ShNodoEstadoMaterialSalida.smMaterialLoteUid = SmMaterialLote.smMaterialLoteUid  \n          LEFT JOIN ShNodoEstadoMaterialSalidaCantidadRechazo  WITH (NOLOCK) \n            ON ShNodoEstadoMaterialSalida.shNodoEstadoMaterialSalidaUid = ShNodoEstadoMaterialSalidaCantidadRechazo.shNodoEstadoMaterialSalidaUid  \n          LEFT JOIN SmMotivoRechazo WITH (NOLOCK) \n            ON ShNodoEstadoMaterialSalidaCantidadRechazo.smMotivoRechazoUid = SmMotivoRechazo.smMotivoRechazoUid \n          WHERE jornadaDte &gt;= '\", fecha_ultima_actualizacion, \"'\n          AND (shNodoEstadoFase.fechaInicioDte = ShNodoEstadoOperario.fechaInicioDte  \n          OR shNodoEstadoFase.fechaInicioDte IS NULL \n          OR ShNodoEstadoOperario.fechaInicioDte IS NULL) \n          AND smRolId = 'Operario'\n          \"\n    )\n  }\n  \n  Produccion_diario_df &lt;- EjecutarConsultaSQLServer(conn, consulta_sql)\n  Produccion_diario_df &lt;- FiltrarDuplicadosTOTAL(Produccion_diario_df)\n  \n  if (!is.null(Produccion_diario_df) && inherits(Produccion_diario_df, \"data.frame\") && nrow(Produccion_diario_df) &gt; 0) {\n    Produccion_diario_df$fecha_extraccion &lt;- Sys.Date()\n    \n    archivo_guardado &lt;- Guardar_Datos_con_Fecha(Produccion_diario_df)\n    cat(paste(\"‚ú® Datos nuevos guardados en:\", archivo_guardado, \"\\n\"))\n  } else if (is.null(Produccion_diario_df) || !inherits(Produccion_diario_df, \"data.frame\")) {\n    cat(\"‚ùå Error al recuperar datos: Verifica la consulta SQL o la conexi√≥n.\\n\")\n  } else {\n    cat(\"üîë No hay datos nuevos para guardar.\\n\")\n  }\n  \n}\nPar√°metros: No requiere par√°metros de entrada, pero depende de una conexi√≥n activa a la base de datos con.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Funciones Adicionales</span>"
    ]
  },
  {
    "objectID": "PreparacionDatos_Visualizacion.html",
    "href": "PreparacionDatos_Visualizacion.html",
    "title": "Preparacion Datos",
    "section": "",
    "text": "4_ProcesarDatos.R",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Preparacion Datos</span>"
    ]
  },
  {
    "objectID": "PreparacionDatos_Visualizacion.html#procesardatos.r",
    "href": "PreparacionDatos_Visualizacion.html#procesardatos.r",
    "title": "Preparacion Datos",
    "section": "",
    "text": "Procesamiento de Datos\nEste script se encarga de cargar, transformar, fusionar y enriquecer los datos\n\nCarga de Datos\nFunci√≥n Inicial: El script inicia cargando los datos b√°sicos de producci√≥n desde archivos CSV:\nOlanet_Producido &lt;- CargarCSV(\"./OlanetDatos/Olanet_Producido.csv\")\nOlanet_Planificado &lt;- CargarCSV(\"./OlanetDatos/Olanet_Planificado.csv\")\n\n\nLimpieza y Transformaci√≥n Inicial\nNormalizaci√≥n de identificadores: Se normalizan los campos EXPEDIENTE y OF para asegurar que sean num√©ricos:\nOlanet_Producido$EXPEDIENTE &lt;- as.numeric(gsub(\"[^0-9]\", \"\", Olanet_Producido$EXPEDIENTE))\nOlanet_Planificado$EXPEDIENTE &lt;- as.numeric(gsub(\"[^0-9]\", \"\", Olanet_Planificado$EXPEDIENTE))\n\nOlanet_Producido$OF &lt;- as.numeric(gsub(\"[^0-9]\", \"\", Olanet_Producido$OF))\nOlanet_Planificado$OF &lt;- as.numeric(gsub(\"[^0-9]\", \"\", Olanet_Planificado$OF))\nEsta normalizaci√≥n elimina cualquier car√°cter no num√©rico y convierte los valores a tipo num√©rico, facilitando las operaciones posteriores.\n\n\nAgregaci√≥n de Datos Producidos\nCreaci√≥n de resumen por claves: Se agregan los datos de producci√≥n por claves principales:\nOlanet_Producido_Agg &lt;- Olanet_Producido %&gt;%\n  mutate(\n    FechaInicio_pr = as.POSIXct(FechaInicio_pr, format = \"%Y-%m-%d %H:%M:%OS\"),\n    FechaFin_pr = as.POSIXct(FechaFin_pr, format = \"%Y-%m-%d %H:%M:%OS\")\n  ) %&gt;%\n  group_by(EXPEDIENTE, REFERENCIA, OF, FASE, MAQUINA) %&gt;%\n  summarise(...)\nEste proceso:\n\nConvierte fechas al formato POSIXct para manipulaci√≥n temporal\nAgrupa por las claves principales de negocio\nCalcula m√©tricas agregadas como fechas m√≠nimas/m√°ximas y sumas de cantidades\n\n\n\nCalculo de Tiempos Operativos\nTiempos de Producci√≥n Se calculan los tiempos efectivos de producci√≥n:\nDF_En_Produccion &lt;- Olanet_Producido %&gt;%\n  filter(Estado == \"PRODUCCION\") %&gt;%  \n  group_by(OF, FASE, MAQUINA, REFERENCIA, EXPEDIENTE) %&gt;%\n  summarise(\n    Tiempo_Total_Produccion_min = sum(Duracion_min, na.rm = TRUE), \n    .groups = \"drop\"\n  )\nTiempos de Preparaci√≥n Se calculan los tiempos de preparaci√≥n:\nDF_En_Preparacion &lt;- Olanet_Producido %&gt;%\n  filter(Estado == \"PREPARACION\") %&gt;%  \n  group_by(OF, FASE, MAQUINA, REFERENCIA, EXPEDIENTE) %&gt;%\n  summarise(\n    TiempoRealPreparacion_min = sum(Duracion_min, na.rm = TRUE), \n    .groups = \"drop\"\n  )\nEstos c√°lculos separan los tiempos por tipo de actividad para an√°lisis detallados.\n\n\nFusi√≥n y C√°lculo de KPIs\nCreaci√≥n del Dataset Completo Se fusionan los datos planificados y reales, calculando m√©tricas de rendimiento:\npr_COMPLETO &lt;- Olanet_Planificado %&gt;%\n  mutate(\n    FechaInicio_Plani = as.POSIXct(paste(FechaInicio_Plani, \"00:00:00\"), format = \"%Y-%m-%d %H:%M:%S\"),\n    FechaFin_Plani = as.POSIXct(paste(FechaFin_Plani, \"00:00:00\"), format = \"%Y-%m-%d %H:%M:%S\")\n  ) %&gt;%\n  left_join(Olanet_Producido_Agg, by = c(\"EXPEDIENTE\", \"REFERENCIA\", \"OF\", \"FASE\", \"MAQUINA\")) %&gt;%\n  left_join(DF_En_Produccion, by = c(\"EXPEDIENTE\", \"REFERENCIA\", \"OF\", \"FASE\", \"MAQUINA\")) %&gt;%\n  left_join(DF_En_Preparacion, by = c(\"EXPEDIENTE\", \"REFERENCIA\", \"OF\", \"FASE\", \"MAQUINA\")) %&gt;% \n  mutate(\n    FechaInicio_pr_trunc = as.Date(FechaInicio_pr),\n    FechaFin_pr_trunc = as.Date(FechaFin_pr),\n    FechaInicio_Plani_trunc = as.Date(FechaInicio_Plani),\n    FechaFin_Plani_trunc = as.Date(FechaFin_Plani),\n    \n    DiasDiferenciaInicio = as.numeric(FechaInicio_pr_trunc - FechaInicio_Plani_trunc),\n    DiasDiferenciaFin = as.numeric(FechaFin_pr_trunc - FechaFin_Plani_trunc),\n    \n    CantidadPendiente = CantidadPrevista - CantidadRealizada,\n    CompletadoCantidad = ifelse(CantidadPrevista &gt; 0, coalesce(round(CantidadRealizada / CantidadPrevista, 2), 0), 0),\n    \n    TiempoProduccionReal_min = TiempoReal_min,\n    TiempoProduccionPlanificado_min = (TiempoPreparacion*CantidadPrevista) / 60 + (TiempoTeorico*60),\n    \n    TiempoPrevistoPreparacionUnitario_min = TiempoPreparacion,\n    \n    TiempoRealPreparacion_min = coalesce(TiempoRealPreparacion_min, 0), \n    TiempoPrevistoPreparacion_min = (TiempoPreparacion * CantidadPrevista) / 60 ,\n\n    TiempoPrevistoProduccion_min = TiempoTeorico*60,\n    TiempoProduccion_min = Tiempo_Total_Produccion_min,\n    \n    TiempoPrevistoUnitario_min = TiempoCiclo, \n    TiempoProducidoUnitario_min = ifelse(CantidadRealizada &gt; 0, Tiempo_Total_Produccion_min / CantidadRealizada, NA),\n\n    Disponibilidad = ifelse(Tiempo_Total_Produccion_min &gt; 0, format(round((TiempoReal_min / Tiempo_Total_Produccion_min) * 100, 2), decimal.mark = \",\"), NA),  \n    Calidad = ifelse((CantidadRealizada + CantidadMalas) &gt; 0, (CantidadRealizada / (CantidadRealizada + CantidadMalas)) * 100, NA),\n    Productividad =ifelse(CantidadPrevista &gt; 0, format(round((CantidadRealizada / CantidadPrevista) * 100, 2), decimal.mark = \",\"), NA)         \n  ) %&gt;%\n  select(\n    EXPEDIENTE, REFERENCIA, OF, FASE, FASE_DESCR, MATERIAL_ENTRADA, MATERIAL_ENTRADA_DESCR,MATERIAL_SALIDA, MAQUINA, MAQ_DESC, ESTADO_OLANET, \n    FechaInicio_Plani, FechaInicio_pr, DiasDiferenciaInicio, FechaFin_Plani, FechaFin_pr, DiasDiferenciaFin, \n    CantidadPrevista, CantidadRealizada, CantidadPendiente, CompletadoCantidad, CantidadMalas, \n    TiempoProduccionPlanificado_min,TiempoProduccionReal_min, \n    TiempoPrevistoPreparacionUnitario_min, \n    TiempoPrevistoPreparacion_min, TiempoRealPreparacion_min, \n    TiempoPrevistoProduccion_min, TiempoProduccion_min, \n    TiempoPrevistoUnitario_min, TiempoProducidoUnitario_min\n  )\npr_COMPLETO &lt;- pr_COMPLETO[, !(names(pr_COMPLETO) %in% c(\"MATERIAL_ENTRADA\", \"MATERIAL_ENTRADA_DESCR\"))]\npr_COMPLETO &lt;- unique(pr_COMPLETO)\nC√°lculos de KPIs importantes:\n\nDiferencias en d√≠as entre fechas planificadas y reales\nCantidades pendientes y porcentajes de completitud\nTiempos de producci√≥n real vs.¬†planificado\nIndicadores de rendimiento: Disponibilidad, Calidad y Productividad\n\n\n\nEnriquecimiento para Producci√≥n\nEnriquecimiento del Dataset Original Se a√±aden datos de planificaci√≥n al dataset original:\nOlanet_Planificado_Key &lt;- pr_COMPLETO %&gt;%\n  select(OF, FASE, MAQUINA, REFERENCIA, EXPEDIENTE, CantidadPrevista, \n         TiempoProduccionPlanificado_min, TiempoPrevistoPreparacion_min, \n         TiempoPrevistoProduccion_min) %&gt;%\n  distinct()\n\nOlanet_Producido &lt;- Olanet_Producido %&gt;%\n  left_join(Olanet_Planificado_Key, by = c(\"OF\", \"FASE\", \"MAQUINA\", \"REFERENCIA\", \"EXPEDIENTE\"))\n\n\nGuardar Datos creados\nExportaci√≥n de Resultados Los datasets procesados se guardan en archivos CSV:\nGuardarCSV(Olanet_Producido, \"Olanet_Producido\", \"./OlanetDatos\")\nGuardarCSV(pr_COMPLETO, \"Olanet_Completo\", \"./OlanetDatos\")",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Preparacion Datos</span>"
    ]
  },
  {
    "objectID": "PreparacionDatos_Visualizacion.html#crearmaestros.r",
    "href": "PreparacionDatos_Visualizacion.html#crearmaestros.r",
    "title": "Preparacion Datos",
    "section": "5_CrearMaestros.R",
    "text": "5_CrearMaestros.R\n\nCreaci√≥n de Maestros Identificadores para Modelo en Estrella\nEste script genera tablas maestras para la gesti√≥n de identificadores y categor√≠as.\n\nCarga de Datos Base\nCarga del Dataset Planificado Se carga el dataset de planificaci√≥n para extraer datos maestros:\nOlanet_Planificado &lt;- CargarCSV(\"./OlanetDatos/Olanet_Planificado.csv\")\n\n\nGeneraci√≥n de Tablas Maestras\nExtracci√≥n de Identificadores √önicos Se crean tablas maestras para las principales entidades con registros unicos:\nMaestro_Maquina &lt;- Olanet_Planificado %&gt;%\n  select(MAQUINA, MAQ_DESC) %&gt;%\n  drop_na()  %&gt;% \n  distinct()\nMaestro_Expediente &lt;- Olanet_Planificado %&gt;%\n  select(EXPEDIENTE) %&gt;%\n  drop_na()  %&gt;% \n  distinct()\nMaestro_OF &lt;- Olanet_Planificado %&gt;%\n  select(OF) %&gt;%\n  drop_na() %&gt;%\n  distinct() \nMaestro_Referencia &lt;- Olanet_Planificado %&gt;%\n  select(REFERENCIA) %&gt;%\n  drop_na() %&gt;%\n  distinct() \nMaestro_EstadoOlanet &lt;- Olanet_Planificado %&gt;%\n  select(ESTADO_OLANET) %&gt;%\n  drop_na()  %&gt;% \n  distinct()\n\n\nExportaci√≥n de Maestros\nGuardado de Tablas Maestras Se utiliza un bucle para guardar todas las tablas maestras:\nXX_MAESTROS_OLANET &lt;- list(\n  Maestro_Maquina = \"Maestro_Maquina\",\n  Maestro_Expediente = \"Maestro_Expediente\",\n  Maestro_OF = \"Maestro_OF\",\n  Maestro_Referencia = \"Maestro_Referencia\",\n  Maestro_EstadoOlanet = \"Maestro_EstadoOlanet\"\n)\n\nfor (nombre in names(XX_MAESTROS_OLANET)) {\n  GuardarCSV(get(nombre), XX_MAESTROS_OLANET[[nombre]], \"./OlanetDatos/MaestrosIdentificadores\")\n}",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Preparacion Datos</span>"
    ]
  },
  {
    "objectID": "Visualizaci√≥n_de_Datos.html",
    "href": "Visualizaci√≥n_de_Datos.html",
    "title": "Visualizaci√≥n de Datos",
    "section": "",
    "text": "Instalaci√≥n de Grafana\nPara facilitar la visualizaci√≥n de los datos en tiempo real, instal√© Grafana. Esta herramienta permite crear paneles interactivos y analizar la producci√≥n de manera din√°mica.\nInicialmente, dise√±√© una visualizaci√≥n simple para probar distintas herramientas. Sin embargo, encontr√© ciertas limitaciones, como la imposibilidad de aplicar filtros y algunos inconvenientes en la configuraci√≥n.",
    "crumbs": [
      "Visualizaci√≥n",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Visualizaci√≥n de Datos</span>"
    ]
  },
  {
    "objectID": "Visualizaci√≥n_de_Datos.html#power-bi",
    "href": "Visualizaci√≥n_de_Datos.html#power-bi",
    "title": "Visualizaci√≥n de Datos",
    "section": "Power BI",
    "text": "Power BI\nEn paralelo, utilic√© Power BI para generar visualizaciones m√°s avanzadas. Los archivos .CSV generados fueron importados en Power BI para realizar an√°lisis detallados y gr√°ficos m√°s sofisticados, dado que Grafana no soportaba ciertos tipos de visualizaciones necesarias.\nGracias a Power BI, pudimos aplicar filtros din√°micos para mejorar la presentaci√≥n de los datos y optimizar su an√°lisis.\nPara probar distintas opciones, creamos varios tipos de gr√°ficos que facilitaron la exploraci√≥n y comprensi√≥n de los datos de Lacor.",
    "crumbs": [
      "Visualizaci√≥n",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Visualizaci√≥n de Datos</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html",
    "href": "VisualizacionPBI.html",
    "title": "Visualizaci√≥n en Power Bi",
    "section": "",
    "text": "Uso Power BI\nTras evaluar varias opciones, llegamos a la conclusi√≥n de que Power BI era la herramienta m√°s adecuada para el proyecto. Esto se debi√≥ a varias razones clave:\nEn resumen, Power BI no solo ofrece un alto grado de personalizaci√≥n y facilidad de uso, sino que tambi√©n es capaz de escalar y adaptarse a las necesidades espec√≠ficas del proyecto a medida que evoluciona.",
    "crumbs": [
      "Visualizaci√≥n",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Visualizaci√≥n en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html#uso-power-bi",
    "href": "VisualizacionPBI.html#uso-power-bi",
    "title": "Visualizaci√≥n en Power Bi",
    "section": "",
    "text": "Facilidad de Conexi√≥n a Datos: Power BI permite una integraci√≥n sencilla con bases de datos como MySQL, facilitando la extracci√≥n y visualizaci√≥n de grandes vol√∫menes de datos.\nVisualizaci√≥n Intuitiva: Ofrece una amplia variedad de opciones de visualizaci√≥n interactivas, lo que facilita la creaci√≥n de dashboards atractivos y f√°ciles de interpretar para los usuarios finales.\nTransformaci√≥n de Datos: Aunque la transformaci√≥n de datos puede realizarse de forma m√°s compleja que en otras herramientas, Power BI ofrece un entorno robusto para preparar los datos a trav√©s de Power Query, lo que permite personalizar las transformaciones seg√∫n las necesidades del negocio junto a sus medidas DAX.\nInteractividad y Filtros: Una de las caracter√≠sticas m√°s destacadas es la capacidad de aplicar filtros y segmentar los datos en tiempo real, lo que permite obtener insights espec√≠ficos de manera instant√°nea.\nEscalabilidad: A medida que el volumen de datos crezca, Power BI es capaz de manejar grandes cantidades de informaci√≥n sin perder rendimiento, lo que es esencial para el monitoreo continuo en un entorno de producci√≥n.",
    "crumbs": [
      "Visualizaci√≥n",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Visualizaci√≥n en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html#obtencion-de-datos",
    "href": "VisualizacionPBI.html#obtencion-de-datos",
    "title": "Visualizaci√≥n en Power Bi",
    "section": "Obtencion de Datos",
    "text": "Obtencion de Datos\nInicialmente, para realizar pruebas, cargamos los datos mediante archivos .csv. Sin embargo, una vez que el servidor estuvo operativo y la estructura de los datos fue definida correctamente, pasamos a conectarnos directamente a la base de datos MySQL previamente configurada.\nPara ello, fue necesario instalar un conector compatible con Windows:\nüîó Descarga Conector MySQL\n\nmysql-connector-net-9.2.0\n\nCon la conexi√≥n establecida y los datos integrados en Power BI, comenzamos con las primeras comprobaciones y la definici√≥n de relaciones entre tablas.",
    "crumbs": [
      "Visualizaci√≥n",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Visualizaci√≥n en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html#primeros-pasos",
    "href": "VisualizacionPBI.html#primeros-pasos",
    "title": "Visualizaci√≥n en Power Bi",
    "section": "Primeros Pasos",
    "text": "Primeros Pasos\nUna vez identificados los datos disponibles, procedimos a dise√±ar un Power BI optimizado para su uso futuro. Esta visualizaci√≥n destaca por su interfaz intuitiva y su formato tipo aplicaci√≥n, con paneles interactivos que permiten aplicar filtros y m√©tricas para una mejor interpretaci√≥n de la informaci√≥n.\nUno de los principales desaf√≠os fue la correcta relaci√≥n entre las tablas, ya que Power BI no proporciona claves for√°neas de manera autom√°tica. Para solucionar esto, fue necesario crear tablas maestras que permitieran establecer las relaciones adecuadas.\n\n\n\nTras estructurar y relacionar correctamente los datos, desarrollamos una visualizaci√≥n din√°mica adaptada a las necesidades de la empresa. El objetivo principal era ofrecer una vista diferenciada de las fases Preparadas y los que estaban en Producci√≥n, permitiendo un mayor control sobre ambos procesos.\nPara mejorar la preparaci√≥n de los datos en R, cre√© varias tablas con diferentes c√°lculos, medidas y filtros, lo que ampli√≥ las posibilidades de visualizaci√≥n y an√°lisis.",
    "crumbs": [
      "Visualizaci√≥n",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Visualizaci√≥n en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html#visualizaci√≥n",
    "href": "VisualizacionPBI.html#visualizaci√≥n",
    "title": "Visualizaci√≥n en Power Bi",
    "section": "Visualizaci√≥n",
    "text": "Visualizaci√≥n\n\nVisualizaci√≥n Inicial\nEn las primeras fases del proyecto, se propuso una visualizaci√≥n tipo tabla que, si bien conten√≠a toda la informaci√≥n necesaria, no resultaba adecuada para una interpretaci√≥n r√°pida y eficaz de los datos de producci√≥n. La presentaci√≥n era densa y poco intuitiva, lo que complicaba el an√°lisis y la toma de decisiones tanto para el equipo t√©cnico como para el de producci√≥n. Este enfoque inicial sirvi√≥, sin embargo, para comprender mejor las necesidades espec√≠ficas del equipo de calidad y producci√≥n, lo que permiti√≥ evolucionar hacia un modelo de visualizaci√≥n m√°s din√°mico, interactivo y orientado al usuario.\n\n\n\n\n\nVisualizacion Final\nFinalmente, tras llegar a un entendimiento con el equipo de producci√≥n, se plante√≥ una nueva secuencia de visualizaci√≥n con un dise√±o m√°s atractivo, similar a una aplicaci√≥n. De esta forma, se pudieron detallar las distintas fases del proceso y detectar errores con mayor facilidad.\nEl dashboard final se organiza en cuatro apartados principales, dise√±ados para ofrecer un acceso r√°pido y una comprensi√≥n detallada del estado de la producci√≥n, los operarios y las incidencias. La navegaci√≥n es intuitiva, con men√∫s y opciones de filtrado que permiten al usuario explorar la informaci√≥n en distintos niveles de profundidad, desde vistas generales hasta an√°lisis espec√≠ficos.\nA continuaci√≥n, se describe cada uno de los apartados:\n\nInicio\nLa secci√≥n de Inicio proporciona una visi√≥n global del desempe√±o de la empresa. Aqu√≠ se presenta un balance comparativo entre el a√±o actual y el anterior, mostrando indicadores clave como el volumen de piezas buenas producidas. Este panel act√∫a como el punto de partida del dashboard, desde el cual los usuarios pueden navegar hacia los an√°lisis m√°s espec√≠ficos mediante un men√∫ lateral interactivo.\n-   Comparativa a√±o actual vs a√±o anterior.\n\n-   Resumen de producci√≥n anual.\n\n-   Acceso r√°pido a los m√≥dulos de an√°lisis detallado.\n\n\n\n\n\nAn√°lisis de L√≠neas:\nEn este apartado se ofrece una vista general del rendimiento de todas las l√≠neas de producci√≥n. Cada l√≠nea es evaluada en tres aspectos fundamentales:\nCalidad\n&gt; F√≥rmula:\n&gt; Calidad = Total de Piezas Buenas / Total de Piezas\n\nEficiencia\n&gt; F√≥rmula:\n&gt; Eficiencia = Tiempo de Producci√≥n / Tiempo Total\n\nProductividad\n&gt; F√≥rmula:\n&gt; Productividad = (Tiempo Unitario Te√≥rico √ó Total de Piezas Realizadas) / (Tiempo Unitario de Producci√≥n √ó Total de Piezas Realizadas)\nPara facilitar la interpretaci√≥n r√°pida de los resultados, se utilizan c√≥digos de color:\n-   üî¥ Rojo: indicadores entre 0% y 20% .\n\n-   üü† Naranja: indicadores entre 20% y 40% .\nOpciones de An√°lisis: El usuario tiene la posibilidad de personalizar el rango temporal del an√°lisis mediante diversos filtros din√°micos:\n```         \n- D√≠a actual: an√°lisis centrado en la jornada de hoy.\n\n- √öltimo d√≠a trabajado: evaluaci√≥n del d√≠a m√°s reciente con actividad productiva.\n\n- Rango de Jornadas: selecci√≥n libre de un intervalo de fechas para an√°lisis hist√≥rico o comparativo.\n```\nEstos filtros permiten adaptar el an√°lisis seg√∫n las necesidades espec√≠ficas del momento, ya sea en tiempo real o para estudios retrospectivos.\n\n\n\n\n\n\nExploraci√≥n Detallada mediante Drillthrough: Una de las funcionalidades m√°s potentes de este apartado es el uso del drillthrough. El drillthrough permite al usuario profundizar en el an√°lisis de un elemento espec√≠fico, en este caso, una m√°quina concreta perteneciente a una l√≠nea de producci√≥n.\n¬øC√≥mo funciona el Drillthrough?\n\nIdentificaci√≥n: El usuario, tras identificar una l√≠nea de producci√≥n con indicadores preocupantes o interesantes, puede examinar la lista de m√°quinas asociadas.\nSelecci√≥n: A trav√©s de un click derecho sobre la m√°quina de inter√©s, se despliega un men√∫ contextual.\nAcceso: Seleccionando la opci√≥n de ‚ÄúDrillthrough‚Äù, el usuario es redirigido autom√°ticamente al m√≥dulo ‚ÄúM√°quinas‚Äù.\n\nAn√°lisis detallado: En la p√°gina ‚ÄúM√°quinas‚Äù, se presenta un desglose pormenorizado del desempe√±o de esa m√°quina espec√≠fica, incluyendo:\n  - Tiempos de producci√≥n y tiempos de parada. \n  - Referencias trabajadas. \n  - N√∫mero de piezas buenas y piezas malas \n  - Incidencias registradas. \n  - C√°lculo de OEE de la m√°quina.\nEsta funcionalidad transforma el an√°lisis de un nivel macro (l√≠nea) a un nivel micro (m√°quina individual), permitiendo un enfoque de diagn√≥stico m√°s √°gil y efectivo.\n\n\n\nAdem√°s, desde la vista detallada de m√°quinas, se puede realizar un segundo drillthrough hacia el an√°lisis por referencia, donde se profundiza a√∫n m√°s para identificar:\n  - Qu√© referencias espec√≠ficas provocaron problemas.\n\n  - Qu√© tipo de incidencias se registraron.\n\n  - Qu√© operarios intervinieron en cada proceso.\n\n\n\n\n\n\nEsta navegaci√≥n en cascada facilita el seguimiento y resoluci√≥n de problemas desde el nivel m√°s general hasta el m√°s espec√≠fico.\n\n\nAn√°lisis de Operarios:\nEsta secci√≥n proporciona una visi√≥n integral del desempe√±o de los operarios en el proceso productivo. Se presentan m√©tricas consolidadas de todos los trabajadores, permitiendo detectar r√°pidamente diferencias de desempe√±o entre ellos.\nEl usuario puede seleccionar un operario espec√≠fico y, a trav√©s de un click derecho, acceder a la p√°gina ‚ÄúDetallesOperario‚Äù, donde se ofrece un desglose detallado de su actividad en la fecha o rango seleccionado:\n  - M√°quinas en las que ha trabajado.\n\n  - Referencias realizadas.\n\n  - N√∫mero de piezas buenas y malas\n\n  - Incidencias asociadas.\n\n  - Tiempos de producci√≥n y preparaci√≥n.\n\n  - C√°lculo de su OEE \nEste an√°lisis facilita identificar operarios que requieren apoyo o formaci√≥n adicional, as√≠ como destacar aquellos de mejor rendimiento.\n\n\n\n\n\n\n\n\nAn√°lisis de Incidencias:\nEl √∫ltimo apartado se centra en el an√°lisis detallado de las incidencias registradas durante la producci√≥n. Esta secci√≥n est√° dise√±ada para detectar patrones recurrentes de fallos en m√°quinas, l√≠neas, turnos o referencias espec√≠ficas.\nEl usuario puede aplicar filtros din√°micos para:\n-   Seleccionar por jornada o intervalo de fechas.\n\n-   Filtrar por turno de trabajo.\n\n-   Analizar incidencias por l√≠nea o por m√°quina.\nPara cada incidencia, se detallan:\n-   Tipo de incidencia ocurrida.\n\n-   Tiempo perdido asociado.\n\n-   Operario responsable.\nEste enfoque permite no solo corregir errores puntuales, sino tambi√©n implementar acciones de mejora continua basadas en datos hist√≥ricos y tendencias.",
    "crumbs": [
      "Visualizaci√≥n",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Visualizaci√≥n en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionGrafana.html",
    "href": "VisualizacionGrafana.html",
    "title": "Visualizaci√≥n en Grafana",
    "section": "",
    "text": "Uso de Grafana\nComo mencion√© anteriormente, al principio la visualizaci√≥n en Grafana era bastante limitada para el proyecto inicial. Sin embargo, logramos adaptar la herramienta para crear un sistema de paneles de visualizaci√≥n enfocado en el monitoreo en tiempo real de las l√≠neas de producci√≥n.\nAdemas, Grafana est√° alojado en el servidor configurado para este proyecto, el mismo en el que se almacenan los datos de producci√≥n, lo que facilita un acceso r√°pido, seguro y centralizado tanto a los paneles de visualizaci√≥n como a la base de datos MySQL.",
    "crumbs": [
      "Visualizaci√≥n",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Visualizaci√≥n en Grafana</span>"
    ]
  },
  {
    "objectID": "VisualizacionGrafana.html#conexi√≥n-a-base-de-datos",
    "href": "VisualizacionGrafana.html#conexi√≥n-a-base-de-datos",
    "title": "Visualizaci√≥n en Grafana",
    "section": "Conexi√≥n a Base de Datos",
    "text": "Conexi√≥n a Base de Datos\nAl igual que en Power BI, nos conectamos directamente a la base de datos MySQL. En este caso, Grafana obtiene y env√≠a los datos mediante consultas SELECT. Sin embargo, a diferencia de Power BI, las transformaciones de los datos son m√°s complicadas en Grafana, lo que a√±ade una capa de complejidad a la hora de trabajar con los datos.\n\nVisualizaci√≥n\nPara este proyecto, creamos un panel/dashboard para cada l√≠nea de fabricaci√≥n, mostrando los datos relevantes de manera clara y concisa. La visualizaci√≥n incluye los siguientes elementos:\n\nTabla Iniciadas:\nEsta tabla muestra el estado de la m√°quina, con un c√≥digo de colores para facilitar la interpretaci√≥n:\n\nAmarillo: Incidencia\nVerde: Producci√≥n\nAzul: Preparaci√≥n\n\nAdem√°s, se incluyen los siguientes datos:\n-   Expediente\n-   Referencia\n-   Orden de Fabricaci√≥n (OF)\n-   Fase\n-   M√°quina\n-   Fecha de inicio de la producci√≥n\n-   Cantidad prevista\n-   Cantidad realizada\n-   Un **% de cantidad completada** con una barra de progreso\n-   Un **% de tiempo completado**, representado con una barra de carga similar\n\n\nTabla por REFERENCIA: Piezas Buenas y Malas:\nOtra tabla que muestra las piezas buenas y malas, agrupadas por referencia.\n\n\nTotal:\nUn gr√°fico tipo ‚Äúpie chart‚Äù que muestra el total de piezas buenas frente a las piezas malas, facilitando la visualizaci√≥n de la proporci√≥n entre ambos.\n\n\nPlano del Taller Interactivo:\nCon el objetivo de mejorar a√∫n m√°s la visualizaci√≥n y localizaci√≥n de los estados de las m√°quinas dentro del entorno productivo, se cre√≥ un plano interactivo del taller utilizando el plugin HTML Graphics de Grafana.\nEste plano incluye:\n\nUbicaci√≥n de las m√°quinas sobre el mapa del taller, mostrando de manera precisa su disposici√≥n real.\nCambio de color din√°mico de cada m√°quina en el plano, en funci√≥n de su estado operativo como en la Tabla Iniciadas\nDelimitaci√≥n de l√≠neas de producci√≥n: Cada grupo de m√°quinas pertenecientes a una l√≠nea de producci√≥n espec√≠fica est√° rodeado por l√≠neas en movimiento que indican visualmente el √°rea que ocupa cada l√≠nea dentro del taller.\nActualizaci√≥n en tiempo real: Los estados de las m√°quinas y las l√≠neas en el plano se actualizan autom√°ticamente gracias a la sincronizaci√≥n constante con la base de datos.\n\nEsta integraci√≥n no solo proporciona un acceso visual inmediato a la situaci√≥n de cada m√°quina, sino que tambi√©n facilita la gesti√≥n y supervisi√≥n r√°pida del entorno productivo, permitiendo actuar de manera inmediata ante cualquier incidencia.\n\n\n \n\n\n\n\n\nImplementacion\nEste sistema de visualizaci√≥n mediante Grafana ha sido dise√±ado para ser mostrado en pantallas informativas distribuidas en la f√°brica, lo que proporciona al personal de producci√≥n una informaci√≥n clara, accesible y actualizada en tiempo real.\nMediante la funci√≥n playlist de Grafana, los diferentes paneles de cada l√≠nea de producci√≥n se rotan autom√°ticamente cada pocos segundos, permitiendo que toda la informaci√≥n relevante est√© disponible de manera c√≠clica sin necesidad de intervenci√≥n manual.\nActualmente se monitorizan las siguientes l√≠neas:\n\n| ALUMINIO | DOM√âSTICO | ENTALLADORAS | INOXIDABLE |\n\nA medida que nuevas m√°quinas se integren en el proceso productivo, sus datos se incorporar√°n autom√°ticamente a este sistema de visualizaci√≥n, ampliando progresivamente el alcance del monitoreo en planta.",
    "crumbs": [
      "Visualizaci√≥n",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Visualizaci√≥n en Grafana</span>"
    ]
  }
]