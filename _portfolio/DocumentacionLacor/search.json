[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LacorDual Sandra Caeiro",
    "section": "",
    "text": "Preparación",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "index.html#comprensión-del-funcionamiento-de-la-producción",
    "href": "index.html#comprensión-del-funcionamiento-de-la-producción",
    "title": "LacorDual Sandra Caeiro",
    "section": "Comprensión del Funcionamiento de la Producción",
    "text": "Comprensión del Funcionamiento de la Producción\nPara iniciar este proyecto, fue esencial entender cómo funciona el proceso de producción en Lacor. Esto me permitió diseñar soluciones adecuadas para la gestión y visualización de los datos. En este sentido, la observación de los flujos de trabajo, los tiempos de producción, y los datos generados a lo largo de cada etapa, fueron clave para la toma de decisiones sobre la mejor manera de estructurar el sistema.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "index.html#objetivos-del-proyecto",
    "href": "index.html#objetivos-del-proyecto",
    "title": "LacorDual Sandra Caeiro",
    "section": "Objetivos del Proyecto",
    "text": "Objetivos del Proyecto\nEl proyecto se enfoca en optimizar los procesos industriales mediante el análisis de datos planificados y producidos. Se estructura en torno a dos objetivos principales:\n1- Desarrollo de un sistema integral para el análisis de secuencias productivas:\n\nComparación entre la planificación y la producción real\nIdentificación de desviaciones para la mejora continua\nOptimización de recursos y procesos industriales\n\n2- Implementación de una solución automatizada que permita:\n\nExtracción diaria de datos de planificación y producción\nLimpieza y validación de los datos históricos\nAlmacenamiento centralizado en una base de datos\nVisualización clara y accesible para facilitar la toma de decisiones",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "index.html#configuración-del-entorno",
    "href": "index.html#configuración-del-entorno",
    "title": "LacorDual Sandra Caeiro",
    "section": "Configuración del Entorno",
    "text": "Configuración del Entorno\nLa primera fase consistió en crear una máquina virtual con Ubuntu 24.04.1. En ella, instalé las herramientas necesarias para trabajar en este proyecto:\n\nR 4.4.2: Herramienta principal para la manipulación y análisis de datos.\nRStudio: Entorno de desarrollo integrado (IDE) utilizado para trabajar con R.\nPython 3.10.16: Utilizado para scripts adicionales y la integración con otras herramientas.\nGitHub Desktop: Para el control de versiones y la colaboración en el proyecto.\nVisual Studio Code: Utilizado principalmente para el desarrollo de scripts en Python y otros lenguajes.\n\n\n    \n\nEste entorno proporcionó una base robusta y flexible, permitiendo realizar pruebas y ajustes en las soluciones de manera eficiente. Además, la configuración de la máquina virtual permitió que los recursos del sistema se aislaran, mejorando el rendimiento y la seguridad de los procesos.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "index.html#metodología-de-trabajo",
    "href": "index.html#metodología-de-trabajo",
    "title": "LacorDual Sandra Caeiro",
    "section": "Metodología de Trabajo",
    "text": "Metodología de Trabajo\nEl proyecto se desarrolla siguiendo una metodología iterativa, basada en etapas definidas que garantizan un avance ordenado y validado:\n1- Extracción automatizada:\n\nImplementación de sistemas que recojan datos de planificación y producción de forma diaria\nEstandarización del formato de entrada de datos\n\n2- Limpieza y comprobación de datos:\n\nDetección y resolución de duplicados e inconsistencias\nValidación de la calidad de los datos históricos recopilados\n\n3- Centralización:\n\nCreación de una base de datos estructurada\nAutomatización de los procesos de carga y actualización de información\n\n4- Visualización:\n\nDesarrollo de gráficos y dashboards sencillos\nFacilitar la interpretación y el análisis por parte de los usuarios finales\nEste enfoque modular permite construir una solución robusta y escalable, orientada a maximizar el valor de los datos industriales.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "Automatizacion_Proceso.html",
    "href": "Automatizacion_Proceso.html",
    "title": "Automatización del Proceso",
    "section": "",
    "text": "Automatización de la Carga de Datos\nEstos scripts están diseñados para gestionar las conexiones necesarias y ejecutar de forma automatizada las funciones del script XX_LLAMADA_PRODUCCION.R, garantizando una actualización eficiente de los datos.\nEl script XX_LLAMADA_PRODUCCION.R se ejecuta automáticamente en segundo plano cada día a las siguientes horas;\nLa tarea programada en cron se configura en el sistema/servidor de Ubuntu editando el crontab con:\ne insertando la siguiente línea:\nAdemás, es necesario otorgar permisos de ejecución al script con:\nAsi nos aseguramos una carga de datos continua y sin intervención manual. Esta configuración optimiza el flujo de trabajo y mantiene la información siempre actualizada.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Automatización del Proceso</span>"
    ]
  },
  {
    "objectID": "Automatizacion_Proceso.html#automatización-de-la-carga-de-datos",
    "href": "Automatizacion_Proceso.html#automatización-de-la-carga-de-datos",
    "title": "Automatización del Proceso",
    "section": "",
    "text": "-6:00 hasta las 22:00 cada 15 mins\n\ncrontab -e\n\n# === Actualizacion Produccion Lacor === #\n\n# Este Script se ejecutara cada **15 minutos**, desde las **05:00 hasta las 22:45**, en las siguientes horas: **05:00, 05:15, 05:30, 05:45, ... hasta las 22:45**.  \n\n*/15 5-22 * * * cd /home/dataml/Documentos/GitHub/Lacor && /usr/bin/Rscript Scripts/XX_LLAMADA_PRODUCCION.R\n\nchmod +x /home/scaeiro/GIT/Lacor/Scripts/XX_LLAMADA_PRODUCCION.R\n\n\nAutomatizacion del Script:\nEl proceso de ejecución automática del script sigue los siguientes pasos:\n\nCarga de configuración y funciones: Se importan los archivos de configuración y las funciones necesarias.\nActualización desde GitHub: Se ejecuta PullGitHub() para sincronizar el código.\nConexión a bases de datos: Se establecen conexiones con SQL Server y MySQL.\nConsulta y procesamiento de datos: Se ejecutan consultas SQL y se realiza la limpieza y transformación de datos.\nCarga de datos a MySQL: Los datos procesados se almacenan en la base de datos MySQL.\nLiberación de recursos: Se desconectan las bases de datos y se limpia el entorno de R.\n\n\nXX_LLAMADA_PRODUCCION.R\n#==============================================================================#\n#                     PRODUCCION LACOR SISTEMA AUTOMATIZADO                    #\n#          SISTEMA DE GESTION, MONITORIZACION Y ANÁLISIS DE DATOS              #\n#                         DESARROLLADO POR: SANDRA CAEIRO                      #\n#==============================================================================#\n#  FUNCIONES PRINCIPALES:                                                      #\n#  - Integración y análisis de datos de producción.                            #\n#  - Cálculo automático de métricas clave.                                     #\n#  - Optimización de procesos mediante automatización.                         #\n#                                                                              #\n#  HERRAMIENTAS UTILIZADAS:                                                    #\n#  - R para procesamiento de datos y generación de informes.                   #\n#  - SQL para la extracción y actualización de datos en la base de datos.      #\n#  - Automatización de flujos de trabajo de producción.                        #\n#==============================================================================#\n\nsource('./Scripts/0_Cargar_Configuracion.R')\nsource('./Scripts/0_Funciones_Guardado.R')\nsource('./Scripts/0_Funciones_GestionDatos.R')\nsource('./Scripts/1_SQLServer_Base.R')\nsource('./Scripts/1_MySQL_Base.R')\n\nImprimirInicioLog()\n\n#===========================================================#\n#                        PULL GITHUB                        #\n#===========================================================#\n\nPullGitHub()\n\n#===========================================================#\n#                    ABRIR CONEXIONES                       #\n#===========================================================#\n\n# SQL SERVER\nconn &lt;- DameConexionSQLServer(FicheroConf = './Config/Config_PERSONAL.json', \n                              TipoDBElegido = 'SQLServer', AliasElegido = 'DBA', \n                              DominioElegido = 'Remoto', DB_conexion = 'Olanet_Lacor')\nconn2 &lt;- DameConexionSQLServer(FicheroConf = './Config/Config_PERSONAL.json', \n                              TipoDBElegido = 'SQLServer', AliasElegido = 'DBA', \n                              DominioElegido = 'Remoto', DB_conexion = 'Olanet_Lacor_Datos')\n# MYSQL\nMcon &lt;- DameConexionMysql(FicheroConf = './Config/Config_PERSONAL.json',\n                          TipoDBElegido = 'MySQL', AliasElegido ='SUPER',\n                          DominioElegido='Local', DB_conexion = 'LacorDatos')\n\n#===========================================================#\n#           CONSULTA Produccion DIARIO TOTAL SQL            #\n#===========================================================#\nImprimirEncabezado(\"CONSULTAS SQL SERVER\")\n\n# Produccion_query &lt;- getSQLSQLServer(\"./SQL_Sentencias/Select_PRODUCCION.sql\")\n# DF_Produccion_TOTAL &lt;- EjecutarConsultaSQLServer(conn, Produccion_query)\n# DF_Produccion_TOTAL&lt;- FiltrarDuplicadosTOTAL(DF_Produccion_TOTAL)\n# GuardarRData(DF_Produccion_TOTAL,\"./Output/Produccion_TOTAL.RData\")\n\n# == CONSULTA Produccion OF CANTIDAD SQL == #\nProduccion_OFCantidad_query &lt;- getSQLSQLServer(\"./SQL_Sentencias/Select_OF_Cantidad.sql\")\nDF_ProduccionOFCantidad &lt;- EjecutarConsultaSQLServer(conn, Produccion_OFCantidad_query)\nif (!dir.exists(\"./Output/DatosPrevistos\")) {\n  dir.create(\"./Output/DatosPrevistos\", recursive = TRUE)\n}\nGuardarRData(DF_ProduccionOFCantidad,  \"./Output/DatosPrevistos/Datos_OF_Cantidad.RData\")\n\n# == CONSULTA OLANET == #\nOlanetDatos_query &lt;- getSQLSQLServer(\"./SQL_Sentencias/Select_PLANIFICADO.sql\")\nDF_OlanetCOMPLETO &lt;- EjecutarConsultaSQLServer(conn2, OlanetDatos_query)\nGuardarCSV(DF_OlanetCOMPLETO, \"Olanet_Planificado\", \"./Output/CSV\") \n\nrm(OlanetDatos_query)\nrm(Produccion_OFCantidad_query)\n\n#===========================================================#\n#                FUSION Y LIMPIEZA DE DATOS                 #\n#===========================================================#\n\nsource('./Scripts/2_Mantener_DatosActualizados.R')\nsource('./Scripts/3_Trasformacion_Y_Limpieza.R')\nsource('./Scripts/4_ProcesarDatos.R')\nsource('./Scripts/5_CrearMaestros.R')\n\n#===========================================================#\n#                   CARGAR DATOS A MYSQL                    #\n#===========================================================#\n\n# Carga a MySQL\n# Tablas Principales\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/Olanet_Completo.csv\", DB_Tabla = \"Olanet_Completo\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/Olanet_Planificado.csv\", DB_Tabla = \"Olanet_Planificado\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/Olanet_Producido.csv\", DB_Tabla = \"Olanet_Producido\", Mcon = Mcon)\n\n# Maestros\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_Referencia.csv\", DB_Tabla = \"Maestro_Referencia\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_OF.csv\", DB_Tabla = \"Maestro_OF\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_Maquina.csv\", DB_Tabla = \"Maestro_Maquina\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_Expediente.csv\", DB_Tabla = \"Maestro_Expediente\", Mcon = Mcon)\nCargarCSVMySQL(archivoCSV = \"/home/dataml/Documentos/GitHub/Lacor/OlanetDatos/MaestrosIdentificadores/Maestro_EstadoOlanet.csv\", DB_Tabla = \"Maestro_EstadoOlanet\", Mcon = Mcon)\n\n# Desconexion MySQl\ndbDisconnectAllMySQL()\n\n#===========================================================#\n#                        PUSH GITHUB                        #\n#===========================================================#\n\n#PushGitHub()\n\n#===========================================================#\n#                     ELIMINAR ENTORNO                      #\n#===========================================================#\nImprimirEncabezado(\"Eliminar entorno de R\")\nrm(list = ls())\ngc()\n\n# === LOGS === #\nsink()\n# ============ #",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Automatización del Proceso</span>"
    ]
  },
  {
    "objectID": "Automatizacion_Proceso.html#ajustes-en-el-log-de-datos",
    "href": "Automatizacion_Proceso.html#ajustes-en-el-log-de-datos",
    "title": "Automatización del Proceso",
    "section": "Ajustes en el Log de Datos",
    "text": "Ajustes en el Log de Datos\nSe realizaron ajustes en los logs para asegurar una correcta visualización de los datos y una mejor trazabilidad de los procesos. Los logs se añadieron para las llamadas de los scripts, lo que permite monitorear el proceso de actualización y garantizar que los datos se estén cargando correctamente en cada ejecución. Asi de esta menera podemos detectar errores con mucha mas facilidad. Estos estan dentro de la carpeta /Logs/ dentro del repositorio.\n\nEjemplo de Log:\n#===================================================================================================#\n#===================================================================================================#\n#___________________________________________________________________________________________________#\n#_______________________________________🍳 PRODUCCION LACOR 🍳______________________________________#\n#            📅 Fecha actual: 2025-03-25                                                            #\n#            🕒 Hora actual: 09:14:17                                                               #\n#                                                                                                   #\n#===================================================================================================#\n#===================================================================================================#\n\n#==============================================================================#\n                            Cargar Datos GitHub\n#==============================================================================#\n\n😺 Realizando GitPull para cargar el repositorio...\nYa está actualizado. \nRepositorio actualizado desde GitHub a las:  1742890459 \n#==============================================================================#\n                            CONEXIÓN A SQL SERVER\n#==============================================================================#\n\n✅ CONEXIÓN A SQL SERVER ESTABLECIDA ✅\n📍 Host: 192.168.100.214\n👤 Usuario: datoslim\n💾 Base de datos: Olanet_Lacor\n#==============================================================================#\n                            CONEXIÓN A SQL SERVER\n#==============================================================================#\n\n✅ CONEXIÓN A SQL SERVER ESTABLECIDA ✅\n📍 Host: 192.168.100.214\n👤 Usuario: datoslim\n💾 Base de datos: Olanet_Lacor_Datos\n#==============================================================================#\n                            CONEXIÓN A MYSQL\n#==============================================================================#\n\n✅ CONEXIÓN A MYSQL ESTABLECIDA ✅\n📍 Host: localhost\n👤 Usuario: LacorIbili\n💾 Base de datos: LacorDatos\n#==============================================================================#\n                            CONSULTAS SQL SERVER\n#==============================================================================#\n\n📝 SENTENCIA LEÍDA DESDE EL ARCHIVO .SQL 📝\n SELECT       [smOrdenId],       [SmfaseId],       [smMaterialId],       [smMaterialnme],       [CantidadaProducirnbr]   FROM [Olanet_Lacor].[Integration].[OutputMaterials]; \n✅ Consulta ejecutada con éxito! ✅\n📊 Resultados obtenidos: 2806 filas.\n✅ Archivo guardado: ' ./Output/DatosPrevistos/Datos_OF_Cantidad.RData ' - Objeto:  DF_ProduccionOFCantidad  - Registros:  2806 \n\n📝 SENTENCIA LEÍDA DESDE EL ARCHIVO .SQL 📝\n SELECT      Olanet.SmFase.smFasePersonalizado1Txt AS 'EXPEDIENTE',     Olanet.SmOrden.smOrdenDsc AS 'REFERENCIA',     Olanet.smMaterial.smMaterialId AS 'MATERIAL_ENTRADA',     Olanet.smMaterial.smMaterialNme AS 'MATERIAL_ENTRADA_DESCR',     Olanet.SmOrden.smOrdenId AS 'OF',     Olanet.SmFase.smFaseId AS 'FASE',     Olanet.SmFase.smFaseNme AS 'FASE_DESCR',     Olanet.smNodo.smNodoId AS 'MAQUINA',     Olanet.smNodo.smNodoNme AS 'MAQ_DESC',     Olanet.SmFase.ssEnumFaseEstadoId AS 'ESTADO_OLANET',     Secuenciador.SmFaseNodo.FechaEstimacionInicioDte AS \"FechaInicio_Plani\",     Secuenciador.SmFaseNodo.FechaEstimacionFinDte AS \"FechaFin_Plani\",     Olanet.SmFase.TiempoCicloNbr AS \"TiempoCiclo\",     Olanet.SmFase.TiempoPreparacionNbr AS \"TiempoPreparacion\",     Secuenciador.SmFaseNodo.duracionTeoricaNbr AS \"TiempoTeorico\" FROM     Olanet.SmOrden JOIN     Olanet.SmFase     ON Olanet.SmOrden.smOrdenUid = Olanet.SmFase.smOrdenUid JOIN     Olanet.smNodo     ON Olanet.SmFase.smNodoUid = Olanet.smNodo.smNodoUid JOIN     Secuenciador.SmFaseNodo     ON Olanet.SmFase.smFaseUid = Secuenciador.SmFaseNodo.smFaseUid JOIN     Olanet.SmFaseMaterialEntrada     ON Olanet.SmFase.smFaseUid = Olanet.SmFaseMaterialEntrada.smFaseUid JOIN     Olanet.SmMaterial     ON Olanet.SmFaseMaterialEntrada.smMaterialUid = Olanet.SmMaterial.SmMaterialUid      \n✅ Consulta ejecutada con éxito! ✅\n📊 Resultados obtenidos: 3861 filas.\n✅ Guardado correctamente: ./Output/CSV/Olanet_Planificado.csv - Registros: 3861 \n#==============================================================================#\n                            Datos de Produccion Diaria\n#==============================================================================#\n✅ Consulta ejecutada con éxito! ✅\n📊 Resultados obtenidos: 334 filas.\n🔄 Total de filas originales:  334 \n🎉 Total de filas después del filtrado:  264 \n🔥 Filas eliminadas:  70 \n✨ Datos nuevos guardados en: ./Output/Produccion_Diario_DIA_2025-03-25.RData \n#==============================================================================#\n                            CERRAR CONEXIONES SQLSERVER\n#==============================================================================#\n\n🌟✨ CERRANDO CONEXIONES A SQL SERVER... ✨🌟\n✅ CONEXIONES SQL SERVER CERRADAS.\n#==============================================================================#\n                            Actualización de Datos\n#==============================================================================#\n🔄 Iniciando la Fusión de Datos para actualizar el archivo:\n✅ El archivo RData ' ./Output/Produccion_Diario_DIA_2025-03-25.RData ' se cargó correctamente. Contiene:  ProduccionDiarioDF \n✅ El archivo RData ' ./Output/Produccion_TOTAL.RData ' se cargó correctamente. Contiene:  DF_Produccion_TOTAL \n\n🔍 Verificando la estructura de los datos...\n\n🚮 Eliminando filas duplicadas (si las hay)...\n🔄 Total de filas originales:  41108 \n🎉 Total de filas después del filtrado:  40844 \n🔥 Filas eliminadas o reemplazadas:  264 \n\n🔄 Sobrescribiendo el archivos actualizado...\n✅ Archivo guardado: ' ./Output/Produccion_Diario_DIA_2025-03-25.RData ' - Objeto:  DF_ProduccionDiario  - Registros:  264 \n✅ Archivo guardado: ' ./Output/Produccion_TOTAL.RData ' - Objeto:  DF_Produccion_TOTAL  - Registros:  40844 \n\n💾 Guardando el archivo CSV actualizado en la carpeta Output...\n✅ Guardado correctamente: ./Output/CSV/Produccion_TOTAL.csv - Registros: 40844 \n#==============================================================================#\n                            Trasformaciones y Limpiezas de Datos\n#==============================================================================#\n🔄 Iniciando la Limpieza de Datos de OLANET:\n✅ El archivo RData ' ./Output/DatosPrevistos/Datos_OF_Cantidad.RData ' se cargó correctamente. Contiene:  DF_ProduccionOFCantidad \n✅ El archivo CSV ' ./Output/CSV/Produccion_TOTAL.csv ' se cargó correctamente. Registros:  40844  - Columnas:  26 \n✅ El archivo CSV ' ./Output/CSV/Olanet_Planificado.csv ' se cargó correctamente. Registros:  3861  - Columnas:  15 \n\n🔍 Columnas y Nombres reorganizados correctamente. \n✅ Guardado correctamente: ./OlanetDatos/Olanet_Planificado.csv - Registros: 3861 \n✅ Guardado correctamente: ./OlanetDatos/Olanet_Producido.csv - Registros: 35949 \n#==============================================================================#\n                            Procesar Datos\n#==============================================================================#\n✅ El archivo CSV ' ./OlanetDatos/Olanet_Producido.csv ' se cargó correctamente. Registros:  35949  - Columnas:  26 \n✅ El archivo CSV ' ./OlanetDatos/Olanet_Planificado.csv ' se cargó correctamente. Registros:  3861  - Columnas:  16 \n✅ Guardado correctamente: ./OlanetDatos/Olanet_Completo.csv - Registros: 3861 \n#==============================================================================#\n                            Creacion de MAESTROS OLANET PLANIFICADO\n#==============================================================================#\n✅ El archivo CSV ' ./OlanetDatos/Olanet_Planificado.csv ' se cargó correctamente. Registros:  3861  - Columnas:  16 \n\n🔢 Creando Maestros Identificadores... \n✅ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_Identificador.csv - Registros: 467 \n✅ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_Maquina.csv - Registros: 9 \n✅ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_Expediente.csv - Registros: 273 \n✅ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_OF.csv - Registros: 1010 \n✅ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_Referencia.csv - Registros: 378 \n✅ Guardado correctamente: ./OlanetDatos/MaestrosIdentificadores/Maestro_EstadoOlanet.csv - Registros: 4 \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\n🔢 Se van a cargar 3861 filas desde el archivo CSV \n\n🧹 Eliminando registros previos:  3861 \n\n📈 Se han cargado 3861 filas en la tabla 'Olanet_Completo'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\n🔢 Se van a cargar 3861 filas desde el archivo CSV \n\n🧹 Eliminando registros previos:  3861 \n\n📈 Se han cargado 3861 filas en la tabla 'Olanet_Planificado'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\n🔢 Se van a cargar 35949 filas desde el archivo CSV \n\n🔢 Creando la tabla y cargando los datos...\n\n✅ Tabla creada y datos cargados con éxito.\n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\n🔢 Se van a cargar 377 filas desde el archivo CSV \n\n🧹 Eliminando registros previos:  377 \n\n📈 Se han cargado 377 filas en la tabla 'Maestro_Referencia'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\n🔢 Se van a cargar 1010 filas desde el archivo CSV \n\n🧹 Eliminando registros previos:  1010 \n\n📈 Se han cargado 1010 filas en la tabla 'Maestro_OF'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\n🔢 Se van a cargar 9 filas desde el archivo CSV \n\n🧹 Eliminando registros previos:  9 \n\n📈 Se han cargado 9 filas en la tabla 'Maestro_Maquina'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\n🔢 Se van a cargar 467 filas desde el archivo CSV \n\n🧹 Eliminando registros previos:  467 \n\n📈 Se han cargado 467 filas en la tabla 'Maestro_Identificador'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\n🔢 Se van a cargar 273 filas desde el archivo CSV \n\n🧹 Eliminando registros previos:  273 \n\n📈 Se han cargado 273 filas en la tabla 'Maestro_Expediente'. \n#==============================================================================#\n                            Import del CSV a MySQL\n#==============================================================================#\n\n🔢 Se van a cargar 4 filas desde el archivo CSV \n\n🧹 Eliminando registros previos:  4 \n\n📈 Se han cargado 4 filas en la tabla 'Maestro_EstadoOlanet'. \n#==============================================================================#\n                            CERRAR CONEXIONES MYSQL\n#==============================================================================#\n\n🔒 CERRANDO CONEXIONES MYSQL...\nCONEXIONES MYSQL CERRADAS: 1 conexión(es) cerrada(s)\n#==============================================================================#\n                            Actualizar Datos GitHub\n#==============================================================================#\n\n😺 Realizando GitPush para actualizar el repositorio...\nCambios subidos a GitHub a las:  1742890473 \n#==============================================================================#\n                            Eliminar entorno de R\n#==============================================================================#\n          used (Mb) gc trigger  (Mb) max used  (Mb)\nNcells 1297512 69.3    2466342 131.8  2466342 131.8\nVcells 3596967 27.5   14786712 112.9 14786712 112.9",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Automatización del Proceso</span>"
    ]
  },
  {
    "objectID": "Conexión_a_la_Base_de_Datos.html",
    "href": "Conexión_a_la_Base_de_Datos.html",
    "title": "Conexion a la Base de Datos",
    "section": "",
    "text": "Conexión a Microsoft SQL Server\nEsta fase consistió en realizar pruebas de conexión a la base de datos utilizando R. Para ello, primero agregué el repositorio de Microsoft e instalé el controlador ODBC para SQL Server:\n🔗 Descarga ODBC Driver 17 para SQL Server\nCon esta configuración, creé un script base que configura la conexión y ejecuta consultas SQL para obtener los datos de consumo diario.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conexion a la Base de Datos</span>"
    ]
  },
  {
    "objectID": "Conexión_a_la_Base_de_Datos.html#conexión-a-microsoft-sql-server",
    "href": "Conexión_a_la_Base_de_Datos.html#conexión-a-microsoft-sql-server",
    "title": "Conexion a la Base de Datos",
    "section": "",
    "text": "ODBC Driver 17 for SQL Server\n\n\n\n1_SQLServer_Base.R\nEste script 1_SQLServer_Base.R sirve para crear las conexiones y desconexiones con la base de datos SQL y generar un archivo TOTAL que se actualice a diario, obteniendo las producciones diarias y acumulándolas.\n\nDetalles del Script\n1.1. Conexión a la Base de Datos:\nEl script utiliza la función DameConexionSQLServer para establecer la conexión a la base de datos. La función toma como parámetros la configuración almacenada en un archivo JSON, el tipo de base de datos, y detalles sobre el alias, dominio y base de datos a conectar.\nconn &lt;- DameConexionSQLServer(FicheroConf = './Config/Config_PERSONAL.json', \n                              TipoDBElegido = 'SQLServer', AliasElegido = 'DBA', \n                              DominioElegido = 'Remoto', DB_conexion = 'Olanet_Lacor')\n1.2. Desconexión a la Base de Datos:\nEl script también incluye una función dbDisconnectAllSQLServer para cerrar las conexiones abiertas a la base de datos, garantizando que no queden conexiones abiertas al finalizar el proceso.\ndbDisconnectAllSQLServer()\n1.3. Ejecutar Consultas SQL:\nLa función EjecutarConsultaSQLServer ejecuta una consulta SQL sobre la base de datos. Toma la conexión y la sentencia SQL como parámetros, ejecuta la consulta y devuelve los resultados en formato de data frame.\nproduccion_query &lt;- getSQLSQLServer(\"Select_ProduccionDiario.sql\")\nproduccion_diaria &lt;- EjecutarConsultaSQLServer(conn, produccion_query)\n1.4. Leer Sentencias SQL desde un Archivo::\nLa función getSQLSQLServer lee un archivo .sql y lo convierte en una sentencia ejecutable en SQL Server.\nsql_sentencia &lt;- getSQLSQLServer(\"path/to/your/sqlfile.sql\")",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conexion a la Base de Datos</span>"
    ]
  },
  {
    "objectID": "Conexión_a_la_Base_de_Datos.html#conexión-a-mysql",
    "href": "Conexión_a_la_Base_de_Datos.html#conexión-a-mysql",
    "title": "Conexion a la Base de Datos",
    "section": "Conexión a MYSQL",
    "text": "Conexión a MYSQL\nSe creó el script 1_MySQL_Base.R para importar los datos de producción a MySQL. La función asociada actualiza automáticamente la base de datos, eliminando los datos anteriores y cargando los nuevos registros del archivo .CSV.\nAntes de nada al igual que en SQL Server deberemos instalar el driver de MySQL:\n🔗 Descarga MySQL Conector NET 9.2.0\n\nMySQL Conector NET 9.2.0\n\n\n1_MySQL_Base.R\n\nDetalles del Script\n1.1. DameConexionMysql:\nEsta función establece una conexión con la base de datos MySQL utilizando los parámetros definidos en un archivo de configuración JSON. En caso de éxito, devuelve la conexión; si ocurre un error, maneja la excepción y muestra un mensaje de advertencia.\nMcon &lt;- DameConexionMysql(FicheroConf = './Config/Config_PERSONAL.json',\n                          AliasElegido ='SUPER', \n                          DominioElegido='Local')\n1.2. dbDisconnectAllMySQL:\nEsta función se encarga de cerrar todas las conexiones abiertas a MySQL, asegurando que no queden conexiones activas al finalizar el proceso.\ndbDisconnectAllMySQL()\n1.3. CargarCSVMySQL:\nEsta función se encarga de cargar un archivo .CSV en una tabla de MySQL. Si la tabla ya existe, elimina los datos previos y luego carga los nuevos registros. Si la tabla no existe, la crea y luego realiza la carga de datos.\nCargarCSVMySQL(Mcon = Mcon)",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conexion a la Base de Datos</span>"
    ]
  },
  {
    "objectID": "Tratamiento_Y_Limpieza.html",
    "href": "Tratamiento_Y_Limpieza.html",
    "title": "Tratamiento y Limpieza de Datos",
    "section": "",
    "text": "2_Mantener_DatosActualizados.R",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tratamiento y Limpieza de Datos</span>"
    ]
  },
  {
    "objectID": "Tratamiento_Y_Limpieza.html#mantener_datosactualizados.r",
    "href": "Tratamiento_Y_Limpieza.html#mantener_datosactualizados.r",
    "title": "Tratamiento y Limpieza de Datos",
    "section": "",
    "text": "Carga y Actualización de Datos:\nEste script está diseñado para actualizar y fusionar datos de producción, asegurando que los archivos se mantengan al día con la información más reciente.\n\nCreacio del RData Diario Antes de iniciar el proceso sacamos los datos del mismo dia y cerramos conexiones ya que la consulta ya esta realizada.\n  #===========================================================#\n  #         CREAR RDATA DIARIO Y CERRAR CONEXIONES            #\n  #===========================================================#\n  Actualizar_Datos_ProduccionDiario(conn)\n  # Desconexion SQLServer\n  dbDisconnectAllSQLServer()\nCargar los DataFrames: El primer paso es cargar los archivos de producción diaria y total para el día actual. Usamos Sys.Date() para obtener la fecha actual y luego cargamos los archivos correspondientes.\n  # Obtener la fecha actual 📅\n  fecha_actual &lt;- Sys.Date()\n\n  archivo_diario &lt;- paste0(\"./Output/Produccion_Diario_DIA_\", fecha_actual, \".RData\")\n  archivo_total &lt;- \"./Output/Produccion_TOTAL.RData\"\n\n  cat(paste(\"🔄 Iniciando la Fusión de Datos para actualizar el archivo:\\n\"))\n  DF_ProduccionDiario &lt;- CargarRData(archivo_diario)\n  DF_Produccion_TOTAL &lt;- CargarRData(archivo_total)\nFusión de Datos: Una vez cargados los datos, se verifica la estructura de los DataFrames y se ajustan las columnas para garantizar que ambos DataFrames tengan el mismo formato. Esto se hace eliminando la última columna del DataFrame diario y renombrando las columnas para que coincidan con el DataFrame total.\n# Verificación de la estructura de los datos y renombrado de columnas\ncolumnas_Produccion_diario_df &lt;- colnames(ProduccionDiarioDF)\ncolumnas_Produccion_total_df &lt;- colnames(ProduccionDiario_TOTAL_DF)\nProduccionDiarioDF &lt;- ProduccionDiarioDF[, -ncol(ProduccionDiarioDF)]\nnames(ProduccionDiarioDF) &lt;- columnas_Produccion_total_df\nEliminación de Duplicados: Después de fusionar los datos, se eliminan posibles filas duplicadas utilizando la función FiltrarDuplicadosTOTALDIARIO para garantizar que los datos sean únicos y no haya redundancias.\n# Eliminación de filas duplicadas\nProduccionDiario_TOTAL_DF &lt;- FiltrarDuplicadosTOTALDIARIO(ProduccionDiario_TOTAL_DF, ProduccionDiarioDF)\nGuardar y Sobrescribir los Datos: Finalmente, los datos fusionados y limpios se guardan en los archivos correspondientes, tanto en formato .RData como en .CSV para su posterior uso y análisis.\n# Guardar archivos actualizados\nGuardarRData(ProduccionDiarioDF, archivo_diario)\nGuardarRData(ProduccionDiario_TOTAL_DF, archivo_total)\nGuardarCSV(ProduccionDiario_TOTAL_DF, \"Produccion_Diario_TOTAL\", \"./Output/CSV\")",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tratamiento y Limpieza de Datos</span>"
    ]
  },
  {
    "objectID": "Tratamiento_Y_Limpieza.html#trasformacion_y_limpieza.r",
    "href": "Tratamiento_Y_Limpieza.html#trasformacion_y_limpieza.r",
    "title": "Tratamiento y Limpieza de Datos",
    "section": "3_Trasformacion_Y_Limpieza.R",
    "text": "3_Trasformacion_Y_Limpieza.R\n\nTransformaciones y Limpiezas de Datos:\nEste script realiza la transformación y limpieza de varios DataFrames relacionados con los datos tanto de Producción como lo Planificado.\n\nCarga de Datos:\nEl primer paso consiste en cargar los datos de las diferentes fuentes, incluyendo archivos .RData y .CSV. Esto proporciona los datos necesarios para realizar las transformaciones posteriores.\ncat(paste(\"🔄 Iniciando la Limpieza de Datos de OLANET:\\n\"))\nDF_CANTIDAD &lt;-CargarRData(\"./Output/DatosPrevistos/Datos_OF_Cantidad.RData\")\nDF_PRODUCIDO &lt;- CargarCSV(\"./Output/CSV/Produccion_TOTAL.csv\")\nDF_PLANIFICADO &lt;- CargarCSV(\"./Output/CSV/Olanet_Planificado.csv\")\n\n\nTranformaciones y Limpieza:\n\nProcesamiento de Datos Planificados con Cantidades: Se fusiona el DataFrame DF_PLANIFICADO con la información de cantidades previstas de DF_CANTIDAD. Esto permite enriquecer los datos planificados con información sobre las cantidades que se esperan producir.\n       DF_PLANIFICADO_CANTIDAD &lt;- merge(DF_PLANIFICADO, DF_CANTIDAD[, c(\"smOrdenId\", \"SmfaseId\", \"smMaterialId\", \"smMaterialnme\", \"CantidadaProducirnbr\")], \n                                    by.x = c(\"OF\", \"FASE\"), \n                                    by.y = c(\"smOrdenId\", \"SmfaseId\"), \n                                    all.x = TRUE)\n\n    colnames(DF_PLANIFICADO_CANTIDAD)[colnames(DF_PLANIFICADO_CANTIDAD) == \"CantidadaProducirnbr\"] &lt;- \"CantidadPrevista\"\n    DF_PLANIFICADO_CANTIDAD &lt;- DF_PLANIFICADO_CANTIDAD[, -c(ncol(DF_PLANIFICADO_CANTIDAD)-1, ncol(DF_PLANIFICADO_CANTIDAD)-2)]\nFiltrado de Datos Planificados: Se aplica un filtro para eliminar referencias que comienzan con “MTO” o que contienen espacios, lo que asegura que solo se consideren referencias válidas en el análisis.\n   DF_PLANIFICADO_CANTIDAD &lt;- DF_PLANIFICADO_CANTIDAD %&gt;%\n  filter(!grepl(\"^MTO\", REFERENCIA) & !grepl(\"\\\\s\", REFERENCIA))\nLimpieza de Datos de Producción: Se procesan los datos de producción, asegurando que el campo OF contiene solo valores numéricos válidos y filtrando aquellos que empiezan con “24-”. Además, se eliminan las filas que no tienen valor en MATERIAL_SALIDA.\n  DF_PRODUCIDO &lt;- DF_PRODUCIDO %&gt;%\n  filter(!is.na(OF) & grepl(\"^[0-9]+$\", OF)) %&gt;%\n  mutate(OF = as.numeric(OF)) %&gt;%\n  filter(!grepl(\"^24-\", OF))\nDF_PRODUCIDO &lt;- DF_PRODUCIDO[!is.na(DF_PRODUCIDO$MATERIAL_SALIDA), ]\n\n\n\nGuardar los Archivos Resultantes:\nDespués de realizar todas las transformaciones y limpiezas necesarias, los DataFrames resultantes se guardan en archivos .CSV para su posterior análisis y uso.\n# Guardar los DataFrames resultantes en CSV\nGuardarCSV(DF_PLANIFICADO_CANTIDAD,\"Olanet_Planificado\",\"./OlanetDatos\")\nGuardarCSV(DF_PRODUCIDO,\"Olanet_Producido\",\"./OlanetDatos\")",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tratamiento y Limpieza de Datos</span>"
    ]
  },
  {
    "objectID": "Tratamiento_Y_Limpieza.html#resumen-final",
    "href": "Tratamiento_Y_Limpieza.html#resumen-final",
    "title": "Tratamiento y Limpieza de Datos",
    "section": "Resumen Final",
    "text": "Resumen Final\nEn resumen, este conjunto de scripts realiza las siguientes operaciones:\n\nCarga de datos de diferentes fuentes.\nFusión de datos, eliminación de duplicados y valores no validos\nGuardado de los datos procesados en archivos .CSV para su uso posterior.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tratamiento y Limpieza de Datos</span>"
    ]
  },
  {
    "objectID": "Funciones_Adicionales.html",
    "href": "Funciones_Adicionales.html",
    "title": "Funciones Adicionales",
    "section": "",
    "text": "Configuración Inicial",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Funciones Adicionales</span>"
    ]
  },
  {
    "objectID": "Funciones_Adicionales.html#configuración-inicial",
    "href": "Funciones_Adicionales.html#configuración-inicial",
    "title": "Funciones Adicionales",
    "section": "",
    "text": "0_Cargar_Configuracion.R\nEs el primer Script que se carga por lo tanto antes de hacer nada cargamos todas las librerias necesarias, si no las tenemos se nos instalaran automaticamente:\noptions(repos = c(CRAN = \"https://cran.r-project.org\"))\noptions(scipen = 999)  \n#==============================================================================#\nif (! (\"jsonlite\" %in% rownames(installed.packages()))) { install.packages(\"jsonlite\") }\nlibrary(jsonlite)\nif (! (\"dplyr\" %in% rownames(installed.packages()))) { install.packages(\"dplyr\") }\nlibrary(dplyr)\nif (!requireNamespace(\"tidyr\", quietly = TRUE)) {install.packages(\"tidyr\")}\nlibrary(tidyr)  \nif (!(\"RODBC\" %in% rownames(installed.packages()))) { install.packages(\"RODBC\") }\nlibrary(RODBC)\n#==============================================================================#\nif (! (\"RMySQL\" %in% rownames(installed.packages()))) { install.packages(\"RMySQL\") }\nlibrary(RMySQL)\nlibrary(tools)\nif (! (\"readr\" %in% rownames(installed.packages()))) { install.packages(\"readr\") }\nlibrary(readr)\nif (! (\"stringr\" %in% rownames(installed.packages()))) { install.packages(\"stringr\") }\nlibrary(stringr)\nif (! (\"svDialogs\" %in% rownames(installed.packages()))) { install.packages(\"svDialogs\") }\nlibrary(svDialogs)\n#==============================================================================#\n\nFunción de Configuración\n1.1 CargarConfiguracion\nLa función CargarConfiguracion() carga y filtra las configuraciones de una base de datos en formato JSON para ajustarlas a las preferencias de base de datos, alias y dominio. La configuración se almacena globalmente para ser utilizada en otros scripts.\nCargarConfiguracion &lt;- function(FicheroConf = './Config/Config.json', TipoDBElegido ='SQL Server', AliasElegido = 'DBA', DominioElegido = 'Remoto'){\n  .GlobalEnv$configuracion &lt;- fromJSON(FicheroConf)\n  .GlobalEnv$configuracion &lt;- .GlobalEnv$configuracion %&gt;% filter((toupper(TipoDB) == toupper(TipoDBElegido))) \n  .GlobalEnv$configuracion &lt;- .GlobalEnv$configuracion %&gt;% filter((toupper(Alias) == toupper(AliasElegido))) \n  .GlobalEnv$configuracion &lt;- .GlobalEnv$configuracion %&gt;% filter((toupper(Dominio) == toupper(DominioElegido)))\n  \n  return(.GlobalEnv$configuracion)\n}\nParámetros:\n\nFicheroConf: Ruta al archivo de configuración (por defecto ‘./Config/Config.json’).\nTipoDBElegido, AliasElegido, DominioElegido: Parámetros para filtrar las configuraciones.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Funciones Adicionales</span>"
    ]
  },
  {
    "objectID": "Funciones_Adicionales.html#funciones-de-guardado-y-carga-de-archivos",
    "href": "Funciones_Adicionales.html#funciones-de-guardado-y-carga-de-archivos",
    "title": "Funciones Adicionales",
    "section": "Funciones de Guardado y Carga de Archivos:",
    "text": "Funciones de Guardado y Carga de Archivos:\n\n0_Funciones_Guardado.R\n\nFunción Encabezados para Logs\n1.2 ImprimirEncabezado\nLa función ImprimirEncabezado() imprime un encabezado formateado en la consola para organizar visualmente los scripts.\nImprimirEncabezado &lt;- function(texto) {\n  cat(paste0(\n    \"#==============================================================================#\\n\",\n    paste(\"                           \", texto),\n    \"\\n#==============================================================================#\\n\"\n  ))\n}\nParámetros:\n\ntexto: El texto a imprimir como encabezado.\n\n\n\nFunción Inicio Logs\n1.3 ImprimirInicioLog\nLa función ImprimirInicioLog() redirige la salida a un archivo de log con fecha actual y muestra un encabezado decorativo con información sobre la fecha y hora de ejecución.\nImprimirInicioLog &lt;- function() {\n  log_file &lt;- paste0('./Logs/LogsProduccion_', Sys.Date(), '.txt')\n  sink(log_file, append = TRUE, split = TRUE)\n  cat(paste0(\n    \"\\n\",\n    \"#===================================================================================================#\\n\",\n    \"#===================================================================================================#\\n\",\n    \"#___________________________________________________________________________________________________#\\n\",\n    \"#_______________________________________🍳 PRODUCCION LACOR 🍳______________________________________#\\n\",\n    sprintf(\"#            📅 Fecha actual: %-70s#\\n\", Sys.Date()),     \n    sprintf(\"#            🕒 Hora actual: %-71s#\\n\", format(Sys.time(), \"%H:%M:%S\")),      \n    \"#                                                                                                   #\\n\",\n    \"#===================================================================================================#\\n\",\n    \"#===================================================================================================#\\n\",\n    \"\\n\"\n  ))\n}\nParámetros: - No requiere parámetros de entrada.\n\n\nFunción de Guardado\n1.4 GuardarCSV\nLa función GuardarCSV() Guarda un DataFrame como archivo CSV y verifica que se guardó correctamente.\nGuardarCSV &lt;- function(df, nombre_archivo, ruta) {\n  tryCatch({\n    if (!dir.exists(ruta)) {\n      dir.create(ruta, recursive = TRUE) \n    }\n    ruta_completa &lt;- file.path(ruta, paste0(nombre_archivo, \".csv\"))\n    write.csv(df, ruta_completa, row.names = FALSE, fileEncoding = \"UTF-8\", na = \"\")\n    cat(\"✅ Guardado correctamente:\", ruta_completa, \"- Registros:\", nrow(df), \"\\n\")\n  }, error = function(e) {\n    cat(\"❌ Error al guardar:\", nombre_archivo, \"- Mensaje:\", e$message, \"\\n\")\n  })\n}\nParámetros:\n\ndf: El DataFrame a guardar.\nnombre_archivo: El nombre del archivo.\nruta: La ruta donde guardar el archivo.\n\n\n\nFunción de Guardado RData\n1.5 GuardarRData\nLa función GuardarRData() Guarda un objeto R en un archivo .RData y verifica que se haya guardado correctamente.\nGuardarRData &lt;- function(objeto, ruta_archivo) {\n  tryCatch({\n    nombre_objeto &lt;- deparse(substitute(objeto))\n    assign(nombre_objeto, objeto)\n    save(list = nombre_objeto, file = ruta_archivo)\n    if (file.exists(ruta_archivo)) {\n      registros &lt;- if (is.data.frame(objeto)) nrow(objeto) else \"N/A\"\n      cat(\"✅ Archivo guardado: '\", ruta_archivo, \"' - Objeto: \", nombre_objeto, \" - Registros: \", registros, \"\\n\")\n    } else {\n      cat(\"❌ Error: No se pudo guardar '\", ruta_archivo, \"'.\\n\")\n    }\n  }, error = function(e) {\n    cat(\"❌ Error al guardar '\", ruta_archivo, \"': \", e$message, \"\\n\")\n  })\n}\nParámetros:\n\nobjeto: El objeto R que deseas guardar (como un DataFrame, lista, etc.).\nruta_archivo: La ruta completa y nombre del archivo .RData donde se guardará el objeto.\n\n\n\nFunción de Carga de CSV\n1.6 CargarCSV\nLa función CargarCSV() Carga un archivo CSV y maneja los casos donde el archivo no existe o el formato no es válido\nCargarCSV &lt;- function(ruta_archivo) {\n  if (!file.exists(ruta_archivo)) {\n    cat(\"❌ El archivo CSV '\", ruta_archivo, \"' no se encuentra. Verifica el nombre o la fecha.\\n\")\n    stop(\"Archivo CSV no encontrado.\")\n  }\n  \n  tryCatch({\n    df &lt;- read.csv(ruta_archivo, sep = \",\", stringsAsFactors = FALSE, fileEncoding = \"UTF-8\")\n    if (ncol(df) == 1) {\n      df &lt;- read.csv(ruta_archivo, sep = \";\", stringsAsFactors = FALSE, fileEncoding = \"UTF-8\")\n    }\n    cat(\"✅ El archivo CSV '\", ruta_archivo, \"' se cargó correctamente. Registros: \", nrow(df), \" - Columnas: \", ncol(df), \"\\n\")\n    return(df)\n  }, error = function(e) {\n    cat(\"❌ Error al cargar '\", ruta_archivo, \"': \", e$message, \"\\n\")\n    stop(e)\n  })\n}\nParámetros:\n\nruta_archivo: La ruta completa del archivo CSV que deseas cargar.\n\n\n\nFunción de Carga de RData\n1.7 CargarRData\nLa función CargarRData() Carga un archivo .RData y verifica que el proceso se haya realizado correctamente.\nCargarRData &lt;- function(ruta_archivo) {\n  if (!file.exists(ruta_archivo)) {\n    cat(\"❌ El archivo RData '\", ruta_archivo, \"' no se encuentra. Verifica el nombre o la fecha.\\n\")\n    stop(\"Archivo RData no encontrado.\")\n  }\n  objetos_cargados &lt;- load(ruta_archivo, envir = .GlobalEnv)  \n  cat(\"✅ El archivo RData '\", ruta_archivo, \"' se cargó correctamente. Contiene: \", paste(objetos_cargados, collapse = \", \"), \"\\n\")\n  if (length(objetos_cargados) == 1) {\n    return(get(objetos_cargados[1], envir = .GlobalEnv))\n  } else {\n    return(mget(objetos_cargados, envir = .GlobalEnv))\n  }\n}\nParámetros:\n\nruta_archivo: La ruta completa del archivo .RData que deseas cargar.\n\n\n\nFunción de Push para Github\n1.8 PushGitHub\nLa función PushGitHub() se encarga de subir los cambios realizados del script para tenerlos actualizados en GitHub.\nPushGitHub &lt;- function() {\n  ImprimirEncabezado(\"Actualizar Datos GitHub\")\n  \n  # === REPOSITORIO === #\n  repo_dir &lt;- \"/home/dataml/Documentos/GitHub/Lacor\" \n  \n  commit_message &lt;- paste(\"Actualización automática diaria: \", Sys.time())\n  \n  setwd(repo_dir)\n  \n  system(\"git add .\")  \n  system(paste(\"git commit -m '\", commit_message, \"'\", sep = \"\")) \n  system(\"git push origin main\")  \n  \n  cat(\"Cambios subidos a GitHub a las: \", Sys.time(), \"\\n\")\n}\nParámetros: - No requiere parámetros de entrada.\n\n\nFunción de Pull para Github\n1.8 PullGitHub\nLa función PullGitHub() se encarga de cargar el repositorio antes de iniciar cualquier proceso para asi mantener todo actualizado\nPullGitHub &lt;- function() {\n  ImprimirEncabezado(\"Cargar Datos GitHub\")\n  \n  # === REPOSITORIO === #\n  ImprimirEncabezado(\"Actualizar Datos desde GitHub\")\n  \n  repo_dir &lt;- \"/home/dataml/Documentos/GitHub/Lacor\"\n  \n  setwd(repo_dir)\n  \n  system(\"git pull origin main\")\n  \n  cat(\"Repositorio actualizado desde GitHub a las: \", Sys.time(), \"\\n\")\n}\nParámetros: - No requiere parámetros de entrada.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Funciones Adicionales</span>"
    ]
  },
  {
    "objectID": "Funciones_Adicionales.html#funciones-de-gestion-de-datos-y-filtrados",
    "href": "Funciones_Adicionales.html#funciones-de-gestion-de-datos-y-filtrados",
    "title": "Funciones Adicionales",
    "section": "Funciones de Gestion de Datos y Filtrados:",
    "text": "Funciones de Gestion de Datos y Filtrados:\n\n0_Funciones_GestionDatos.R\n\nFunción Guardar Datos con Fecha\n2.1 Guardar_Datos_con_Fecha\nLa función Guardar_Datos_con_Fecha() Guarda un DataFrame de producción diario con la fecha actual en el nombre del archivo .RData.\nGuardar_Datos_con_Fecha &lt;- function(ProduccionDiarioDF) {\n  fecha_actual &lt;- Sys.Date()\n  output_file &lt;- paste0('./Output/Produccion_Diario_DIA_', fecha_actual, '.RData')\n  save(ProduccionDiarioDF, file = output_file)\n  return(output_file)\n}\nParámetros:\n\nProduccionDiarioDF: El DataFrame que contiene los datos de producción diaria.\n\n\n\nFunción Obtener Última Fecha de Actualización\n2.2 ObtenerUltimaFechaAct\nEsta función ObtenerUltimaFechaAct() revisa los archivos de producción diaria almacenados en la carpeta de salida (./Output/) y devuelve la fecha más reciente de los archivos guardados. Es útil para conocer la última actualización de datos disponible en el sistema.\nObtenerUltimaFechaAct &lt;- function() {\n  archivo_total &lt;- './Output/Produccion_Diario_TOTAL.RData'\n  \n  if (!file.exists(archivo_total)) {\n    return(NULL) \n  }\n  \n  ProduccionDiario_TOTAL_DF &lt;- get(load(archivo_total))\n  \n  if (nrow(ProduccionDiario_TOTAL_DF) &gt; 0 && \"Jornada\" %in% colnames(ProduccionDiario_TOTAL_DF)) {\n    return(max(ProduccionDiario_TOTAL_DF$Jornada, na.rm = TRUE))\n  } else {\n    warning(\"El archivo total está vacío o no contiene la columna 'Jornada'.\")\n    return(NULL)\n  }\n}\nParámetros: - No requiere parámetros de entrada.\n\n\nFunción Filtrar Duplicados\n2.3 FiltrarDuplicadosTOTAL\nLa función FiltrarDuplicadosTOTAL() elimina registros duplicados o no válidos del conjunto de datos de producción, priorizando registros con incidencias, fechas de fin más recientes y mayores cantidades buenas.\nFiltrarDuplicadosTOTAL &lt;- function(df) {\n  if (!dir.exists(\"./Output\")) {\n    dir.create(\"./Output\")\n  }\n  df &lt;- df %&gt;%\n    filter(!(Estado == \"PARO\" & is.na(Operario) & is.na(EXPEDIENTE) & is.na(REFERENCIA) & is.na(OF)))\n  \n  df &lt;- df %&gt;%\n    group_by(OF, FASE, EXPEDIENTE, REFERENCIA, MAQUINA, FechaInicio_pr, FechaFin_pr) %&gt;%\n    filter(!(is.na(MATERIAL_SALIDA) | MATERIAL_SALIDA == \"\")) %&gt;%\n    ungroup()\n  \n  df_filtrado &lt;- df %&gt;%\n    group_by(OF, FechaInicio_pr, Estado, FASE, MATERIAL_SALIDA) %&gt;% \n    mutate(IncidenciaPrioridad = !is.na(Incidencia),\n           OperarioSeleccionado = coalesce(first(Operario), NA)) %&gt;%  \n    slice_max(order_by = IncidenciaPrioridad, with_ties = TRUE) %&gt;% \n    slice_max(order_by = FechaFin_pr, with_ties = TRUE) %&gt;%\n    slice_max(order_by = Cantidad_Buenas, with_ties = FALSE) %&gt;%\n    select(-IncidenciaPrioridad, -OperarioSeleccionado) %&gt;%\n    ungroup()\n  \n  filas_eliminadas &lt;- nrow(df) - nrow(df_filtrado)\n  \n  cat(\"🔄 Total de filas originales: \", nrow(df), \"\\n\")\n  cat(\"🎉 Total de filas después del filtrado: \", nrow(df_filtrado), \"\\n\")\n  cat(\"🔥 Filas eliminadas: \", filas_eliminadas, \"\\n\")\n  \n  return(df_filtrado)\n}\nParámetros:\n\ndf: El DataFrame del que deseas eliminar los duplicados.\n\n\n\nFunción Filtrar Duplicados TOTAL\n2.3 FiltrarDuplicadosTOTALDIARIO\nLa función FiltrarDuplicadosTOTALDIARIO() optimiza el proceso de filtrado de duplicados al trabajar solo con un subconjunto de los datos totales, enfocándose en los registros recientes para mejorar el rendimiento.\nFiltrarDuplicadosTOTALDIARIO &lt;- function(df_total, df_diario) {\n  n_filas_a_registrar &lt;- nrow(df_diario) * 2\n  filas_a_registrar &lt;- tail(df_total, n_filas_a_registrar)\n  filas_a_registrar &lt;- filas_a_registrar %&gt;%\n    filter(!(Estado == \"PARO\" & is.na(Operario) & is.na(EXPEDIENTE) & is.na(REFERENCIA) & is.na(OF)))\n  \n  filas_a_registrar &lt;- filas_a_registrar %&gt;%\n    group_by(OF, FASE, EXPEDIENTE, REFERENCIA, MAQUINA, FechaInicio_pr, FechaFin_pr) %&gt;%\n    filter(!(is.na(MATERIAL_SALIDA) | MATERIAL_SALIDA == \"\")) %&gt;%\n    ungroup()\n  \n  df_filtrado &lt;- filas_a_registrar %&gt;%\n    group_by(OF, FechaInicio_pr, Estado, FASE, MATERIAL_SALIDA) %&gt;% \n    mutate(IncidenciaPrioridad = !is.na(Incidencia),\n           OperarioSeleccionado = coalesce(first(Operario), NA)) %&gt;%  \n    slice_max(order_by = IncidenciaPrioridad, with_ties = TRUE) %&gt;% \n    slice_max(order_by = FechaFin_pr, with_ties = TRUE) %&gt;%\n    slice_max(order_by = Cantidad_Buenas, with_ties = FALSE) %&gt;%\n    select(-IncidenciaPrioridad, -OperarioSeleccionado) %&gt;%\n    ungroup()\n  \n  df_sin_filas_a_registrar &lt;- df_total %&gt;%\n    anti_join(filas_a_registrar, by = c(\"OF\", \"FechaInicio_pr\", \"Estado\", \"FASE\", \"MATERIAL_SALIDA\"))\n  df_actualizado &lt;- bind_rows(df_sin_filas_a_registrar, df_filtrado)\n  filas_eliminadas &lt;- nrow(df_total) - nrow(df_actualizado)\n  \n  cat(\"🔄 Total de filas originales: \", nrow(df_total), \"\\n\")\n  cat(\"🎉 Total de filas después del filtrado: \", nrow(df_actualizado), \"\\n\")\n  cat(\"🔥 Filas eliminadas o reemplazadas: \", filas_eliminadas, \"\\n\")\n  \n  \n  return(df_actualizado)\n}\nParámetros:\n\ndf_total: El DataFrame total que contiene todos los registros históricos\ndf_diario: El DataFrame con los nuevos registros diarios\n\n\n\nFunción Filtrar Duplicados Total Diario\n2.5 Actualizar_Datos_ProduccionDiario\nActualizar_Datos_ProduccionDiario() recupera los datos de producción desde la base de datos, adaptando la consulta SQL según exista o no una actualización previa, filtra duplicados y guarda los datos procesados.\nActualizar_Datos_ProduccionDiario &lt;- function(conn) {\n  fecha_ultima_actualizacion &lt;- ObtenerUltimaFechaAct() -1\n  ImprimirEncabezado(\"Datos de Produccion Diaria\")\n  \n  if (is.null(fecha_ultima_actualizacion)) {\n    consulta_sql &lt;- getSQLSQLServer(\"./SQL_Sentencias/Select_PRODUCCION.sql\")\n  } else {\n    consulta_sql &lt;- paste0(\n      \"SELECT  \n            SmFase.smFasePersonalizado1Txt AS 'EXPEDIENTE',\n            smOrdenNme AS 'REFERENCIA', \n            SmMaterial.smMaterialId AS 'MATERIAL_SALIDA', \n            SmMaterial.smMaterialNme AS 'MATERIAL_SALIDA_DESCR', \n            smOrdenId AS 'OF', \n            smFaseId AS 'FASE',\n            smFaseNme AS 'FASE_DESCR', \n            smNodo.smNodoId AS 'MAQUINA',  \n            smNodo.smNodoNme AS 'MAQ_DESC',\n            jornadaDte AS Jornada, \n            OLANET.ObtenerTurnoId(smCalendarioHorarioDetalle.smTurnoUid) AS Turno, \n            ISNULL(ISNULL(ISNULL(shNodoEstadoOperario.FechaInicioLocalDte, shNodoEstadoOperario.FechaInicioDte), ISNULL(shNodoEstadoFase.FechaInicioLocalDte, shNodoEstadoFase.FechaInicioDte)), ISNULL(shNodoEstado.FechaInicioLocalDte, shNodoEstado.FechaInicioDte)) AS FechaInicio_pr,  \n            ISNULL(ISNULL(ISNULL(ISNULL(shNodoEstadoOperario.FechaFinLocalDte, shNodoEstadoOperario.FechaFinDte), ISNULL(shNodoEstadoFase.FechaFinLocalDte, shNodoEstadoFase.FechaFinDte)), ISNULL(shNodoEstado.FechaFinLocalDte,shNodoEstado.FechaFinDte)),GETDATE()) AS FechaFin_pr,  \n            DATEDIFF(SECOND, isnull(ShNodoEstadoFase.fechaInicioDte, ShNodoEstado.fechaInicioDte), iSNULL((isnull(ShNodoEstadoFase.fechafinDte, ShNodoEstado.fechaFinDte)), GETDATE()) ) / 60.0 as Duracion_min, \n            DATEDIFF(SECOND, isnull(ShNodoEstadoFase.fechaInicioDte, ShNodoEstado.fechaInicioDte), iSNULL((isnull(ShNodoEstadoFase.fechafinDte, ShNodoEstado.fechaFinDte)), GETDATE()) ) / 3600.0 as Duracion_h, \n            SmMaterialLote.LoteTxt AS Lote,  \n            (ISNULL(isnull(ShNodoEstadoMaterialSalida.CantidadAutomaticaNbr,0)-isnull(ShNodoEstadoMaterialSalida.cantidadAutomaticaInicialNbr,0)+isnull(ShNodoEstadoMaterialSalida.CantidadManualNbr,0),0)) as Cantidad_Buenas, \n            (ISNULL(ShNodoEstadoMaterialSalidaCantidadRechazo.cantidadNbr,0)) as Cantidad_Malas, \n            CASE WHEN shNodoEstadoFase.NumeroOperariosPresentesNbr = 0 THEN NULL ELSE SmOperario.smOperarioId END as CodOperario, \n            SmOperario.smOperarioNme + ' ' + ISNULL(SmOperario.Apellido1Txt,'') + ' ' + ISNULL(SmOperario.Apellido2Txt,'') as Operario, \n            SmRol.SmRolId AS 'RolOperario',\n            smEstadoId AS Estado,\n            smIncidenciaNme AS Incidencia, \n            ShNodoEstado.observacionesInicioTxt AS ObservacionesInicio,\n            ShNodoEstado.observacionesFinTxt AS ObservacionesFin\n          FROM ShNodoTurno  WITH (NOLOCK) \n          INNER JOIN smNodo  WITH (NOLOCK) \n            ON shNodoTurno.smNodoUid = smNodo.smNodoUid  \n          INNER JOIN ShNodoEstado WITH (NOLOCK) \n            ON ShNodoTurno.ShNodoTurnoUid = ShNodoEstado.ShNodoTurnoUid  \n          INNER JOIN smEstado  WITH (NOLOCK) \n            ON shNodoEstado.smEstadoUid = SmEstado.SmEstadoUid  \n          LEFT JOIN smIncidencia  WITH (NOLOCK) \n            ON shNodoEstado.smIncidenciaUid = smIncidencia.smIncidenciaUid  \n          LEFT JOIN SmCalendarioHorarioDetalle  WITH (NOLOCK) \n            ON ShNodoTurno.smCalendarioHorarioDetalleUid = smCalendarioHorarioDetalle.smCalendarioHorarioDetalleUid  \n          LEFT JOIN ShNodoEstadoFase  WITH (NOLOCK) \n            ON ShNodoEstado.shNodoEstadoUid = shNodoEstadoFase.shNodoEstadoUid  \n          LEFT JOIN ShNodoEstadoOperario  WITH (NOLOCK) \n            ON ShNodoEstado.shNodoEstadoUid = ShNodoEstadoOperario.shNodoEstadoUid   \n          LEFT JOIN ShNodoTurnoOperario  WITH (NOLOCK) \n            ON shNodoTurnoOperario.shNodoTurnoOperarioUid = ShNodoEstadoOperario.shNodoTurnoOperarioUid  \n          LEFT JOIN smOperario  WITH (NOLOCK) \n            ON ShNodoTurnoOperario.smOperarioUid = smOperario.SmOperarioUid \n          LEFT JOIN SmOperarioRol WITH (NOLOCK) \n            ON SmOperario.SmOperarioUid = SmOperarioRol.SmOperarioUid \n          LEFT JOIN SmRol WITH (NOLOCK) \n            ON SmOperarioRol.SmRolUid = SmRol.SmRolUid \n          LEFT JOIN SmMotivoInterrupcion WITH (NOLOCK) \n            ON ShNodoEstadoFase.smMotivoInterrupcionUid = SmMotivoInterrupcion.smMotivoInterrupcionUid  \n          LEFT JOIN SmFase WITH (NOLOCK) \n            ON ShNodoEstadoFase.smFaseUid = SmFase.smFaseUid  \n          LEFT JOIN SmOrden WITH (NOLOCK) \n            ON SmFase.smOrdenUid = SmOrden.smOrdenUid \n          LEFT JOIN ShNodoEstadoMaterialSalida WITH (NOLOCK) \n            ON ShNodoEstado.shNodoEstadoUid = ShNodoEstadoMaterialSalida.shNodoEstadoUid  \n            AND shNodoEstadoFase.shNodoEstadoFaseUid = ShNodoEstadoMaterialSalida.shNodoEstadoFaseUid \n          LEFT JOIN smFaseMaterialSalida  WITH (NOLOCK) \n            ON ShNodoEstadoMaterialSalida.smFaseMaterialSalidaUid = SmFaseMaterialSalida.SmFaseMaterialSalidaUid  \n          LEFT JOIN SmMaterial WITH (NOLOCK) \n            ON smFaseMaterialSalida.smMaterialUid = smMaterial.smMaterialUid  \n          LEFT JOIN SmMaterialLote WITH (NOLOCK) \n            ON ShNodoEstadoMaterialSalida.smMaterialLoteUid = SmMaterialLote.smMaterialLoteUid  \n          LEFT JOIN ShNodoEstadoMaterialSalidaCantidadRechazo  WITH (NOLOCK) \n            ON ShNodoEstadoMaterialSalida.shNodoEstadoMaterialSalidaUid = ShNodoEstadoMaterialSalidaCantidadRechazo.shNodoEstadoMaterialSalidaUid  \n          LEFT JOIN SmMotivoRechazo WITH (NOLOCK) \n            ON ShNodoEstadoMaterialSalidaCantidadRechazo.smMotivoRechazoUid = SmMotivoRechazo.smMotivoRechazoUid \n          WHERE jornadaDte &gt;= '\", fecha_ultima_actualizacion, \"'\n          AND (shNodoEstadoFase.fechaInicioDte = ShNodoEstadoOperario.fechaInicioDte  \n          OR shNodoEstadoFase.fechaInicioDte IS NULL \n          OR ShNodoEstadoOperario.fechaInicioDte IS NULL) \n          AND smRolId = 'Operario'\n          \"\n    )\n  }\n  \n  Produccion_diario_df &lt;- EjecutarConsultaSQLServer(conn, consulta_sql)\n  Produccion_diario_df &lt;- FiltrarDuplicadosTOTAL(Produccion_diario_df)\n  \n  if (!is.null(Produccion_diario_df) && inherits(Produccion_diario_df, \"data.frame\") && nrow(Produccion_diario_df) &gt; 0) {\n    Produccion_diario_df$fecha_extraccion &lt;- Sys.Date()\n    \n    archivo_guardado &lt;- Guardar_Datos_con_Fecha(Produccion_diario_df)\n    cat(paste(\"✨ Datos nuevos guardados en:\", archivo_guardado, \"\\n\"))\n  } else if (is.null(Produccion_diario_df) || !inherits(Produccion_diario_df, \"data.frame\")) {\n    cat(\"❌ Error al recuperar datos: Verifica la consulta SQL o la conexión.\\n\")\n  } else {\n    cat(\"🔑 No hay datos nuevos para guardar.\\n\")\n  }\n  \n}\nParámetros: No requiere parámetros de entrada, pero depende de una conexión activa a la base de datos con.",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Funciones Adicionales</span>"
    ]
  },
  {
    "objectID": "PreparacionDatos_Visualizacion.html",
    "href": "PreparacionDatos_Visualizacion.html",
    "title": "Preparacion Datos",
    "section": "",
    "text": "4_ProcesarDatos.R",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparacion Datos</span>"
    ]
  },
  {
    "objectID": "PreparacionDatos_Visualizacion.html#procesardatos.r",
    "href": "PreparacionDatos_Visualizacion.html#procesardatos.r",
    "title": "Preparacion Datos",
    "section": "",
    "text": "Procesamiento de Datos\nEste script se encarga de cargar, transformar, fusionar y enriquecer los datos\n\nCarga de Datos\nFunción Inicial: El script inicia cargando los datos básicos de producción desde archivos CSV:\nOlanet_Producido &lt;- CargarCSV(\"./OlanetDatos/Olanet_Producido.csv\")\nOlanet_Planificado &lt;- CargarCSV(\"./OlanetDatos/Olanet_Planificado.csv\")\n\n\nLimpieza y Transformación Inicial\nNormalización de identificadores: Se normalizan los campos EXPEDIENTE y OF para asegurar que sean numéricos:\nOlanet_Producido$EXPEDIENTE &lt;- as.numeric(gsub(\"[^0-9]\", \"\", Olanet_Producido$EXPEDIENTE))\nOlanet_Planificado$EXPEDIENTE &lt;- as.numeric(gsub(\"[^0-9]\", \"\", Olanet_Planificado$EXPEDIENTE))\n\nOlanet_Producido$OF &lt;- as.numeric(gsub(\"[^0-9]\", \"\", Olanet_Producido$OF))\nOlanet_Planificado$OF &lt;- as.numeric(gsub(\"[^0-9]\", \"\", Olanet_Planificado$OF))\nEsta normalización elimina cualquier carácter no numérico y convierte los valores a tipo numérico, facilitando las operaciones posteriores.\n\n\nAgregación de Datos Producidos\nCreación de resumen por claves: Se agregan los datos de producción por claves principales:\nOlanet_Producido_Agg &lt;- Olanet_Producido %&gt;%\n  mutate(\n    FechaInicio_pr = as.POSIXct(FechaInicio_pr, format = \"%Y-%m-%d %H:%M:%OS\"),\n    FechaFin_pr = as.POSIXct(FechaFin_pr, format = \"%Y-%m-%d %H:%M:%OS\")\n  ) %&gt;%\n  group_by(EXPEDIENTE, REFERENCIA, OF, FASE, MAQUINA) %&gt;%\n  summarise(...)\nEste proceso:\n\nConvierte fechas al formato POSIXct para manipulación temporal\nAgrupa por las claves principales de negocio\nCalcula métricas agregadas como fechas mínimas/máximas y sumas de cantidades\n\n\n\nCalculo de Tiempos Operativos\nTiempos de Producción Se calculan los tiempos efectivos de producción:\nDF_En_Produccion &lt;- Olanet_Producido %&gt;%\n  filter(Estado == \"PRODUCCION\") %&gt;%  \n  group_by(OF, FASE, MAQUINA, REFERENCIA, EXPEDIENTE) %&gt;%\n  summarise(\n    Tiempo_Total_Produccion_min = sum(Duracion_min, na.rm = TRUE), \n    .groups = \"drop\"\n  )\nTiempos de Preparación Se calculan los tiempos de preparación:\nDF_En_Preparacion &lt;- Olanet_Producido %&gt;%\n  filter(Estado == \"PREPARACION\") %&gt;%  \n  group_by(OF, FASE, MAQUINA, REFERENCIA, EXPEDIENTE) %&gt;%\n  summarise(\n    TiempoRealPreparacion_min = sum(Duracion_min, na.rm = TRUE), \n    .groups = \"drop\"\n  )\nEstos cálculos separan los tiempos por tipo de actividad para análisis detallados.\n\n\nFusión y Cálculo de KPIs\nCreación del Dataset Completo Se fusionan los datos planificados y reales, calculando métricas de rendimiento:\npr_COMPLETO &lt;- Olanet_Planificado %&gt;%\n  mutate(\n    FechaInicio_Plani = as.POSIXct(paste(FechaInicio_Plani, \"00:00:00\"), format = \"%Y-%m-%d %H:%M:%S\"),\n    FechaFin_Plani = as.POSIXct(paste(FechaFin_Plani, \"00:00:00\"), format = \"%Y-%m-%d %H:%M:%S\")\n  ) %&gt;%\n  left_join(Olanet_Producido_Agg, by = c(\"EXPEDIENTE\", \"REFERENCIA\", \"OF\", \"FASE\", \"MAQUINA\")) %&gt;%\n  left_join(DF_En_Produccion, by = c(\"EXPEDIENTE\", \"REFERENCIA\", \"OF\", \"FASE\", \"MAQUINA\")) %&gt;%\n  left_join(DF_En_Preparacion, by = c(\"EXPEDIENTE\", \"REFERENCIA\", \"OF\", \"FASE\", \"MAQUINA\")) %&gt;% \n  mutate(\n    FechaInicio_pr_trunc = as.Date(FechaInicio_pr),\n    FechaFin_pr_trunc = as.Date(FechaFin_pr),\n    FechaInicio_Plani_trunc = as.Date(FechaInicio_Plani),\n    FechaFin_Plani_trunc = as.Date(FechaFin_Plani),\n    \n    DiasDiferenciaInicio = as.numeric(FechaInicio_pr_trunc - FechaInicio_Plani_trunc),\n    DiasDiferenciaFin = as.numeric(FechaFin_pr_trunc - FechaFin_Plani_trunc),\n    \n    CantidadPendiente = CantidadPrevista - CantidadRealizada,\n    CompletadoCantidad = ifelse(CantidadPrevista &gt; 0, coalesce(round(CantidadRealizada / CantidadPrevista, 2), 0), 0),\n    \n    TiempoProduccionReal_min = TiempoReal_min,\n    TiempoProduccionPlanificado_min = (TiempoPreparacion*CantidadPrevista) / 60 + (TiempoTeorico*60),\n    \n    TiempoPrevistoPreparacionUnitario_min = TiempoPreparacion,\n    \n    TiempoRealPreparacion_min = coalesce(TiempoRealPreparacion_min, 0), \n    TiempoPrevistoPreparacion_min = (TiempoPreparacion * CantidadPrevista) / 60 ,\n\n    TiempoPrevistoProduccion_min = TiempoTeorico*60,\n    TiempoProduccion_min = Tiempo_Total_Produccion_min,\n    \n    TiempoPrevistoUnitario_min = TiempoCiclo, \n    TiempoProducidoUnitario_min = ifelse(CantidadRealizada &gt; 0, Tiempo_Total_Produccion_min / CantidadRealizada, NA),\n\n    Disponibilidad = ifelse(Tiempo_Total_Produccion_min &gt; 0, format(round((TiempoReal_min / Tiempo_Total_Produccion_min) * 100, 2), decimal.mark = \",\"), NA),  \n    Calidad = ifelse((CantidadRealizada + CantidadMalas) &gt; 0, (CantidadRealizada / (CantidadRealizada + CantidadMalas)) * 100, NA),\n    Productividad =ifelse(CantidadPrevista &gt; 0, format(round((CantidadRealizada / CantidadPrevista) * 100, 2), decimal.mark = \",\"), NA)         \n  ) %&gt;%\n  select(\n    EXPEDIENTE, REFERENCIA, OF, FASE, FASE_DESCR, MATERIAL_ENTRADA, MATERIAL_ENTRADA_DESCR,MATERIAL_SALIDA, MAQUINA, MAQ_DESC, ESTADO_OLANET, \n    FechaInicio_Plani, FechaInicio_pr, DiasDiferenciaInicio, FechaFin_Plani, FechaFin_pr, DiasDiferenciaFin, \n    CantidadPrevista, CantidadRealizada, CantidadPendiente, CompletadoCantidad, CantidadMalas, \n    TiempoProduccionPlanificado_min,TiempoProduccionReal_min, \n    TiempoPrevistoPreparacionUnitario_min, \n    TiempoPrevistoPreparacion_min, TiempoRealPreparacion_min, \n    TiempoPrevistoProduccion_min, TiempoProduccion_min, \n    TiempoPrevistoUnitario_min, TiempoProducidoUnitario_min\n  )\npr_COMPLETO &lt;- pr_COMPLETO[, !(names(pr_COMPLETO) %in% c(\"MATERIAL_ENTRADA\", \"MATERIAL_ENTRADA_DESCR\"))]\npr_COMPLETO &lt;- unique(pr_COMPLETO)\nCálculos de KPIs importantes:\n\nDiferencias en días entre fechas planificadas y reales\nCantidades pendientes y porcentajes de completitud\nTiempos de producción real vs. planificado\nIndicadores de rendimiento: Disponibilidad, Calidad y Productividad\n\n\n\nEnriquecimiento para Producción\nEnriquecimiento del Dataset Original Se añaden datos de planificación al dataset original:\nOlanet_Planificado_Key &lt;- pr_COMPLETO %&gt;%\n  select(OF, FASE, MAQUINA, REFERENCIA, EXPEDIENTE, CantidadPrevista, \n         TiempoProduccionPlanificado_min, TiempoPrevistoPreparacion_min, \n         TiempoPrevistoProduccion_min) %&gt;%\n  distinct()\n\nOlanet_Producido &lt;- Olanet_Producido %&gt;%\n  left_join(Olanet_Planificado_Key, by = c(\"OF\", \"FASE\", \"MAQUINA\", \"REFERENCIA\", \"EXPEDIENTE\"))\n\n\nGuardar Datos creados\nExportación de Resultados Los datasets procesados se guardan en archivos CSV:\nGuardarCSV(Olanet_Producido, \"Olanet_Producido\", \"./OlanetDatos\")\nGuardarCSV(pr_COMPLETO, \"Olanet_Completo\", \"./OlanetDatos\")",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparacion Datos</span>"
    ]
  },
  {
    "objectID": "PreparacionDatos_Visualizacion.html#crearmaestros.r",
    "href": "PreparacionDatos_Visualizacion.html#crearmaestros.r",
    "title": "Preparacion Datos",
    "section": "5_CrearMaestros.R",
    "text": "5_CrearMaestros.R\n\nCreación de Maestros Identificadores para Modelo en Estrella\nEste script genera tablas maestras para la gestión de identificadores y categorías.\n\nCarga de Datos Base\nCarga del Dataset Planificado Se carga el dataset de planificación para extraer datos maestros:\nOlanet_Planificado &lt;- CargarCSV(\"./OlanetDatos/Olanet_Planificado.csv\")\n\n\nGeneración de Tablas Maestras\nExtracción de Identificadores Únicos Se crean tablas maestras para las principales entidades con registros unicos:\nMaestro_Maquina &lt;- Olanet_Planificado %&gt;%\n  select(MAQUINA, MAQ_DESC) %&gt;%\n  drop_na()  %&gt;% \n  distinct()\nMaestro_Expediente &lt;- Olanet_Planificado %&gt;%\n  select(EXPEDIENTE) %&gt;%\n  drop_na()  %&gt;% \n  distinct()\nMaestro_OF &lt;- Olanet_Planificado %&gt;%\n  select(OF) %&gt;%\n  drop_na() %&gt;%\n  distinct() \nMaestro_Referencia &lt;- Olanet_Planificado %&gt;%\n  select(REFERENCIA) %&gt;%\n  drop_na() %&gt;%\n  distinct() \nMaestro_EstadoOlanet &lt;- Olanet_Planificado %&gt;%\n  select(ESTADO_OLANET) %&gt;%\n  drop_na()  %&gt;% \n  distinct()\n\n\nExportación de Maestros\nGuardado de Tablas Maestras Se utiliza un bucle para guardar todas las tablas maestras:\nXX_MAESTROS_OLANET &lt;- list(\n  Maestro_Maquina = \"Maestro_Maquina\",\n  Maestro_Expediente = \"Maestro_Expediente\",\n  Maestro_OF = \"Maestro_OF\",\n  Maestro_Referencia = \"Maestro_Referencia\",\n  Maestro_EstadoOlanet = \"Maestro_EstadoOlanet\"\n)\n\nfor (nombre in names(XX_MAESTROS_OLANET)) {\n  GuardarCSV(get(nombre), XX_MAESTROS_OLANET[[nombre]], \"./OlanetDatos/MaestrosIdentificadores\")\n}",
    "crumbs": [
      "Desarrollo",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Preparacion Datos</span>"
    ]
  },
  {
    "objectID": "Visualización_de_Datos.html",
    "href": "Visualización_de_Datos.html",
    "title": "Visualización de Datos",
    "section": "",
    "text": "Instalación de Grafana\nPara facilitar la visualización de los datos en tiempo real, instalé Grafana. Esta herramienta permite crear paneles interactivos y analizar la producción de manera dinámica.\nInicialmente, diseñé una visualización simple para probar distintas herramientas. Sin embargo, encontré ciertas limitaciones, como la imposibilidad de aplicar filtros y algunos inconvenientes en la configuración.",
    "crumbs": [
      "Visualización",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualización de Datos</span>"
    ]
  },
  {
    "objectID": "Visualización_de_Datos.html#power-bi",
    "href": "Visualización_de_Datos.html#power-bi",
    "title": "Visualización de Datos",
    "section": "Power BI",
    "text": "Power BI\nEn paralelo, utilicé Power BI para generar visualizaciones más avanzadas. Los archivos .CSV generados fueron importados en Power BI para realizar análisis detallados y gráficos más sofisticados, dado que Grafana no soportaba ciertos tipos de visualizaciones necesarias.\nGracias a Power BI, pudimos aplicar filtros dinámicos para mejorar la presentación de los datos y optimizar su análisis.\nPara probar distintas opciones, creamos varios tipos de gráficos que facilitaron la exploración y comprensión de los datos de Lacor.",
    "crumbs": [
      "Visualización",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualización de Datos</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html",
    "href": "VisualizacionPBI.html",
    "title": "Visualización en Power Bi",
    "section": "",
    "text": "Uso Power BI\nTras evaluar varias opciones, llegamos a la conclusión de que Power BI era la herramienta más adecuada para el proyecto. Esto se debió a varias razones clave:\nEn resumen, Power BI no solo ofrece un alto grado de personalización y facilidad de uso, sino que también es capaz de escalar y adaptarse a las necesidades específicas del proyecto a medida que evoluciona.",
    "crumbs": [
      "Visualización",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualización en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html#uso-power-bi",
    "href": "VisualizacionPBI.html#uso-power-bi",
    "title": "Visualización en Power Bi",
    "section": "",
    "text": "Facilidad de Conexión a Datos: Power BI permite una integración sencilla con bases de datos como MySQL, facilitando la extracción y visualización de grandes volúmenes de datos.\nVisualización Intuitiva: Ofrece una amplia variedad de opciones de visualización interactivas, lo que facilita la creación de dashboards atractivos y fáciles de interpretar para los usuarios finales.\nTransformación de Datos: Aunque la transformación de datos puede realizarse de forma más compleja que en otras herramientas, Power BI ofrece un entorno robusto para preparar los datos a través de Power Query, lo que permite personalizar las transformaciones según las necesidades del negocio junto a sus medidas DAX.\nInteractividad y Filtros: Una de las características más destacadas es la capacidad de aplicar filtros y segmentar los datos en tiempo real, lo que permite obtener insights específicos de manera instantánea.\nEscalabilidad: A medida que el volumen de datos crezca, Power BI es capaz de manejar grandes cantidades de información sin perder rendimiento, lo que es esencial para el monitoreo continuo en un entorno de producción.",
    "crumbs": [
      "Visualización",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualización en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html#obtencion-de-datos",
    "href": "VisualizacionPBI.html#obtencion-de-datos",
    "title": "Visualización en Power Bi",
    "section": "Obtencion de Datos",
    "text": "Obtencion de Datos\nInicialmente, para realizar pruebas, cargamos los datos mediante archivos .csv. Sin embargo, una vez que el servidor estuvo operativo y la estructura de los datos fue definida correctamente, pasamos a conectarnos directamente a la base de datos MySQL previamente configurada.\nPara ello, fue necesario instalar un conector compatible con Windows:\n🔗 Descarga Conector MySQL\n\nmysql-connector-net-9.2.0\n\nCon la conexión establecida y los datos integrados en Power BI, comenzamos con las primeras comprobaciones y la definición de relaciones entre tablas.",
    "crumbs": [
      "Visualización",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualización en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html#primeros-pasos",
    "href": "VisualizacionPBI.html#primeros-pasos",
    "title": "Visualización en Power Bi",
    "section": "Primeros Pasos",
    "text": "Primeros Pasos\nUna vez identificados los datos disponibles, procedimos a diseñar un Power BI optimizado para su uso futuro. Esta visualización destaca por su interfaz intuitiva y su formato tipo aplicación, con paneles interactivos que permiten aplicar filtros y métricas para una mejor interpretación de la información.\nUno de los principales desafíos fue la correcta relación entre las tablas, ya que Power BI no proporciona claves foráneas de manera automática. Para solucionar esto, fue necesario crear tablas maestras que permitieran establecer las relaciones adecuadas.\n\n\n\nTras estructurar y relacionar correctamente los datos, desarrollamos una visualización dinámica adaptada a las necesidades de la empresa. El objetivo principal era ofrecer una vista diferenciada de las fases Preparadas y los que estaban en Producción, permitiendo un mayor control sobre ambos procesos.\nPara mejorar la preparación de los datos en R, creé varias tablas con diferentes cálculos, medidas y filtros, lo que amplió las posibilidades de visualización y análisis.",
    "crumbs": [
      "Visualización",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualización en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionPBI.html#visualización",
    "href": "VisualizacionPBI.html#visualización",
    "title": "Visualización en Power Bi",
    "section": "Visualización",
    "text": "Visualización\n\nVisualización Inicial\nEn las primeras fases del proyecto, se propuso una visualización tipo tabla que, si bien contenía toda la información necesaria, no resultaba adecuada para una interpretación rápida y eficaz de los datos de producción. La presentación era densa y poco intuitiva, lo que complicaba el análisis y la toma de decisiones tanto para el equipo técnico como para el de producción. Este enfoque inicial sirvió, sin embargo, para comprender mejor las necesidades específicas del equipo de calidad y producción, lo que permitió evolucionar hacia un modelo de visualización más dinámico, interactivo y orientado al usuario.\n\n\n\n\n\nVisualizacion Final\nFinalmente, tras llegar a un entendimiento con el equipo de producción, se planteó una nueva secuencia de visualización con un diseño más atractivo, similar a una aplicación. De esta forma, se pudieron detallar las distintas fases del proceso y detectar errores con mayor facilidad.\nEl dashboard final se organiza en cuatro apartados principales, diseñados para ofrecer un acceso rápido y una comprensión detallada del estado de la producción, los operarios y las incidencias. La navegación es intuitiva, con menús y opciones de filtrado que permiten al usuario explorar la información en distintos niveles de profundidad, desde vistas generales hasta análisis específicos.\nA continuación, se describe cada uno de los apartados:\n\nInicio\nLa sección de Inicio proporciona una visión global del desempeño de la empresa. Aquí se presenta un balance comparativo entre el año actual y el anterior, mostrando indicadores clave como el volumen de piezas buenas producidas. Este panel actúa como el punto de partida del dashboard, desde el cual los usuarios pueden navegar hacia los análisis más específicos mediante un menú lateral interactivo.\n-   Comparativa año actual vs año anterior.\n\n-   Resumen de producción anual.\n\n-   Acceso rápido a los módulos de análisis detallado.\n\n\n\n\n\nAnálisis de Líneas:\nEn este apartado se ofrece una vista general del rendimiento de todas las líneas de producción. Cada línea es evaluada en tres aspectos fundamentales:\nCalidad\n&gt; Fórmula:\n&gt; Calidad = Total de Piezas Buenas / Total de Piezas\n\nEficiencia\n&gt; Fórmula:\n&gt; Eficiencia = Tiempo de Producción / Tiempo Total\n\nProductividad\n&gt; Fórmula:\n&gt; Productividad = (Tiempo Unitario Teórico × Total de Piezas Realizadas) / (Tiempo Unitario de Producción × Total de Piezas Realizadas)\nPara facilitar la interpretación rápida de los resultados, se utilizan códigos de color:\n-   🔴 Rojo: indicadores entre 0% y 20% .\n\n-   🟠 Naranja: indicadores entre 20% y 40% .\nOpciones de Análisis: El usuario tiene la posibilidad de personalizar el rango temporal del análisis mediante diversos filtros dinámicos:\n```         \n- Día actual: análisis centrado en la jornada de hoy.\n\n- Último día trabajado: evaluación del día más reciente con actividad productiva.\n\n- Rango de Jornadas: selección libre de un intervalo de fechas para análisis histórico o comparativo.\n```\nEstos filtros permiten adaptar el análisis según las necesidades específicas del momento, ya sea en tiempo real o para estudios retrospectivos.\n\n\n\n\n\n\nExploración Detallada mediante Drillthrough: Una de las funcionalidades más potentes de este apartado es el uso del drillthrough. El drillthrough permite al usuario profundizar en el análisis de un elemento específico, en este caso, una máquina concreta perteneciente a una línea de producción.\n¿Cómo funciona el Drillthrough?\n\nIdentificación: El usuario, tras identificar una línea de producción con indicadores preocupantes o interesantes, puede examinar la lista de máquinas asociadas.\nSelección: A través de un click derecho sobre la máquina de interés, se despliega un menú contextual.\nAcceso: Seleccionando la opción de “Drillthrough”, el usuario es redirigido automáticamente al módulo “Máquinas”.\n\nAnálisis detallado: En la página “Máquinas”, se presenta un desglose pormenorizado del desempeño de esa máquina específica, incluyendo:\n  - Tiempos de producción y tiempos de parada. \n  - Referencias trabajadas. \n  - Número de piezas buenas y piezas malas \n  - Incidencias registradas. \n  - Cálculo de OEE de la máquina.\nEsta funcionalidad transforma el análisis de un nivel macro (línea) a un nivel micro (máquina individual), permitiendo un enfoque de diagnóstico más ágil y efectivo.\n\n\n\nAdemás, desde la vista detallada de máquinas, se puede realizar un segundo drillthrough hacia el análisis por referencia, donde se profundiza aún más para identificar:\n  - Qué referencias específicas provocaron problemas.\n\n  - Qué tipo de incidencias se registraron.\n\n  - Qué operarios intervinieron en cada proceso.\n\n\n\n\n\n\nEsta navegación en cascada facilita el seguimiento y resolución de problemas desde el nivel más general hasta el más específico.\n\n\nAnálisis de Operarios:\nEsta sección proporciona una visión integral del desempeño de los operarios en el proceso productivo. Se presentan métricas consolidadas de todos los trabajadores, permitiendo detectar rápidamente diferencias de desempeño entre ellos.\nEl usuario puede seleccionar un operario específico y, a través de un click derecho, acceder a la página “DetallesOperario”, donde se ofrece un desglose detallado de su actividad en la fecha o rango seleccionado:\n  - Máquinas en las que ha trabajado.\n\n  - Referencias realizadas.\n\n  - Número de piezas buenas y malas\n\n  - Incidencias asociadas.\n\n  - Tiempos de producción y preparación.\n\n  - Cálculo de su OEE \nEste análisis facilita identificar operarios que requieren apoyo o formación adicional, así como destacar aquellos de mejor rendimiento.\n\n\n\n\n\n\n\n\nAnálisis de Incidencias:\nEl último apartado se centra en el análisis detallado de las incidencias registradas durante la producción. Esta sección está diseñada para detectar patrones recurrentes de fallos en máquinas, líneas, turnos o referencias específicas.\nEl usuario puede aplicar filtros dinámicos para:\n-   Seleccionar por jornada o intervalo de fechas.\n\n-   Filtrar por turno de trabajo.\n\n-   Analizar incidencias por línea o por máquina.\nPara cada incidencia, se detallan:\n-   Tipo de incidencia ocurrida.\n\n-   Tiempo perdido asociado.\n\n-   Operario responsable.\nEste enfoque permite no solo corregir errores puntuales, sino también implementar acciones de mejora continua basadas en datos históricos y tendencias.",
    "crumbs": [
      "Visualización",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualización en Power Bi</span>"
    ]
  },
  {
    "objectID": "VisualizacionGrafana.html",
    "href": "VisualizacionGrafana.html",
    "title": "Visualización en Grafana",
    "section": "",
    "text": "Uso de Grafana\nComo mencioné anteriormente, al principio la visualización en Grafana era bastante limitada para el proyecto inicial. Sin embargo, logramos adaptar la herramienta para crear un sistema de paneles de visualización enfocado en el monitoreo en tiempo real de las líneas de producción.\nAdemas, Grafana está alojado en el servidor configurado para este proyecto, el mismo en el que se almacenan los datos de producción, lo que facilita un acceso rápido, seguro y centralizado tanto a los paneles de visualización como a la base de datos MySQL.",
    "crumbs": [
      "Visualización",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Visualización en Grafana</span>"
    ]
  },
  {
    "objectID": "VisualizacionGrafana.html#conexión-a-base-de-datos",
    "href": "VisualizacionGrafana.html#conexión-a-base-de-datos",
    "title": "Visualización en Grafana",
    "section": "Conexión a Base de Datos",
    "text": "Conexión a Base de Datos\nAl igual que en Power BI, nos conectamos directamente a la base de datos MySQL. En este caso, Grafana obtiene y envía los datos mediante consultas SELECT. Sin embargo, a diferencia de Power BI, las transformaciones de los datos son más complicadas en Grafana, lo que añade una capa de complejidad a la hora de trabajar con los datos.\n\nVisualización\nPara este proyecto, creamos un panel/dashboard para cada línea de fabricación, mostrando los datos relevantes de manera clara y concisa. La visualización incluye los siguientes elementos:\n\nTabla Iniciadas:\nEsta tabla muestra el estado de la máquina, con un código de colores para facilitar la interpretación:\n\nAmarillo: Incidencia\nVerde: Producción\nAzul: Preparación\n\nAdemás, se incluyen los siguientes datos:\n-   Expediente\n-   Referencia\n-   Orden de Fabricación (OF)\n-   Fase\n-   Máquina\n-   Fecha de inicio de la producción\n-   Cantidad prevista\n-   Cantidad realizada\n-   Un **% de cantidad completada** con una barra de progreso\n-   Un **% de tiempo completado**, representado con una barra de carga similar\n\n\nTabla por REFERENCIA: Piezas Buenas y Malas:\nOtra tabla que muestra las piezas buenas y malas, agrupadas por referencia.\n\n\nTotal:\nUn gráfico tipo “pie chart” que muestra el total de piezas buenas frente a las piezas malas, facilitando la visualización de la proporción entre ambos.\n\n\nPlano del Taller Interactivo:\nCon el objetivo de mejorar aún más la visualización y localización de los estados de las máquinas dentro del entorno productivo, se creó un plano interactivo del taller utilizando el plugin HTML Graphics de Grafana.\nEste plano incluye:\n\nUbicación de las máquinas sobre el mapa del taller, mostrando de manera precisa su disposición real.\nCambio de color dinámico de cada máquina en el plano, en función de su estado operativo como en la Tabla Iniciadas\nDelimitación de líneas de producción: Cada grupo de máquinas pertenecientes a una línea de producción específica está rodeado por líneas en movimiento que indican visualmente el área que ocupa cada línea dentro del taller.\nActualización en tiempo real: Los estados de las máquinas y las líneas en el plano se actualizan automáticamente gracias a la sincronización constante con la base de datos.\n\nEsta integración no solo proporciona un acceso visual inmediato a la situación de cada máquina, sino que también facilita la gestión y supervisión rápida del entorno productivo, permitiendo actuar de manera inmediata ante cualquier incidencia.\n\n\n \n\n\n\n\n\nImplementacion\nEste sistema de visualización mediante Grafana ha sido diseñado para ser mostrado en pantallas informativas distribuidas en la fábrica, lo que proporciona al personal de producción una información clara, accesible y actualizada en tiempo real.\nMediante la función playlist de Grafana, los diferentes paneles de cada línea de producción se rotan automáticamente cada pocos segundos, permitiendo que toda la información relevante esté disponible de manera cíclica sin necesidad de intervención manual.\nActualmente se monitorizan las siguientes líneas:\n\n| ALUMINIO | DOMÉSTICO | ENTALLADORAS | INOXIDABLE |\n\nA medida que nuevas máquinas se integren en el proceso productivo, sus datos se incorporarán automáticamente a este sistema de visualización, ampliando progresivamente el alcance del monitoreo en planta.",
    "crumbs": [
      "Visualización",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Visualización en Grafana</span>"
    ]
  }
]