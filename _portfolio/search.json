[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Soy Sandra Caeiro Pires, especializada en análisis de datos, programación, visualización y procesos ETL. A lo largo de mi formación y experiencia profesional, he desarrollado soluciones automatizadas para el monitoreo de datos de producción en tiempo real, así como dashboards interactivos que permiten mejorar la eficiencia y apoyar la toma de decisiones estratégicas.\nHe trabajado conectando y transformando datos desde múltiples fuentes mediante procesos ETL, automatizando flujos de datos y asegurando su integridad y calidad para su análisis. Utilizo herramientas como Power BI, Grafana, Python, R y SQL, además de bases de datos como InfluxDB, SQL Server y MongoDB.\nCuento con una especialización en Inteligencia Artificial y Big Data, donde me enfoqué en la extracción, limpieza y consolidación de datos, así como en la creación de reportes interactivos y sistemas automatizados. También tengo formación en programación y entornos visuales, lo que me permite aportar una visión técnica y creativa a los proyectos.\nMe considero una persona proactiva, con gran capacidad para el trabajo en equipo y muchas ganas de seguir aprendiendo y aplicando soluciones basadas en datos para resolver problemas reales.\n\n\n\n\n\n\n\n Mendaro, Guipuzkoa\n\n\n 688 607 204\n\n\n sandracapi03@gmail.com"
  },
  {
    "objectID": "index.html#áreas-de-especialización-etl",
    "href": "index.html#áreas-de-especialización-etl",
    "title": "Portfolio",
    "section": "Áreas de Especialización: ETL",
    "text": "Áreas de Especialización: ETL\n\n\n\n\nVisualización de Datos\n\nCreación de dashboards interactivos y sistemas de seguimiento en tiempo real utilizando Power BI, Grafana, Matplotlib y Tableau.\n\n\n\n\nAutomatización\n\nDesarrollo de scripts y soluciones automatizadas para extracción, transformación y monitoreo de datos de producción.\n\n\n\n\nAnálisis Avanzado\n\nOptimización de procesos productivos mediante análisis de datos y algoritmos de eficiencia."
  },
  {
    "objectID": "index.html#proyectos-destacados",
    "href": "index.html#proyectos-destacados",
    "title": "Portfolio",
    "section": "Proyectos Destacados",
    "text": "Proyectos Destacados\nDebido a acuerdos de confidencialidad con las empresas, los repositorios de GitHub no pueden ser publicados. No obstante, si está interesado en revisar más a fondo la lógica de los scripts o los flujos de procesamiento implementados, puede contactarme para proporcionar ejemplos o explicaciones adicionales bajo acuerdo.\n\n\n\nVisualización y Automatización en Lacor\n\n\n\nDesarrollo de scripts para extracción, limpieza y análisis de datos usando Python y R.\n\n\nImplementación de dashboards interactivos para el seguimiento en tiempo real y toma de decisiones estratégicas.\n\n\nExplorar proyecto →\n\n\n\n\nExtracción de datos de Consumos WebScraping en Lacor\n\n\n\nExtracción de informes de consumos de máquinas, planta y placas solares.\n\n\nBase para el análisis del coste energético por fuente y área de producción.\n\n\nExplorar proyecto →\n\n\n\n\nExperiencia Laboral\n\nEspecialista en Visualización y Automatización\nLacor Menaje\nDiciembre 2024 - Mayo 2025\n\nDesarrollo e implementación de procesos ETL utilizando Python y R para la extracción, transformación y carga de datos de producción desde múltiples fuentes (SQL Server, InfluxDB).\nDiseño y programación de dashboards en Power BI con análisis avanzado mediante DAX para seguimiento de KPIs críticos de producción y eficiencia operativa.\nImplementación de sistema de monitorización en tiempo real con Grafana conectado a bases de datos de series temporales para supervisión continua del rendimiento de maquinaria industrial.\nAutomatización de flujos de datos mediante scripts en Python para procesamiento y limpieza de información procedente de sensores IoT industriales.\nDesarrollo de soluciones de predicción utilizando bibliotecas scikit-learn y pandas para anticipar mantenimientos preventivos y optimizar tiempos de producción.\n\n\n\nAnimador 3D\nIMH Campus\nAbril 2021 - Junio 2021\n\nDModelado 3D detallado: Creación precisa del torno Pinacho, asegurando fidelidad en dimensiones y detalles técnicos.\nTexturizado y materiales: Aplicación de texturas y materiales realistas para mejorar la apariencia visual del modelo.\nRigging del torno: Configuración de elementos móviles del torno para garantizar movimientos funcionales y realistas.\nAnimación explicativa: Desarrollo de animaciones que simulan el funcionamiento real de la máquina, destacando sus partes y procesos clave.\nAplicación educativa: Diseño del proyecto con un enfoque pedagógico, facilitando la comprensión de los alumnos sobre el funcionamiento del torno.\n\n\n\n\nLenguajes de Programación\n\n\n\n\nPython\n\n\nAnálisis de datos, automatización y desarrollo de scripts para ETL (Extract, Transform, Load).\n\n\n\n\n\nR\n\n\nAnálisis estadístico y visualización de datos para proyectos de data science.\n\n\n\n\n\nSQL\n\n\nGestión, consulta y optimización de bases de datos relacionales.\n\n\n\n\n\nHTML/SVG\n\n\nDesarrollo de interfaces web y gráficos vectoriales para visualizaciones.\n\n\n\n\n\nVisualización de Datos\n\n\n\n\nPower BI\n\n\nCreación de dashboards interactivos y análisis de datos empresariales.\n\n\n\n\n\nGrafana\n\n\nImplementación de sistemas de monitoreo y visualización en tiempo real.\n\n\n\n\n\nMatplotlib\n\n\nVisualización de datos científicos con librerías de Python.\n\n\n\n\n\nTableau\n\n\nDesarrollo de visualizaciones dinámicas para análisis de negocio.\n\n\n\n\n\nBases de Datos\n\n\n\n\nMongoDB\n\n\nGestión de bases de datos NoSQL para datos no estructurados.\n\n\n\n\n\nSQL Server\n\n\nAdministración de bases de datos empresariales.\n\n\n\n\n\nInfluxDB\n\n\nBases de datos de series temporales para monitoreo.\n\n\n\n\n\nMySQL\n\n\nGestión de bases de datos relacionales open source.\n\n\n\n\n\nHerramientas y Tecnologías\n\n\n\n\nGit/GitHub\n\n\nControl de versiones y colaboración en desarrollo de software.\n\n\n\n\n\nExcel Avanzado\n\n\nAnálisis, modelado y visualización de datos corporativos.\n\n\n\n\n\nAdobe Suite\n\n\nPhotoshop, Premiere Pro, After Effects para edición visual.\n\n\n\n\n\nModelado 3D\n\n\nAutodesk Maya, 3ds Max, MudBox para diseño tridimensional."
  },
  {
    "objectID": "index.html#formación-académica",
    "href": "index.html#formación-académica",
    "title": "Portfolio",
    "section": "Formación Académica",
    "text": "Formación Académica\n\n\nEspecialización en Inteligencia Artificial y Big Data\nMiguel Altuna | Septiembre 2024 - Mayo 2025\n\nDesarrollo de scripts para la extracción, limpieza y consolidación de datos desde diversas fuentes\nAnálisis, limpieza y transformación de datos mediante programación en Python y R\nGestión y análisis de diferentes Bases de Datos\nCreación de dashboards y reportes interactivos\nAutomatización de flujos de datos y mejora de la infraestructura\n\n\n\nGrado Superior en Animación 3D y Programación\nCeinpro | Septiembre 2019 - Junio 2021\n\nDiseño y desarrollo de entornos interactivos y multimedia\nProgramación de aplicaciones y sistemas visuales"
  },
  {
    "objectID": "index.html#idiomas-y-habilidades-adicionales",
    "href": "index.html#idiomas-y-habilidades-adicionales",
    "title": "Portfolio",
    "section": "Idiomas y Habilidades Adicionales",
    "text": "Idiomas y Habilidades Adicionales\n\n\n\nIdiomas\n\nEspañol: Nativo\nEuskera: Nativo, B2\nInglés: B2\n\n\n\n\n\n\nCualidades Personales\n\nAlta capacidad para trabajo en equipo\nActitud positiva y proactiva\nOrientación a resultados\nCarné de conducir y vehículo propio\n\n\n\n\n\nConocer más sobre mis Habilidades"
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Habilidades Técnicas",
    "section": "",
    "text": "Estas herramientas forman parte de mi día a día para transformar datos crudos en análisis útiles y automatizados. Me permiten conectar fuentes diversas, limpiar y preparar datos de forma robusta, y presentar la información de forma clara y orientada a decisiones.\n\n\n\n\n\n\n\n\nUso Python principalmente para el tratamiento automatizado de datos:\n\n\nDesarrollo de procesos ETL adaptados a distintas fuentes\n\nLimpieza, transformación y validación de datos con Pandas/Numpy\n\nAutomatización de tareas repetitivas de análisis\n\nRecolección de datos desde bases de datos y ficheros\n\n\n\n\n\n\n\n\nLenguaje clave en mi trabajo diario con datos, especialmente en entornos industriales:\n\n\nProcesos ETL y scripts automatizados para limpieza de datos\n\nInformes dinámicos con R Markdown\n\nVisualizaciones con ggplot2\n\nModelos analíticos y dashboards interactivos con Shiny\n\nIntegración con múltiples fuentes de datos\n\n\n\n\n\n\n\n\n\n\n\n\nUtilizo SQL para extraer datos de manera precisa desde bases de datos relacionales.\n\nUna vez obtenidos, realizo el tratamiento y análisis en R o Python según el objetivo del proyecto:\n\n\nExtracción limpia y eficiente desde múltiples sistemas\nCombinación de tablas y aplicación de filtros según lógica de negocio\nPreparación de datos estructurados para análisis avanzado\nApoyo a procesos de automatización, visualización y toma de decisiones\n\n\n\n\n\n\n\n\n\n\n\n\nAmplia experiencia en el uso de todas las funciones de Power BI:\n\n\nModelado de datos complejo con DAX\n\nPower Query para ETL visual y conexión a múltiples fuentes\n\nPaneles interactivos adaptados a usuarios finales\n\nConfiguración de seguridad, thresholds, segmentaciones\n\nOptimización de rendimiento y mantenimiento de informes\n\n\n\n\n\n\n\n\nVisualización en tiempo real con foco técnico y funcional:\n\n\nCreación de paneles personalizados con SQL o InfluxDB\n\nVisualización de métricas industriales o de telemetría\n\nConfiguración de alertas y seguimiento continuo\n\nDashboards claros adaptados a distintos equipos\n\n\n\n\n\n\n\n\n\n\n\n\nExperiencia práctica trabajando con distintos sistemas de almacenamiento:\n\n\nRelacionales: SQL Server, PostgreSQL, MySQL\n\nNo relacionales: MongoDB, InfluxDB\n\nExtracción de datos para su posterior análisis\n\nEstructuración eficiente y uso según tipo de dato y volumen"
  },
  {
    "objectID": "skills.html#por-qué-estas-habilidades",
    "href": "skills.html#por-qué-estas-habilidades",
    "title": "Habilidades Técnicas",
    "section": "",
    "text": "Estas herramientas forman parte de mi día a día para transformar datos crudos en análisis útiles y automatizados. Me permiten conectar fuentes diversas, limpiar y preparar datos de forma robusta, y presentar la información de forma clara y orientada a decisiones.\n\n\n\n\n\n\n\n\nUso Python principalmente para el tratamiento automatizado de datos:\n\n\nDesarrollo de procesos ETL adaptados a distintas fuentes\n\nLimpieza, transformación y validación de datos con Pandas/Numpy\n\nAutomatización de tareas repetitivas de análisis\n\nRecolección de datos desde bases de datos y ficheros\n\n\n\n\n\n\n\n\nLenguaje clave en mi trabajo diario con datos, especialmente en entornos industriales:\n\n\nProcesos ETL y scripts automatizados para limpieza de datos\n\nInformes dinámicos con R Markdown\n\nVisualizaciones con ggplot2\n\nModelos analíticos y dashboards interactivos con Shiny\n\nIntegración con múltiples fuentes de datos\n\n\n\n\n\n\n\n\n\n\n\n\nUtilizo SQL para extraer datos de manera precisa desde bases de datos relacionales.\n\nUna vez obtenidos, realizo el tratamiento y análisis en R o Python según el objetivo del proyecto:\n\n\nExtracción limpia y eficiente desde múltiples sistemas\nCombinación de tablas y aplicación de filtros según lógica de negocio\nPreparación de datos estructurados para análisis avanzado\nApoyo a procesos de automatización, visualización y toma de decisiones\n\n\n\n\n\n\n\n\n\n\n\n\nAmplia experiencia en el uso de todas las funciones de Power BI:\n\n\nModelado de datos complejo con DAX\n\nPower Query para ETL visual y conexión a múltiples fuentes\n\nPaneles interactivos adaptados a usuarios finales\n\nConfiguración de seguridad, thresholds, segmentaciones\n\nOptimización de rendimiento y mantenimiento de informes\n\n\n\n\n\n\n\n\nVisualización en tiempo real con foco técnico y funcional:\n\n\nCreación de paneles personalizados con SQL o InfluxDB\n\nVisualización de métricas industriales o de telemetría\n\nConfiguración de alertas y seguimiento continuo\n\nDashboards claros adaptados a distintos equipos\n\n\n\n\n\n\n\n\n\n\n\n\nExperiencia práctica trabajando con distintos sistemas de almacenamiento:\n\n\nRelacionales: SQL Server, PostgreSQL, MySQL\n\nNo relacionales: MongoDB, InfluxDB\n\nExtracción de datos para su posterior análisis\n\nEstructuración eficiente y uso según tipo de dato y volumen"
  }
]